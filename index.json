[
{
	"uri": "https://blog.yingchi.io/tags/golang.html",
	"title": "golang",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.yingchi.io/posts.html",
	"title": "Posts",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.yingchi.io/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.yingchi.io/",
	"title": "Yingchi Blog",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.yingchi.io/posts/2020/8/go-context.html",
	"title": "理解 Golang Context 机制",
	"tags": ["golang"],
	"description": "",
	"content": "在使用 Golang 的一些框架的时候，比如 Gin，每一个请求的 Handler 方法总是需要传递进去一个 context 对象，然后很多请求数据，比如请求参数，路径变量等都可以从中读出来，其实在这个使用过程中已经大体理解了这个 context 是个什么东西，但是对于其中的一些细节包括具体的使用方式还是缺乏了解，因此本文就针对 golang 里面的 context 概念进行简单的探讨。\nGoroutine 并发控制的几种方式 我们知道 Golang 是一门擅长高并发的编程语言，可以通过 Goroutine 快速地创建并发任务，但是如何有效地管理这些执行的 Goroutine 是一个值得思考的问题。通常我们有下面几种方式实现 Goroutine 的控制：\n 使用 WaitGroup，根 goroutine 通过 add() 来记录已经开启的 Goroutine 数量，通过 wait() 来等待执行任务的 goroutine 的 done()，实现同步的工作； 使用 for/select + stop channel，通过向 stop channel 中传递 stop signal 实现 goroutine 的结束； 使用 Context， 可以控制具有复杂层级关系的 goroutine 任务，此时使用前两种方式实现会比较复杂，使用 context 会更优雅；  Context 原理概述 Goroutine 的创建和调用关系往往是有着层级关系的，顶部的 Goroutine 应有办法主动关闭其下属的 Goroutine 的执行。为了实现这种关系，Context 结构也应该像一棵树，叶子节点须总是由根节点衍生出来的。\n第一个创建 Context 的 goroutine 被称为 root 节点：root 节点负责创建一个实现 Context 接口的具体对象，并将该对象作为参数传递至新拉起的 goroutine 作为其上下文。下游 goroutine 继续封装该对象并以此类推向下传递。\nContext 可以安全的被多个 goroutine 使用。开发者可以把一个 Context 传递给任意多个 goroutine 然后 cancel 这个 context 的时候就能够通知到所有的 goroutine。\nContext 接口源码\n// A Context carries a deadline, cancelation signal, and request-scoped values // across API boundaries. Its methods are safe for simultaneous use by multiple // goroutines. type Context interface { // Done returns a channel that is closed when this Context is canceled  // or times out.  Done() \u0026lt;-chan struct{} // Err indicates why this context was canceled, after the Done channel  // is closed.  Err() error // Deadline returns the time when this Context will be canceled, if any.  Deadline() (deadline time.Time, ok bool) // Value returns the value associated with key or nil if none.  Value(key interface{}) interface{} }   Done 方法在 Context 被取消或超时时返回一个 close 的 channel，close 的 channel 可以作为广播通知，告诉给 context 相关的函数要停止当前工作然后返回。\n  Err 方法返回 context 为什么被取消。\n  Deadline 返回 context 何时会超时。\n  Value 返回 context 相关的数据\n  根 Context 创建方法 顶部的Goroutine应有办法主动关闭其下属的Goroutine的执行（不然程序可能就失控了）。为了实现这种关系，Context结构也应该像一棵树，叶子节点须总是由根节点衍生出来的。\n有两种方式创建根 Context：\n context.Background() context.TODO()  context.Background() 创建根 context 的第一种方式。\n在顶层 goroutine 中通过调用 context.Background() 可以返回一个空 Context，这个 Context 是 所有 Context 的 root，不能够被cancel。\nctx, cancel := context.WithCancel(context.Background()) context.TODO() 创建根 context 的第二种方式。\n一般情况使用 Background() 方法创建根 context。TODO() 用于当前不确定使用何种 context，留待以后调整。\n注意：不要传递 nil 的 context，在不确定使用何种 context 的时候应该使用 context.TODO()\n子 Context 派生方法 父 context 被 cancel，那么它的派生 context 都会收到 cancel 信号，即 Done() 返回的 channel 读到数据。\n有四种方法派生 context ：\n func WithCancel(parent Context) (ctx Context, cancel CancelFunc) func WithValue(parent Context, key, val interface{}) Context func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc)  context.WithCancel() 最常用的一种 context 派生方式，接收一个父 context（可以是 background context，或其它 Context），返回一个派生的 context 以及一个用于控制的 cancel 函数对象。\nWithCancel 返回一个继承的 context，这个 context 在父 context 的 Done 被关闭时关闭自己的 Done 通道，或者在自己被 Cancel 的时候关闭自己的 Done。（注意：读关闭的 channel 返回类型零值）\nWithCancel 同时还返回一个取消函数 cancel，这个 cancel 用于取消当前的 Context，在父任务执行 cancel() 时，接收 context 的所有 goroutine 会从 Done() 返回的通道中读取到值从而退出。\nfunc job() { ctx, cancel := context.WithCancel(context.Background()) go doSomething(ctx) time.Sleep(5 * time.Second) cancel() } func doSomething(ctx context.Context) { for { time.Sleep(1 * time.Second) select { case \u0026lt;-ctx.Done(): fmt.Println(\u0026#34;done\u0026#34;) return default: fmt.Println(\u0026#34;working\u0026#34;) } } } context.WithValue() 派生一个携带信息的 context 用于传递。\n比如在 Request 中携带认证信息，携带用户数据等。\nWithValue(parent Context, key, val interface{}) 方法的参数包含三个部分：\n parent，用于派生子 context 的父 context； key，携带信息的 key，interface{} 类型； value，携带信息的 value，interface{} 类型，通常在接收到信息后通过断言（.(T)）将 value 转换成正确的类型使用；  接收 context 携带的信息可以使用 ctx.Value(K) 接收到 value（interface{}类型）\ncontext.WithTimeout() 派生一个带有超时机制的 context。\n达到 Timeout 时长后，该 context 以及该 context 的子 context 会收到 cancel 信号退出。\n当然，如果在 Timeout 时长内调用 cancel，则会提前发送 cancel 信号退出。\nctx, cancel := context.WithTimeout(parentCtx, 5*time.Second) context.WithDeadline() 派生一个带有绝对时限的 context，与 WithTimeout() 作用基本相同，仅仅是时间设定方式上不同。\n达到 deadline 设定的时间后，该 context 以及该 context 的子 context 会收到 cancel 信号退出。\n当然，如果在 deadline 之前调用 cancel，则会提前发送 cancel 信号退出。\nctx, cancel := context.WithDeadline(parentCtx, time.Now().Add(5*time.Second)) 层级 Context 间的传递与控制  生命周期：Context 对象的生命周期一般仅为一个请求的处理周期。即针对一个请求创建一个 Context 变量（它为 Context 树的根）；在请求处理结束后，撤销此 ctx 变量，释放资源。 传递方式：每次创建一个 Goroutine，要么将原有的 Context 传递给 Goroutine，要么创建一个子 Context 并传递给 Goroutine。 安全读写：Context能灵活地存储不同类型、不同数目的值，并且使多个 Goroutine 安全地读写其中的值。 控制权：当通过父 Context 对象创建子 Context 对象时，可同时获得子 Context 的一个 Cancel 函数对象，这样就获得了对子任务的控制权。  使用原则  传递 Context 时，不应把 Context 放入 struct，而应该显式地传入函数，并且放在参数列表第一个位置，通常命名为 ctx； 不要传递 nil 的 Context，在不确定的时候应该传递 context.TODO()； 使用 context 的 Value 相关方法时只应该用于传递和请求相关的元数据，不要用它传递一些可选参数； 同一个 context 可以传递到不同的 goroutine 中，且在多个 goroutine 可以安全访问；  参考  https://www.sohu.com/a/302713513_99930294 https://www.cnblogs.com/zhangboyu/p/7456606.html https://www.cnblogs.com/sunlong88/p/11272559.html https://www.bbsmax.com/A/1O5E3Dg3z7/ "
},
{
	"uri": "https://blog.yingchi.io/tags/kubernetes.html",
	"title": "kubernetes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.yingchi.io/posts/2020/8/k8s-flannel.html",
	"title": "循序渐进理解CNI机制与Flannel工作原理",
	"tags": ["kubernetes"],
	"description": "",
	"content": "CNI，它的全称是 Container Network Interface，即容器网络的 API 接口。kubernetes 网络的发展方向是希望通过插件的方式来集成不同的网络方案， CNI 就是这一努力的结果。CNI 只专注解决容器网络连接和容器销毁时的资源释放，提供一套框架，所以 CNI 可以支持大量不同的网络模式，并且容易实现。\n从网络模型到 CNI 在理解 CNI 机制以及 Flannel 等具体实现方案之前，首先要理解问题的背景，这里从 kubernetes 网络模型开始回顾。\n从底层网络来看，kubernetes 的网络通信可以分为三层去看待：\n Pod 内部容器通信； 同主机 Pod 间容器通信； 跨主机 Pod 间容器通信；  对于前两点，其网络通信原理其实不难理解。\n 对于 Pod 内部容器通信，由于 Pod 内部的容器处于同一个 Network Namespace 下（通过 Pause 容器实现），即共享同一网卡，因此可以直接通信。 对于同主机 Pod 间容器通信，Docker 会在每个主机上创建一个 Docker0 网桥，主机上面所有 Pod 内的容器全部接到网桥上，因此可以互通。  而对于第三点，跨主机 Pod 间容器通信，Docker 并没有给出很好的解决方案，而对于 Kubernetes 而言，跨主机 Pod 间容器通信是非常重要的一项工作，但是有意思的是，Kubernetes 并没有自己去解决这个问题，而是专注于容器编排问题，对于跨主机的容器通信则是交给了第三方实现，这就是 CNI 机制。\nCNI，它的全称是 Container Network Interface，即容器网络的 API 接口。kubernetes 网络的发展方向是希望通过插件的方式来集成不同的网络方案， CNI 就是这一努力的结果。CNI 只专注解决容器网络连接和容器销毁时的资源释放，提供一套框架，所以 CNI 可以支持大量不同的网络模式，并且容易实现。平时比较常用的 CNI 实现有 Flannel、Calico、Weave 等。\nCNI 插件通常有三种实现模式：\n Overlay：靠隧道打通，不依赖底层网络； 路由：靠路由打通，部分依赖底层网络； Underlay：靠底层网络打通，强依赖底层网络；  在选择 CNI 插件时是要根据自己实际的需求进行考量，比如考虑 NetworkPolicy 是否要支持 Pod 网络间的访问策略，可以考虑 Calico、Weave；Pod 的创建速度，Overlay 或路由模式的 CNI 插件在创建 Pod 时比较快，Underlay 较慢；网络性能，Overlay 性能相对较差，Underlay 及路由模式相对较快。\nFlannel 工作原理 CNI 中经常见到的解决方案是 Flannel，由CoreOS推出，Flannel 采用的便是上面讲到的 Overlay 网络模式。\nOverlay 网络简介 Overlay 网络 (overlay network) 属于应用层网络，它是面向应用层的，不考虑网络层，物理层的问题。\n具体而言， Overlay 网络是指建立在另一个网络上的网络。该网络中的结点可以看作通过虚拟或逻辑链路而连接起来的。虽然在底层有很多条物理链路，但是这些虚拟或逻辑链路都与路径一一对应。例如：许多P2P网络就是 Overlay 网络，因为它运行在互连网的上层。 Overlay 网络允许对没有IP地址标识的目的主机路由信息，例如：Freenet 和DHT（分布式哈希表）可以路由信息到一个存储特定文件的结点，而这个结点的IP地址事先并不知道。\nOverlay 网络被认为是一条用来改善互连网路由的途径，让二层网络在三层网络中传递，既解决了二层的缺点，又解决了三层的不灵活。\nFlannel的工作原理 Flannel 实质上就是一种 Overlay 网络，也就是将 TCP 数据包装在另一种网络包里面进行路由转发和通信，目前已经支持 UDP、VxLAN、AWS VPC 和 GCE 路由等数据转发方式。\nFlannel会在每一个宿主机上运行名为 flanneld 代理，其负责为宿主机预先分配一个子网，并为 Pod 分配IP地址。Flannel 使用Kubernetes 或 etcd 来存储网络配置、分配的子网和主机公共IP等信息。数据包则通过 VXLAN、UDP 或 host-gw 这些类型的后端机制进行转发。\nFlannel 规定宿主机下各个Pod属于同一个子网，不同宿主机下的Pod属于不同的子网。\nFlannel 工作模式 支持3种实现：UDP、VxLAN、host-gw，\n UDP 模式：使用设备 flannel.0 进行封包解包，不是内核原生支持，频繁地内核态用户态切换，性能非常差； VxLAN 模式：使用 flannel.1 进行封包解包，内核原生支持，性能较强； host-gw 模式：无需 flannel.1 这样的中间设备，直接宿主机当作子网的下一跳地址，性能最强；  host-gw的性能损失大约在10%左右，而其他所有基于VxLAN“隧道”机制的网络方案，性能损失在20%~30%左右。\nUDP 模式 官方已经不推荐使用 UDP 模式，性能相对较差。\nUDP 模式的核心就是通过 TUN 设备 flannel0 实现。TUN设备是工作在三层的虚拟网络设备，功能是：在操作系统内核和用户应用程序之间传递IP包。 相比两台宿主机直接通信，多出了 flanneld 的处理过程，这个过程，使用了 flannel0 这个TUN设备，仅在发出 IP包的过程中经过多次用户态到内核态的数据拷贝（linux的上下文切换代价比较大），所以性能非常差 原理如下： 以flannel0为例，操作系统将一个IP包发给flannel0，flannel0把IP包发给创建这个设备的应用程序：flannel进程（内核态-\u0026gt;用户态） 相反，flannel进程向flannel0发送一个IP包，IP包会出现在宿主机的网络栈中，然后根据宿主机的路由表进行下一步处理（用户态-\u0026gt;内核态） 当IP包从容器经过docker0出现在宿主机，又根据路由表进入flannel0设备后，宿主机上的flanneld进程就会收到这个IP包\nflannel管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的“子网”，子网与宿主机的对应关系，存在Etcd中（例如Node1的子网是100.96.1.0/24，container-1的IP地址是100.96.1.2）\n当flanneld进程处理flannel0传入的IP包时，就可以根据目的IP地址（如100.96.2.3），匹配到对应的子网（比如100.96.2.0/24），从Etcd中找到这个子网对应的宿主机的IP地址（10.168.0.3）\n然后 flanneld 在收到container-1给container-2的包后，把这个包直接封装在UDP包里，发送给Node2（UDP包的源地址，就是Node1，目的地址是Node2）\n每台宿主机的flanneld都监听着8285端口，所以flanneld只要把UDP发给Node2的8285端口就行了。然后Node2的flanneld再把IP包发送给它所管理的TUN设备flannel0，flannel0设备再发给docker0\nVxLAN模式 VxLAN，即Virtual Extensible LAN（虚拟可扩展局域网），是Linux本身支持的一网种网络虚拟化技术。VxLAN可以完全在内核态实现封装和解封装工作，从而通过“隧道”机制，构建出 Overlay 网络（Overlay Network）\nVxLAN的设计思想是： 在现有的三层网络之上，“覆盖”一层虚拟的、由内核VxLAN模块负责维护的二层网络，使得连接在这个VxLAN二层网络上的“主机”（虚拟机或容器都可以），可以像在同一个局域网（LAN）里那样自由通信。 为了能够在二层网络上打通“隧道”，VxLAN会在宿主机上设置一个特殊的网络设备作为“隧道”的两端，叫VTEP：VxLAN Tunnel End Point（虚拟隧道端点） 原理如下：\nflannel.1设备，就是VxLAN的VTEP，即有IP地址，也有MAC地址 与UDP模式类似，当container-发出请求后，上的地址10.1.16.3的IP包，会先出现在docker网桥，再路由到本机的flannel.1设备进行处理（进站），为了能够将“原始IP包”封装并发送到正常的主机，VxLAN需要找到隧道的出口：宿主机的VTEP设备，这个设备信息，由宿主机的flanneld进程维护\nVTEP设备之间通过二层数据桢进行通信 源VTEP设备收到原始IP包后，在上面加上一个目的MAC地址，封装成一个导去数据桢，发送给目的VTEP设备（获取 MAC地址需要通过三层IP地址查询，这是ARP表的功能） 封装过程只是加了一个二层头，不会改变“原始IP包”的内容 这些VTEP设备的MAC地址，对宿主机网络来说没什么实际意义，称为内部数据桢，并不能在宿主机的二层网络传输，Linux内核还需要把它进一步封装成为宿主机的一个普通的数据桢，好让它带着“内部数据桢”通过宿主机的eth0进行传输，Linux会在内部数据桢前面，加上一个我死的VxLAN头，VxLAN头里有一个重要的标志叫VNI，它是VTEP识别某个数据桢是不是应该归自己处理的重要标识。 在Flannel中，VNI的默认值是1，这也是为什么宿主机的VTEP设备都叫flannel.1的原因\n一个flannel.1设备只知道另一端flannel.1设备的MAC地址，却不知道对应的宿主机地址是什么。 在linux内核里面，网络设备进行转发的依据，来自FDB的转发数据库，这个flannel.1网桥对应的FDB信息，是由flanneld进程维护的 linux内核再在IP包前面加上二层数据桢头，把Node2的MAC地址填进去。这个MAC地址本身，是Node1的ARP表要学习的，需 Flannel维护，这时候Linux封装的“外部数据桢”的格式如下 然后Node1的flannel.1设备就可以把这个数据桢从eth0发出去，再经过宿主机网络来到Node2的eth0 Node2的内核网络栈会发现这个数据桢有VxLAN Header，并且VNI为1，Linux内核会对它进行拆包，拿到内部数据桢，根据VNI的值，所它交给Node2的flannel.1设备\nhost-gw模式 Flannel 第三种协议叫 host-gw (host gateway)，这是一种纯三层网络的方案，性能最高，即 Node 节点把自己的网络接口当做 pod 的网关使用，从而使不同节点上的 node 进行通信，这个性能比 VxLAN 高，因为它没有额外开销。不过他有个缺点， 就是各 node 节点必须在同一个网段中 。\nhowt-gw 模式的工作原理，就是将每个Flannel子网的下一跳，设置成了该子网对应的宿主机的 IP 地址，也就是说，宿主机（host）充当了这条容器通信路径的“网关”（Gateway），这正是 host-gw 的含义 所有的子网和主机的信息，都保存在 Etcd 中，flanneld 只需要 watch 这些数据的变化 ，实时更新路由表就行了。 核心是IP包在封装成桢的时候，使用路由表的“下一跳”设置上的MAC地址，这样可以经过二层网络到达目的宿主机。\n另外，如果两个 pod 所在节点在同一个网段中 ，可以让 VxLAN 也支持 host-gw 的功能， 即直接通过物理网卡的网关路由转发，而不用隧道 flannel 叠加，从而提高了 VxLAN 的性能，这种 flannel 的功能叫 directrouting。\nFlannel 通信过程描述 以 UDP 模式为例，跨主机容器间通信过程如下图所示：\n上图是 Flannel 官网提供的在 UDP 模式下一个数据包经过封包、传输以及拆包的示意图，从这个图片中可以看出两台机器的 docker0 分别处于不同的段：10.1.20.1/24 和 10.1.15.1/24 ，如果从 Web App Frontend1 pod（10.1.15.2）去连接另一台主机上的 Backend Service2 pod（10.1.20.3），网络包从宿主机 192.168.0.100 发往 192.168.0.200，内层容器的数据包被封装到宿主机的 UDP 里面，并且在外层包装了宿主机的 IP 和 mac 地址。这就是一个经典的 overlay 网络，因为容器的 IP 是一个内部 IP，无法从跨宿主机通信，所以容器的网络互通，需要承载到宿主机的网络之上。\n以 VxLAN 模式为例。\n在源容器宿主机中的数据传递过程：\n1）源容器向目标容器发送数据，数据首先发送给 docker0 网桥\n在源容器内容查看路由信息：\n$ kubectl exec -it -p {Podid} -c {ContainerId} -- ip route 2）docker0 网桥接受到数据后，将其转交给flannel.1虚拟网卡处理\ndocker0 收到数据包后，docker0 的内核栈处理程序会读取这个数据包的目标地址，根据目标地址将数据包发送给下一个路由节点： 查看源容器所在Node的路由信息：\n$ ip route 3）flannel.1 接受到数据后，对数据进行封装，并发给宿主机的eth0\nflannel.1收到数据后，flannelid会将数据包封装成二层以太包。 Ethernet Header的信息：\n From:{源容器flannel.1虚拟网卡的MAC地址} To:{目录容器flannel.1虚拟网卡的MAC地址}  4）对在flannel路由节点封装后的数据，进行再封装后，转发给目标容器Node的eth0；\n由于目前的数据包只是vxlan tunnel上的数据包，因此还不能在物理网络上进行传输。因此，需要将上述数据包再次进行封装，才能源容器节点传输到目标容器节点，这项工作在由linux内核来完成。 Ethernet Header的信息：\n From:{源容器Node节点网卡的MAC地址} To:{目录容器Node节点网卡的MAC地址}  IP Header的信息：\n From:{源容器Node节点网卡的IP地址} To:{目录容器Node节点网卡的IP地址}  通过此次封装，就可以通过物理网络发送数据包。\n在目标容器宿主机中的数据传递过程：\n5）目标容器宿主机的eth0接收到数据后，对数据包进行拆封，并转发给flannel.1虚拟网卡；\n6）flannel.1 虚拟网卡接受到数据，将数据发送给docker0网桥；\n7）最后，数据到达目标容器，完成容器之间的数据通信。\n参考  https://blog.csdn.net/alauda_andy/article/details/80132922 https://blog.csdn.net/liukuan73/article/details/78883847 https://www.kubernetes.org.cn/6908.html https://www.cnblogs.com/chenqionghe/p/11718365.html https://zhuanlan.zhihu.com/p/105942115 https://www.jianshu.com/p/165a256fb1da https://www.cnblogs.com/ssgeek/p/11492150.html https://www.cnblogs.com/sandshell/p/11777312.html "
},
{
	"uri": "https://blog.yingchi.io/posts/2020/7/k8s-cm-informer.html",
	"title": "解读 kubernetes Controller Manager 工作原理",
	"tags": ["kubernetes"],
	"description": "",
	"content": "kubernetes master 节点最重要的三个组件是：kube-apiserver、kube-controller-manager、kube-scheduler，分别负责 kubernetes 集群的资源访问入口、集群状态管理、资源调度。\n这篇文章的主角就是其中的 kube-controller-manager 组件，分析一下它以及其核心组件 informer 是如何有效管理集群状态的。\nController Manager \u0026amp; Controller 工作原理概述 我们都知道 kubernetes 中管理资源的方式比较简单，通常就是写一个 YAML 清单，简单的可以通过 kubectl 命令直接解决，在这个过程中，我们定义了某个资源的「期望状态」，比如 YAML 清单文件中的 spec 字段，Deployment 的 YAML 中的 spec 字段可能定义了期望的 replicas，我们期望集群的某个 pod 的副本数维持在某个数量上，当我们提交清单给集群，kubernetes 会在一段时间内将集群中的某些资源调整至我们期望的状态；亦或是另一个场景，集群中某个 Pod 挂掉了，或者我们将 Pod 从某个 Worker Node 上驱逐了，然后我们没有做任何操作，Pod 又会自动重建，并且达到指定的副本数，这是很常见的场景。\n上面说的这些资源的状态管理是由谁实现的呢？没错，就是 Controller Manager，Controller Manager 是 Kubernetes 的灵魂组件之一，可以说通过定义资源期望状态实现集群资源编排管理的思想其底层就是依赖 Controller Manager 这个组件。\nController Manager 的作用简而言之：保证集群中各种资源的实际状态（status）和用户定义的期望状态（spec）一致。\n按照官方定义：kube-controller-manager 运行控制器，它们是处理集群中常规任务的后台线程。\nController Manager 就是集群内部的管理控制中心，刚才说 Controller Manager 的作用是保证集群中各种资源的实际状态和用户定义的期望状态一致，但是如果出现不一致的情况怎么办？是由 Controller Manager 自己来对各种资源进行调整吗？\n这时候就要说到 Controller 的概念了，之所以叫 Controller Manager，是因为 Controller Manager 由负责不同资源的多个 Controller 构成，如 Deployment Controller、Node Controller、Namespace Controller、Service Controller 等，这些 Controllers 各自明确分工负责集群内资源的管理。\n如图，Controller Manager 发现资源的实际状态和期望状态有偏差之后，会触发相应 Controller 注册的 Event Handler，让它们去根据资源本身的特点进行调整。\n比如当通过 Deployment 创建的某个 Pod 发生异常退出时，Deployment Controller 便会接受并处理该退出的 Event，并创建新的 Pod 来维持期望副本数。这样设计的原因也很好理解，可以将 Controller Manager 与具体的状态管理工作相解耦，因为不同的资源对于状态的管理多种多样，Deployment Controller 关注 Pod 副本数，而 Service 则关注 Service 的 IP、Port 等等。\nclient-go Controller Manager 中一个很关键的部分就是 client-go，client-go 在 controller manager 中起到向 controllers 进行事件分发的作用。目前 client-go 已经被单独抽取出来成为一个项目了，除了在 kubernetes 中经常被用到，在 kubernetes 的二次开发过程中会经常用到 client-go，比如可以通过 client-go 开发自定义 controller。\nclient-go 包中一个非常核心的工具就是 informer，informer 可以让与 kube-apiserver 的交互更加优雅。\ninformer 主要功能可以概括为两点：\n 资源数据缓存功能，缓解对 kube-apiserver 的访问压力； 资源事件分发，触发事先注册好的 ResourceEventHandler；  Informer 另外一块内容在于提供了事件 handler 机制，并会触发回调，这样 Controller 就可以基于回调处理具体业务逻辑。因为 Informer 通过 List、Watch 机制可以监控到所有资源的所有事件，因此只要给 Informer 添加ResourceEventHandler 实例的回调函数实例取实现 OnAdd(obj interface{})、 OnUpdate(oldObj, newObj interface{})  和  OnDelete(obj interface{}) 这三个方法，就可以处理好资源的创建、更新和删除操作\nclient-go 工作机制 上图是官方给出的 client-go 与自定义 controller 的实现原理。\nReflactor 反射器，具有以下几个功能：\n 采用 List、Watch 机制与 kube-apiserver 交互，List 短连接获取全量数据，Watch 长连接获取增量数据； 可以 Watch 任何资源包括 CRD； Watch 到的增量 Object 添加到 Delta FIFO 队列，然后 Informer 会从队列里面取数据;  Informer Informer 是 client-go 中较为核心的一个模块，其主要作用包括如下两个方面：\n 同步数据到本地缓存。Informer 会不断读取 Delta FIFO 队列中的 Object，在触发事件回调之前先更新本地的 store，如果是新增 Object，如果事件类型是 Added（添加对象），那么 Informer 会通过 Indexer 的库把这个增量里的 API 对象保存到本地的缓存中，并为它创建索引。之后通过 Lister 对资源进行 List / Get 操作时会直接读取本地的 store 缓存，通过这种方式避免对 kube-apiserver 的大量不必要请求，缓解其访问压力； 根据对应的事件类型，触发事先注册好的 ResourceEventHandler。client-go 的 informer 模块启动时会创建一个 shardProcessor，各种 controller（如 Deployment Controller、自定义 Controller\u0026hellip;）的事件 handler 注册到 informer 的时候会转换为一个 processorListener 实例，然后 processorListener 会被 append 到 shardProcessor 的 Listeners 切片中，shardProcessor 会管理这些 listeners。  processorListener 的重要作用就是当事件到来时触发对应的处理方法，因此不停地从 nextCh 中拿到事件并执行对应的 handler。sharedProcessor 的职责便是管理所有的 Handler 以及分发事件，而真正做分发工作的是 distribute 方法。\n梳理一下这中间的过程：\n Controller 将 Handler 注册给 Informer； Informer 通过 sharedProcessor 维护了所有转换为 processorListener 的 Handler； Informer 收到事件时，通过 sharedProcessor.distribute 将事件分发下去； Controller 被触发对应的 Handler 来处理自己的逻辑。  Reflactor 启动后会执行一个 processLoop 死循环，Loop 中不停地将 Delta FIFO 队列中的事件 Pop 出来，Pop 时会取出该资源的所有事件，并交给 sharedIndexInformer 的 HandleDeltas 方法（创建 controller 时赋值给了 config.Process，传递到 Pop 参数的处理函数中 Pop(PopProcessFunc(c.config.Process))），HandleDeltas 调用了 processor.distribute 完成事件的分发。\n在注册的 ResourceEventHandler 回调函数中，只是做了一些很简单的过滤，然后将关心变更的 Object 放到 workqueue 里面。之后 Controller 从 workqueue 里面取出 Object，启动一个 worker 来执行自己的业务逻辑，通常是对比资源的当前运行状态与期望状态，做出相应的处理，实现运行状态向期望状态的收敛。\n注意，在 worker 中就可以使用 lister 来获取 resource，这个时候不需要频繁的访问 kube-apiserver 了，对于资源的 List / Get 会直接访问 informer 本地 store 缓存，apiserver 中资源的的变更都会反映到这个缓存之中。同时，LocalStore 会周期性地把所有的 Pod 信息重新放到 DeltaFIFO 中。\n参考  http://dockone.io/article/9557 https://www.cnblogs.com/lovezbs/p/13172842.html https://zhuanlan.zhihu.com/p/59660536 https://www.jianshu.com/p/1e2e686fe363 https://www.lizenghai.com/archives/50034.html#1 https://www.jianshu.com/p/d17f70369c35 https://www.jianshu.com/p/ac9179007fe2 https://blog.hdls.me/15763918313590.html https://blog.ihypo.net/15763910382218.html "
},
{
	"uri": "https://blog.yingchi.io/posts/2020/7/k8s-rbac.html",
	"title": "一文读懂 Kubernetes RBAC 机制",
	"tags": ["kubernetes"],
	"description": "",
	"content": "之前在做 PaaS 平台开发时涉及到租户的权限管理，考虑到 Kubernetes 默认提供了 RBAC（基于角色的访问控制）机制，于是想如何利用好 Kubernetes 的 RBAC 来实现。但是开始学习这块儿知识的时候还是遇到了一些问题，比如 Role 和 ClusterRole，Role Binding 和 ClusterRoleBinding，很多概念是比较模糊的，随着后来深入的学习了解和实践才算理清它们之间的关系，这篇文章就是分享一下这期间学到的内容。\n什么是 RBAC？ RBAC（基于角色的访问控制） RBAC，Role-Based Access Control，即基于角色的访问控制，通过自定义具有某些特定 Permission 的 Role，然后将 Role 和特定的 Subject（user，group，serviceaccounts\u0026hellip;)关联起来已达到权限控制的目的。\nRBAC 中有三个比较重要的概念：\n Role：角色，本质是一组规则权限的集合，注意：RBAC 中，Role 只声明授予权限，而不存在否定规则； Subject：被作用者，包括 user，group，通俗来讲就是认证机制中所识别的用户； RoleBinding：定义了“Role”和“Subject”的绑定关系，也就是将用户以及操作权限进行绑定；  RBAC 其实就是通过创建角色(Role），通过 RoleBinding 将被作用者（subject）和角色（Role）进行绑定。下图是 RBAC 中的几种绑定关系：\nKubernetes RBAC 现在以 Kubernetes 的视角重新来看 RBAC 的实现。\nKubernetes 中实现 RBAC 角色创建、角色绑定整个流程还是比较清晰的，RBAC 的配置都是以资源配置的形式呈现给管理员，我们只需要定义一些配置文件即可，当然在这其中会涉及到对一些配置的理解。首先来看 Kubernetes 中 RBAC 配置关系图：\n如图可知，实现 Kubernetes 的自定义 RBAC 过程，主要涉及到几个概念，其实一开始已经提到过了，这不过在 Kubernetes 中定义了更具体的概念，这是重点需要关注的。\nSubject 主体，kubernetes 中的 Subject 包括了 User、Group、Service Account，前两个好理解，Service Account 其实就是区别开 User Account 的，Kubernetes 中的资源也是可以当做一个 Subject 的，比如一个 Pod 需要操作集群资源时，它采用的就是 Service Account。\nKubernetes 并没有实现对于 User Account 的直接管理，需要管理员自行手动创建 User Account，本文不针对这一块儿展开说明，大家可以查阅相关文章进行学习。\nRole 角色，角色本质就是一组规则权限的集合，通过与具体的 Subject 绑定，从而赋予 Subject 相应的一组权限。\n在 Kubernetes 中，Role 分为两类：\n Role：角色，可以理解为 Namespace Role，Role 在定义时要指定 Namespace，即这个 Role 是定义在 Namespace 内部，因此它所定义的规则权限集合也是针对于其定义所在的 Namespace 下的资源。 ClusterRole：集群角色，顾名思义，这种类型的 Role 是定义在整个 Cluster 下的，定义 Cluster Role 时不需要指定具体的 Namespace。  Role 示例：\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: ns-a # 定义于 namespace 下 name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] ClusterRole 示例：\nkind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] 注意：ClusterRole 并不代表权限控制的范围是整个 Cluster，还与之后要讲的 Binding 类型有关系，这里的 ClusterRole 应该理解为一种全局量，可以作用于某个 Namespace 下（起到复用的作用），可以作用于 Cluster 范围下。\n每个 Role 在定义时关键的一步要指定该 Role 所具有的 rules，Kubernetes 中权限规则需要指定以下几点：\n apiGroups：资源组，\u0026quot;\u0026quot; 缺省为 core 组资源，其它诸如 apps 等； resources：资源，比如 pods、deployments、services、secrets 等； verbs：操作动词，如 get、list、watch、create、delete、update 等。  Binding 绑定，单从概念上很好理解，就是把定义好规则权限的 Role 绑定到指定的 Subject 上，从而赋予 Subject 相应的一系列权限。\nkubernetes 中 Binding 分为两种：\n RoleBinding：绑定 Role/ClusterRole 到 Subject，生效于具体的 Namespace 范围资源； ClusterRoleBinding：绑定 ClusterRole 到 Subject，生效于 Cluster 范围资源；  再次强调：\nKubernetes 中权限控制的范围（Namespace/Cluster）是由 Binding 的类型决定的，而不是根据 Role 和 ClusterRole 决定的，这一点一定要记住。\n作用于 Namespace 范围的 RBAC 方案分析 方法1：Role + RoleBinding 的组合 如上图，这种方式是比较直观的，因为之前说过 Kubernetes 中的 Role 应该理解为 Namespace Role，它是定义在 Namespace 下面的，因此如果要实现 Namespace 范围内的 Subject 与 Role 的绑定，无非就是通过配置一个 RoleBinding 把两者联系起来。\n配置示例：\nkind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: pods-reader-binding namespace: ns-a subjects: - kind: User name: yingchi apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io 方法2：ClusterRole + RoleBinding 的组合（推荐） 一听 ClusterRole，很多朋友会潜意识认为是关于集群范围下的权限控制，但是实际上，权限控制的范围和 Role 的类型没有直接的关系，ClusterRole，它的本质是一个定义在全局的角色，可以作用于 Namespace 下，可以被多个 Namespace 复用，也可以作用于 Cluster 下。\n我们现在需要实现 Namespace 范围内绑定 Subject \u0026amp; Role，就是将 ClusterRole 的权限范围控制在 Namespace 层面，因此采用 RoleBinding 将 Subject 与 ClusterRole 即可实现。\n配置示例：\nkind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: pods-reader-binding namespace: ns-a subjects: - kind: User name: yingchi apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: pod-reader apiGroup: rbac.authorization.k8s.io 方法分析 为什么会推荐使用 ClusterRole + RoleBinding 的组合呢？\n思考一个问题，假如设计一个 PodReader Role，作用就是有 get/list/watch 当前 Namespace 下 Pod 的权限，如果两个 namespace 都需要这么一个 Role，那么应该怎么设计呢？\n采用上面讲的第一种方法的话，每个 namespace 需要定义一个 PodReader 的 Role：\n这样做有个很明显的缺点，PodReader 的规则权限其实在每个 Namespace 下都是一样的，但是由于 Role 的限制，必须在每个 Namespace 下单独定义，资源浪费，过程也不优雅，如何去复用 PodReader 这个 Role 呢？于是，才有了第二种方法：\n如图，也就是将 PodReader 定义为 ClusterRole，然后通过 RoleBinding 在不同 Namespace 下绑定不同的 Subject 到同一个 ClusterRole。\n小结 如果自定义的 Role 在很多个 Namespace 中都会用到，那么推荐采用 ClusterRole + RoleBinding 的组合，实现 Role 的复用，如果真的是某个 Namespace 必须要单独定义的私有 Role，再去用 Role + RoleBinding 的组合，但是这样的话 Role 就会比较局限。\n作用于集群范围的 RBAC 方案分析 方案只有一个，就是通过 ClusterRoleBinding + ClusterRole：\n注意我在图中画的 ClusterRoleBinding 的位置，之前在介绍 Binding 概念的时候就提到一句话：Kubernetes 中权限控制的范围（Namespace/Cluster）是由 Binding 的类型决定的，而不是根据 Role 和 ClusterRole 决定的。想要实现作用于集群范围的 RBAC，一定是通过 ClusterRoleBinding，并且要注意的是，通过 ClusterRoleBinding 方式进行绑定时，只允许绑定到 ClusterRole 角色，可以简单理解为全局方法只能去用 ClusterRole 这种全局量。\n配置示例：\nkind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cluster-admin-biding subjects: - kind: User name: yingchi apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io 总结 关于 Kubernetes RBAC 机制的学习，首先要从基本的 RBAC 概念出发，理解 RBAC 这种权限管理机制的本质思想，即主体-角色-权限三者的关系，在此基础上，针对 Kubernetes 对于 RBAC 的实现进行理解学习，核心是要理解 Role 和 ClusterRole 的本质概念与作用，比如为什么在复用角色时要使用 ClusterRole。然后，深刻理解 RoleBinding 和 ClusterRoleBinding 的区别，要记住，Kubernetes RBAC 中对于权限控制的范围不是由 Role 决定的，而是由 Binding 的类型决定的。\n参考  https://www.xiaoman.cn/detail/150 https://www.cnblogs.com/panwenbin-logs/p/10046572.html https://blog.51cto.com/wzlinux/2334119 https://www.cnblogs.com/peng-zone/p/11689017.html "
},
{
	"uri": "https://blog.yingchi.io/posts/2020/7/k8s-authn-authz.html",
	"title": "Kubernetes API Server 认证与授权机制",
	"tags": ["kubernetes"],
	"description": "",
	"content": "kube-apiserver 是 kubernetes 的网关性质的组件，是 kubernetes 集群资源操作的唯一入口，因此像认证与授权等一些过程很明显是要基于这个组件实施。kubernetes 集群的所有操作基本上都是通过 apiserver 这个组件进行的，它提供 HTTP RESTful 形式的 API 供集群内外客户端调用。kubernetes 对于访问 API 来说提供了三个步骤的安全措施：认证、授权、准入控制，当用户使用 kubectl，client-go 或者 REST API 请求 apiserver 时，都要经过这三个步骤的校验。\n 认证解决的问题是识别用户的身份； 授权是明确用户具有哪些权限； 准入控制是作用于 kubernetes 中的资源对象。  Kubernetes API Server 认证机制（Authentication） 一旦TLS连接建立，请求就进入到身份认证阶段，在这一阶段，请求有效负载由一个或多个认证器模块检查。\n认证模块时管理员在集群创建过程中配置的，一个集群可能有多个认证模块配置，每个模块会依次尝试认证， 直到其中一个认证成功。\n在主流的认证模块中会包括客户端证书、密码、plain tokens、bootstrap tokens以及JWT tokens（用于service account）。客户端证书的使用是默认的并且是最常见的方案。\nkubernetes 目前所有的认证策略如下所示：\n X509 client certs Static Token File Bootstrap Tokens Static Password File Service Account Tokens OpenId Connect Tokens Webhook Token Authentication Authticating Proxy Anonymous requests User impersonation Client-go credential plugins  Kubernetes 常用认证机制 X509 client certs X509 client certs 认证方式是用在一些客户端访问 apiserver 以及集群组件之间访问时使用，比如 kubectl 请求 apiserver 时。\n适用对象：外部用户\nX509是一种数字证书的格式标准，现在 HTTPS 依赖的 SSL 证书使用的就是使用的 X509 格式。X509 客户端证书认证方式是 kubernetes 所有认证中使用最多的一种，相对来说也是最安全的一种，kubernetes 的一些部署工具 kubeadm、minkube 等都是基于证书的认证方式。客户端证书认证叫作 TLS 双向认证，也就是服务器客户端互相验证证书的正确性，在都正确的情况下协调通信加密方案。目前最常用的 X509 证书制作工具有 openssl、cfssl 等。\nService Account Tokens serviceaccounts 是用在 pod 中访问 apiserver 时进行认证的，比如使用自定义 controller 时。\n适用对象：内部用户\n有些情况下，我们希望在 pod 内部访问 apiserver，获取集群的信息，甚至对集群进行改动。针对这种情况，kubernetes 提供了一种特殊的认证方式：serviceaccounts。\nserviceaccounts 是面向 namespace 的，每个 namespace 创建的时候，kubernetes 会自动在这个 namespace 下面创建一个默认的 serviceaccounts；并且这个 serviceaccounts 只能访问该 namespace 的资源。\nserviceaccounts 和 pod、service、deployment 一样是 kubernetes 集群中的一种资源，用户也可以创建自己的 serviceaccounts。\nserviceaccounts 主要包含了三个内容：namespace、token 和 ca，每个 serviceaccounts 中都对应一个 secrets，namespace、token 和 ca 信息都是保存在 secrets 中且都通过 base64 编码的。namespace 指定了 pod 所在的 namespace，ca 用于验证 apiserver 的证书，token 用作身份验证，它们都通过 mount 的方式保存在 pod 的文件系统中，其三者都是保存在 /var/run/secrets/kubernetes.io/serviceaccount/目录下。\nKubernetes API Server 授权机制（Authorization） 请求经过认证之后，下一步就是确认这一操作是否被允许执行，即授权。\n对于授权一个请求，Kubernetes主要关注三个方面：\n 请求者的用户名 请求动作 动作影响的对象  用户名从嵌入 token 的头部中提取，动作是映射到CRUD操作的HTTP动词之一（如 GET、POST、PUT、DELETE），对象是其中一个有效的 Kubernetes 资源对象。\nKubernetes基于一个存在策略来决定授权。默认情况下，Kubernetes遵循封闭开放的理念，这意味着需要一个明确的允许策略才可以访问资源。\n与身份认证类似，授权也是基于一个或多个模块配置的，如ABAC模式、RBAC模式以及Webhook模式。当管理员创建集群时，他们配置与API sever集成的授权模块。如果多个模块都在使用，Kubernetes会检查每个模块并且如果其中任一模块授权了请求，则请求授权通过。如果所有模块全部拒绝请求，则请求被拒绝（HTTP状态码403）。\nkubernetes 目前支持如下四种授权机制：\n Node ABAC RBAC Webhook  kubernetes 常用授权机制 RBAC（基于角色的访问控制） RBAC，Role-Based Access Control，即基于角色的访问控制，通过自定义角色并将角色和特定的 user，group，serviceaccounts 关联起来已达到权限控制的目的。\nRBAC 中有三个比较重要的概念：\n Role：角色，它其实是一组规则，定义了一组对 Kubernetes API 对象的操作权限； Subject：被作用者，包括 user，group，serviceaccounts，通俗来讲就是认证机制中所识别的用户； RoleBinding：定义了“Role”和“Subject”的绑定关系，也就是将用户以及操作权限进行绑定；  RBAC 其实就是通过创建角色(Role），通过 RoleBinding 将被作用者（subject）和角色（Role）进行绑定。下图是 RBAC 中的几种绑定关系：\n准入控制(Admission Control) 准入控制是请求的最后一个步骤，准入控制有许多内置的模块，可以作用于对象的 \u0026ldquo;CREATE\u0026rdquo;、\u0026ldquo;UPDATE\u0026rdquo;、\u0026ldquo;DELETE\u0026rdquo;、\u0026ldquo;CONNECT\u0026rdquo; 四个阶段。在这一过程中，如果任一准入控制模块拒绝，那么请求立刻被拒绝。一旦请求通过所有的准入控制器后就会写入对象存储中。\n服务网格 Istio 技术的 Sidecar 注入过程就是依赖于 Kubernetes 的准入控制实现的。\nAdmission Controller 是拦截 API Server 请求 (经过身份验证的) 的网关，并且可以修改请求对象或拒绝请求。简而言之，它可以认为是拦截器，类似 web 框架中的 middleware，是 Kubernetes API Server 用于拦截请求的一种手段。\nKubernetes 为什么引入 admission 这种机制？\n 虽然 Kubernetes 有 Authentication \u0026amp; Authorization 这种认证授权机制，Authentication \u0026amp; Authorization 运行在 filter 中，只能获取 http 请求 header 以及证书，并不能获取请求的 body。所以不可以对请求的对象进行任何操作，因为获取不到对象。 Admission Controller 运行在 API Server 的增删改查 handler 中，可以自然地操作 API resource。  API Server 接收到客户端请求后首先进行认证授权，认证授权通过后才会进行后续的 endpoint handler 处理，kube-apiserver 处理资源请求的流程如下：\n可以看到，在 Authentication \u0026amp; Authorization 之后请求就交由 Admission Controller 继续处理，这里涉及到 Admission 两个重要阶段，Mutating和Validating，它们的区别如下：\n Mutating：可以对请求内容进行修改； Validating：不允许修改请求内容，但可以根据请求的内容判断是继续执行该请求还是拒绝该请求；  参考  https://www.cnblogs.com/Dev0ps/p/10852445.html https://zhuanlan.zhihu.com/p/111245097 "
},
{
	"uri": "https://blog.yingchi.io/posts/2020/6/cka-note.html",
	"title": "2020 年 6 月 CKA 认证通过分享",
	"tags": ["kubernetes", "note"],
	"description": "",
	"content": " CKA (Certified Kubernetes Administrator)认证是由 CNCF 与 Linux Foundation 管理的与 Kubernetes 运维技能相关的一个认证，目前业内对于云原生这一块儿专门的认证还是比较少的。自己目前所做的大部分工作都与 Kubernetes 关系比较密切，因此就报名预约了 6 月份的认证，好好准备一段时间，最后成功通过了认证（证书详见文末）\n 考试大纲 CNCF 官网中找到 CKA 认证考试页面 https://www.cncf.io/certification/cka/ 可以看到基本的考试大纲，分值分配情况。\nThe online exam consists of a set of performance-based items (problems) to be solved in a command line and candidates have 3 hours to complete the tasks.\nThe Certification focuses on the skills required to be a successful Kubernetes Administrator in industry today. This includes these general domains and their weights on the exam:\n Application Lifecycle Management 8% Installation, Configuration \u0026amp; Validation 12% Core Concepts 19% Networking 11% Scheduling 5% Security 12% Cluster Maintenance 11% Logging / Monitoring 5% Storage 7% Troubleshooting 10%  可以看到考察的项目还是比较繁杂的，但是没办法，Kubernetes 本身就不是一个小框架，毕竟是 Google 这么多年来的积累精华，因此考察这些内容也是可以理解的。\n前期准备 准备时间我大概用了两个星期，两个星期的时间除了最后几天集中精力练习了一段时间，其它时候都是随缘练习\u0026hellip; 这也导致最后认证的时候过于自信了，比预计的分数少了十几分\u0026hellip;我现在都不知道究竟错在哪里了，当时感觉除了有个 8 分 Troubleshooting 的没做好，其它都是 OK 的\u0026hellip;\n整个准备过程的话主要分为几个部分：\n Kubernetes 架构； Kubernetes 核心资源概念； 集群环境下训练；  Kubernetes 架构，这一块儿的话主要是看书、看视频，这里推荐华为云官方做的一个培训视频，对于 Kubernetes 架构这一块儿的话讲的还是蛮清楚的。对于架构这一块的学习或者说是复习，主要针对的倒不是考纲中前面这些部分，主要是针对 Troubleshooting 这一部分，如果你对整体架构，或者说工作原理不是很清楚的话，一旦其中一个组件出了问题，很难准确定位到错误。我听说2020年9月之后 CKA 认证考试大纲会有变动，Troubleshooting 部分分值比例会调高，所以打算之后进行 CKA 认证的朋友一定要注意这一块儿，不要老是在那里调 YAML 清单\u0026hellip;。所以，前期的话我主要是在 Kubernetes 架构这块儿进行了一段时间的深入学习，虽然那道 Troubleshooting 还是没做出来，但是我并不觉得这思路有啥问题~\nKubernetes 核心资源概念，这个就不用说了吧，重中之重，这块儿的核心说白了就是两个部分，一个是 kubectl 的使用，另一个是 YAML 清单的配置，这一块儿的关键是文档。值得一提的是，CKA 认证考试过程中允许查阅官方文档，也就是说允许考试页面 Tab 外再开一个 Kubernetes Docs 页面，所以有些东西是可以查到复制过来的，这也是我觉得这个认证很接地气的一点，毕竟平时也是这么干的，文档是给人查的，不是用来背的，不是吗？但是，我并不推荐做一个题查一个文档，你要知道考试时长2小时，然后要做25道题，平均几分钟就要搞定一道，遇到复杂的题目你还得琢磨一下，所以时间还是有限制的，别太依赖文档，这也是我要说的第三部分；\n集群环境下训练，这部分的重要性不言而喻，前面两部分都是铺垫，无论是熟悉架构还是熟悉概念，真正实操不行的话，其它都免谈\u0026hellip; 很简单的例子，如果说 pv，pvc，secret，configMap 之类的概念，很多人不屑一顾，但是真正让他去从头配置一个，并且通过挂载的方式或者环境变量的方式关联到应用，有些人磨磨唧唧半天整不好，熟练的朋友三下五除二搞定，这个问题在这种题目繁多的认证考试中显得尤为突出。CKA 认证考试毕竟是个完全上机的认证考试，不是背背题库就能解决的，因此考试前一定要在集群环境下进行大量的训练。因为我实验室里有自己配的一台 DELL 服务器，上面 ESXi 分了三个 Node 跑了一个三节点的 Kubernetes 集群，所以这一块儿还是蛮方便的。如果真的搞不起来集群的（真的有特殊原因，否则集群配置是 CKA 的考察项目之一，需要掌握），可以在 Kubernetes 官网上的 Trainning Courses 上用他们的 Minikube 来当做训练环境，但是也不是很方便，一方面网络原因，一方面他们会在空闲时进行资源的回收，所以适合一些一次性的快速练习。\n报名及考试 认证考试报名，这个直接去 CNCF 的相关页面报名即可，在 CNCF 官网的导航栏“Certification”项目里找到“CKA”点进去就是，也可以直接进这个链接：https://www.cncf.io/certification/cka/，报名的话目前是2088元，前一阵子有优惠，我报名的时候花了1400元，你想白嫖是不可能的~ 报名成功后会给你个考试码，之后你就根据考试流程管理页面的步骤提示一步一步往下走就可以了。对了，需要绑定一个16位号的银行卡，其它位数的不可以，虽然没啥用，但是不绑还不行，没法申请考试。\n预约考试，这个时候要根据提示进一个叫做 PSI 的考试系统，里面可以进行考试的 Sheduling，也就是预约考试时间，你先确定一个大体的时间，系统给你返回这段时间附近可用的预约时间，预约之后就会有天数时间倒计时，准备考试就可以了。\n准备开考，线上考试当天，提前十几分钟就显示可以进入考试页面，点进去之后，首先是常规项目的检查，以及操作提示，然后右下角有个聊天框，考试官会要求你端着电脑通过摄像头观察周围环境，观察位置左右，观察桌面，观察有无佩戴手表\u0026hellip;，包括很重要的，检查你的证件，我是拿了身份证还有港澳通行证，因为听之前考试的朋友说他们需要一个带有英文名的正规证件\u0026hellip;所以护照啥的都可以，总之这个检查过程比较繁杂，考试官事儿还不少，要有耐心，还有一点，聊天全英文，注意，考试题目是可以中英文切换的，但是考试官是说英文的，这一点懒得吐槽了，当时还是预约的所谓中文面试官\u0026hellip;但是还好自己英语水平还可以，交流没啥障碍，如果英文不好的朋友这一点还是要注意的，如果有什么问题要问考试官也是通过这个聊天框。考试页面的话左边是题目和提示，题目下方有题号的导航，可以标记不会做的题，有时间导航回来做，这个是允许的，有的朋友喜欢浏览一遍题目跳着做，这个看个人喜好，但是注意不要漏题目，一定要做好标记。然后屏幕上有个可以活动的 Notepad，可以用来复制粘贴过程中暂存一些内容修改修改，如果 Vim 操作熟练的话就忽略这个，我虽然 Vim 不是贼6那种，但是这个 Notepad 没咋用，我觉得用处不是很大。然后右边主要的区域就是一个 Terminal，上方还有个重连按钮，网络原因，Terminal 老是断开，点一下重连就没问题了，操作不会丢失，这个放心，对于 Terminal 的复制粘贴操作，因为我是用的 Mac OS，所以没啥影响，command C/V 正常使用，Windows 下的朋友注意一下可能需要“终端复制/粘贴”操作，而不是普通的复制粘贴。\n考试环节，当考试官提示你可以开考后，Terminal 会自动打开，然后页面左上方会有一个条状的计时，当然也可以看自己电脑的时间。考试过程中房间不能有别人进进出出，不能老是摇晃头，不能说话，这个是常识。如果你要去卫生间，要跟面试官申请，他同意后你可以暂时离开，但是别时间太久，这个过程中考试不会暂停。如果网络中断或一些因素导致考试中断，可以自己重新进考试页面，跟考试官说明情况，但是如果老是这么中断的话，估计就不让你考了\u0026hellip;\n指南 \u0026amp; 技巧  选一个好用的科学上网工具，这个很关键，否则报名或考试流程会很难受，而且 Kubernetes 官方文档也是部署在境外服务器； 关于备考方面，强烈推荐 CSDN 上一个博主的总结，很详细，很有帮助：https://blog.csdn.net/fly910905/article/details/102966474，我建议把这个系列的文章全部过一遍，练习一遍，考试时一点不慌，考察的点基本上都包含了； 官方文档最佳用法是直接分类放进收藏夹里，这个是允许的，可以按照你的思路进行一个归类，到时候真的需要查的时候直接在收藏夹里翻找一下即可，不要每次都是在 Docs 页面 Search，那样很浪费时间； 官方文档有个很有帮助的页面叫做 Kubectl Cheat Sheet，https://kubernetes.io/docs/reference/kubectl/cheatsheet/，这里面列出了各种常用的命令，挺方便的，可以熟悉几遍然后收藏起来； 熟练利用 Linux 的 alias 命令进行一些快捷配置，比如最基本的，把 kubectl 换成 k，爽多了；   最后附上自己的证书，也祝各位朋友顺利！\n"
},
{
	"uri": "https://blog.yingchi.io/tags/note.html",
	"title": "note",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.yingchi.io/posts/2020/6/concurrent-pattern.html",
	"title": "浅析并发模型：共享内存/Actor/CSP",
	"tags": ["golang"],
	"description": "",
	"content": "Golang 编程中，涉及到并发问题时，通常有以下两种解决方案：\n 采用共享内存模型，利用 sync.Mutex / sync.RWMutex 等加锁、设置临界区解决数据并发访问问题； 采用消息通信模型，利用 channel 进行 goroutine 间通信，避开内存共享来解决。  官方推荐大家采用第二种方案，那么它究竟好在哪里呢？\n共享内存模型 所谓共享内存模型，就是我们在并发编程的时候，通过让多个并发执行实体（线程/Go程/协程/\u0026hellip;）去操作同一个共享变量，从而达到通信的目的。\n比如下面这个 Go 程序例子，全局变量 count 初始值 10000，然后开启 10000 个 Goroutine 去分别执行一次取 count 并 -1 的操作。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) var ( count = 10000 wg sync.WaitGroup ) func buy() { defer wg.Done() countReplica := count count = countReplica - 1 } func main() { for i := 0; i \u0026lt; 10000; i++ { wg.Add(1) go buy() } wg.Wait() fmt.Println(count) } 你认为执行完毕后 count 还剩多少？0 ？还是？\n答案是：不确定\n可以列举几次的运行结果：\n1221 1270 1259 ... 没有一次等于 0。为什么？原因很简单，学过并发编程的朋友都会回答到：数据同步问题。\n很好理解为什么会出现问题，在扣减 count 的过程中，需要先获取共享的 count 值，执行 -1 后赋值回去，假如此时的 count == 9000，GoroutineA 和 GoroutineB 同时读到 count == 9000（并发过程中这是完全可能的），然后各自 -1，赋值回 count，结果怎么样，虽然两个 Goroutine 均扣减了 count，但是最终赋值回去的都是 8999，这就叫做数据不同步，所谓数据同步，就是协同步调，按照顺序来执行，明显上面这个过程就违背了，因此出现了并发问题。\n在共享内存模型下解决这一问题的基本方式就是通过锁机制实现资源的互斥访问，将需要互斥访问的资源（也就是共享的资源，一般称为临界资源/互斥资源）加锁，当一个 Goroutine 访问临界资源时，如果未加锁，则加锁访问，访问结束后释放锁，其它 Goroutine 访问加锁资源时会阻塞住，通过这种方式可以实现数据的同步问题，其中代码中涉及到了操作临界资源的代码段叫临界区。\n现在把上面的程序进行改造，将 count 设置为临界资源，通过 golang 提供的 sync.Mutex 互斥量加锁：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) var ( count = 10000 wg sync.WaitGroup mutex sync.Mutex ) func buyWithMutex() { mutex.Lock() defer wg.Done() //---------- 临界区 -------------- \tcountReplica := count count = countReplica - 1 //------------------------------- \tmutex.Unlock() } func main() { for i := 0; i \u0026lt; 10000; i++ { wg.Add(1) go buyWithMutex() } wg.Wait() fmt.Println(count) } 通过实现 count 的互斥访问，此时 10000 个 Goroutine 并发扣减 count 不再出现由于数据不同步而扣减失败的现象，因此执行结果输出为 0，不多不少，全部扣减完成。\n共享内存模型必须要考虑的是共享变量的同步问题，当共享变量变多的时候，并发编程中需要考虑的数据同步问题就会越来越多，这也共享内存容易出错的原因所在。\n在所有的并发模型中，共享内存这种方案最为常见，也是较为底层的一种实现方案，这种方案源于早期的单核时代，仅从单机角度去解决并发问题的话，这个并发模型的设计是无可厚非的。但是在分布式时代、多核时代，一旦出现并行执行的问题，共享内存模型就有些捉襟见肘了，因此针对这些场景，需要有更好的方案来解决，首先说方案是多种多样的，比如可以从锁机制本身去考虑，因此有分布式锁的实现方式，本文因为主要是将的并发模型，因此接下来就介绍一种更适合分布式时代及多核时代的并发模型。\n消息传递模型 消息传递模型，先不去管接下来要说的两种设计实现，首先要记住这句话：\n 不要通过共享内存来通信，而应该通过通信来共享内存\n 很多人在接触到消息传递模型这种并发模型的时候都会听到这句话，我在学习 golang 的时候不知道听到多少次这句话了，但是一开始还是有些不理解的，感觉说的有点绕，但是随着接触到的并发编程相关的代码越来越多，才逐渐明白这句话的意义。\nActor CSP 参考  https://blog.csdn.net/u014800094/article/details/53993203 https://blog.51cto.com/nxlhero/1666250 https://blog.csdn.net/qq_41895747/article/details/105991486  "
},
{
	"uri": "https://blog.yingchi.io/tags/istio.html",
	"title": "istio",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.yingchi.io/posts/2020/6/istio-sidecar-proxy.html",
	"title": "Istio Sidecar 流量拦截机制分析",
	"tags": ["service-mesh", "istio"],
	"description": "",
	"content": "流量流经主机的基本过程 过程如下：\n Inbound 流量经过 NIC（网卡）进入主机的网络协议栈； 协议栈会根据预先定制的网络规则(iptables/netfilter)对报文进行检查； 协议栈规则检查后，符合要求的 Inbound 流量会从内核空间进入到用户空间，并进入指定监听端口的进程； 处于用户态的用户进程接收到网络流量报文进行处理后，将处理后的结果再通过用户空间返回给内核空间的网络协议栈； 网络协议栈检查报文，并将结果报文根据指定的网络策略通过网卡发送出去；  Sidecar 流量拦截基本过程 之前的文章已经介绍过了 Sidecar 的注入机制，注入到 Pod 中的即下面两个容器：\n istio-init：InitContainer，用于在 Pod 初始化过程中对 Pod 的 iptables 进行初始配置； istio-proxy：负责与 pilot 组件通信以及流量的控制；该容器运行时会启动两个关键的进程 pilot-agent 和 envoy。pilot-agent 进程会定时跟 istio 的 pilot 组件进行通信，envoy 进程会接收入口和出口网络流量。  注意：istio-proxy 和 Kubernetes 中的 kube-proxy 都是通过 iptables/netfilter 来处理网络流量。只不过 istio-proxy 位于 pod 网络空间，处理的是 pod 内的网络流量，而 kube-proxy 位于宿主机网络空间，处理的是宿主机内网络流量（因为 kube-proxy 是 daemonset，因此它位于 k8s 集群的每个 node 节点上）。\nSidecar 流量拦截其实指基于 iptables 规则，由 init 容器在 Pod 启动的时候首先设置iptables 规则。\niptables 规则拦截应用容器 Inbound/Outbound 的流量，目前只能拦截 TCP 流量，不能拦截UDP，因为Envoy目前并不支持UDP的转发。\n下面来分析 Inbound 流量一系列走向：\n  Inbound 流量在进入 Pod 的网络协议栈时首先被 iptables 规则拦截；\n  iptables 规则将数据包转发给 istio-proxy 容器的 Envoy 进程；\n  Envoy 再根据自身监听器的配置，将流量转发给 app 容器中的应用进程。\n  注意：Envoy 在将流量转发给应用时也会流经内核协议栈由 iptables 规则处理，这里 init 容器设置的规则并没有拦截，因此中间省略iptables的处理过程；\n然后来分析 Outbound 流量一系列走向：\n Outbound 流量由应用发出，首先被 iptables 规则拦截； iptables 规则将出口数据包转发给 istio-proxy 容器的 Envoy 进程； Envoy 再根据自身配置决定是否将流量转发到容器外。  流量拦截实现细节 Istio 流量代理转发模式 在Istio中，流量拦截的实现依赖initContainer iptables规则的设置，目前有两种流量拦截模式：REDIRECT模式和TPROXY模式。\nREDIRECT 模式 REDIRECT 模式下，通过iptables，可以将所有的流量都重定向到一个特定的端口上。\n例如下面一条 REDIRECT 规则：\niptables -t nat -A PREROUTING -p tcp -j REDIRECT --to-port 15001 即将所有流量都重定向到 15001 端口。看起来和 iptables 的 DNAT 很像，本质上 REDIRECT 就是一个特殊的DNAT规则，一般情况下，我们利用 iptables 做 DNAT 的时候，需要指定目标的IP和端口，这样 iptables 才能知道需要将数据包的目的修改成什么，而 REDIRECT 模式下，只需要指定端口就可以，iptables会自动帮我们判断需要设置的IP地址。\nREDIRECT 模式虽然会进行源地址转换，但依然是默认的设置，因为配合 Istio提供的遥测数据依然可以进行调用链分析。在Kubernetes平台上Pod及其IP地址并不是持久不变的，会随着集群的资源状况动态销毁及迁移，所以源地址这种传统的软件系统记录客户端的方式并不适合云原生应用平台Kubernetes；\nTPROXY 模式 除了利用 REDIRECT 模式，Istio还提供 TPROXY 模式，对于 TPROXY 模式，需要借助iptables和路由，比较复杂一点，用来做透明代理，操作的是mangle表。\n同时需要原始客户端 socket 设置 IP_TRANSPARENT 选项，Linux提供了一个 IP_TRANSPARENT 选项 ，这个选项可以让程序bind一个不属于本机的地址，作为客户端，它可以使用一个不属于本机地址的IP地址作为源IP发起连接，作为服务端，它可以侦听在一个不属于本机的IP地址上，而这正是透明代理所必须的。\n由于 TPROXY 模式并没有改变数据包，所以直接通过 getsockname 获取到原始的IP端口信息。\n结合 iptables 分析 Inbound/Outbound 流量走向 Inbound 流量  Inbound 首先匹配 iptables nat 表的 PREROUTING 链的第一条规则，因此 Inbound 流量被路由到 ISTIO_INBOUND 链； 在 ISTIO_INBOUND 链中，根据访问端口，匹配该链的第 X 条规则，流量路由到 ISTIO_IN_REDIRECT 链； 路由到 ISTIO_IN_REDIRECT 链的流量最终会从内核态打入到用户态的监听 xxxx 端口的 Envoy 进程； Envoy 进程处理完流量后，会将流量从用户态的进程传回内核态的网络协议栈，根据预先定义好的协议栈规则，流量会流经 OUTPUT 链，OUTPUT 链又会根据规则再把流量路由给 ISTIO_OUTPUT 链； 因为 Envoy 处理完流量最终要重新路由给目的端口的应用进程，因此处于 ISTIO_OUTPUT 链的第 X 条规则会被匹配（因为 Envoy 跟应用容器进程在同一个网络命名空间，因此环回地址 lo 被匹配），此时流量会重新从内核态返回到用户态，并进入监听端口为 X 的应用进程； 应用进程处理完后，将结果通过 socket 连接返回给 envoy 进程（用户态）； Envoy 再将流量通过 POSTROUTING 链、NIC，将响应流量返回给用户。  Outbound 流量   Outbound 流量首先会通过 iptables nat 表的 OUTPUT 链进入到 ISTIO_OUTPUT 链；\n  根据目的地址，匹配到 ISTIO_REDIRECT 链；\n  ISTIO_REDIRECT 链会将流量路由给 Envoy 进程；\n  Envoy 将处理后的 Outbound 流量重新通过用户态进入到内核态的网络协议栈，流量会首先经 OUTPUT 链到 ISTIO_OUTPUT 链；\n  流量最终会通过 POSTROUTING 链进入到 NIC 然后发送出去。\n  参考   https://blog.51cto.com/14625168/2476894\n  https://blog.gmem.cc/istio-tproxy\n  https://www.ichenfu.com/2019/04/09/istio-inbond-interception-and-linux-transparent-proxy/\n  http://blog.fatedier.com/2018/11/21/service-mesh-traffic-hijack/\n  "
},
{
	"uri": "https://blog.yingchi.io/tags/service-mesh.html",
	"title": "service-mesh",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.yingchi.io/posts/2020/6/istio-sidecar-injection.html",
	"title": "Istio Sidecar 注入机制",
	"tags": ["service-mesh", "istio"],
	"description": "",
	"content": "Service Mesh 及 Sidecar 概念 在了解 Sidecar 的注入机制前还是先要明确是什么和为什么的问题。\n首先，Service Mesh 是什么？\nService Mesh，或者翻译为「服务网格」，是一个可配置的低延迟的基础设施层，目的是通过API（应用程序编程接口）处理应用程序服务之间的大量基于网络的进程间通信。服务网络确保容器化的短暂存在的应用程序的基础结构服务之间的通信快速，可靠和安全。网格提供关键功能，包括服务发现，负载平衡，加密，可观察性，可追溯性，身份验证和授权，以及对断路器模式的支持。其实服务网格的目的说的最简单就是「接管和治理应用程序间的通信」，其中有个最为核心的要点就是，通信，基于 Service Mesh 的服务治理就是在服务的通信或者说调用过程中「做手脚」，离开这一点，Service Mesh 毫无意义，当然，这个应用程序似乎也没有意义了。如下图所示，就是典型的 Service Mesh 基础设施层架构。\n可以看到分为 Control Plane 和 Data Plane，Control Plane 的主要作用就是治理规则的控制与下发，而 Data Plane 的主要作用就是处理服务实例间的通信过程，实施指定的治理策略。Data Plane 中就可以看到今天的主角，也就是 Sidecar，正如图中标识的，Sidecar 最准确的表述应该是 「Sidecar Proxy」，它的本质是一个代理组件，这个组件会被直接注入到服务实例相同的 Network Namesapce 下，在 Kubernetes 中，就是注入到 Pod 里面，此时 Sidecar 与服务实例共享 Pod Network Namespace，可以通过 iptables 对流经实例的 inbound 和 outbound 流量进行相应的规则处理。\n本文主要基于 Istio 这个 Service Mesh 实现针对 Sidecar 的注入机制进行分析。\nAdmission Controller 与 Admission Webhook Sidecar 的注入依赖于 Kubernetes 的几个概念，其中比较核心的就是 Admission Controller 和 Admission Webhook。\nAdmission Controller，可翻译为「准入控制器」，按照官方的解释是： Admission Controller是拦截 API Server 请求(经过身份验证的)的网关，并且可以修改请求对象或拒绝请求。简而言之，它可以认为是拦截器，类似web框架中的middleware，是 Kubernetes API Server 用于拦截请求的一种手段。\nKubernetes 为什么引入 admission 这种机制？\n 虽然 Kubernetes 有 Authentication \u0026amp; Authorization 这种认证鉴权机制，Authentication \u0026amp; Authorization 运行在 filter 中，只能获取 http 请求 header 以及证书，并不能获取请求的 body。所以不可以对请求的对象进行任何操作，因为获取不到对象。 Admission Controller 运行在 API Server 的增删改查 handler 中，可以自然地操作 API resource。  API Server 接收到客户端请求后首先进行认证鉴权，认证鉴权通过后才会进行后续的 endpoint handler 处理，kube-apiserver 处理资源请求的流程如下：\n可以看到，在 Authentication \u0026amp; Authorization 之后请求就交由 Admission Controller 继续处理，这里涉及到 Admission 两个重要阶段，Mutating和Validating，它们的区别如下：\n Mutating：可以对请求内容进行修改； Validating：不允许修改请求内容，但可以根据请求的内容判断是继续执行该请求还是拒绝该请求；  Kubernetes 提供了很多内置的 Admission Controller Plugin，一些常用的准入控制策略都能找到。\n# 支持的plugin 如下 AlwaysAdmit, AlwaysDeny, AlwaysPullImages, DefaultStorageClass, DefaultTolerationSeconds, DenyEscalatingExec, DenyExecOnPrivileged, EventRateLimit, ExtendedResourceToleration, ImagePolicyWebhook, Initializers, LimitPodHardAntiAffinityTopology, LimitRanger, MutatingAdmissionWebhook, NamespaceAutoProvision, NamespaceExists, NamespaceLifecycle, NodeRestriction, OwnerReferencesPermissionEnforcement, PersistentVolumeClaimResize, PersistentVolumeLabel, PodNodeSelector, PodPreset, PodSecurityPolicy, PodTolerationRestriction, Priority, ResourceQuota, SecurityContextDeny, ServiceAccount, StorageObjectInUseProtection, ValidatingAdmissionWebhook. Kubernetes 提供了这么多 Admission 插件， 但是并不能保证满足所有开发者的需求。因此 Kubernetes 同样允许用户自定义自己的 Admission Controller，Kubernetes 提供了 Admission Webhook 这种扩展机制。\n MutatingAdmissionWebhook：在对象持久化之前进行修改 ValidatingAdmissionWebhook：在对象持久化之前进行  Admission Webhook 属于同步调用，需要用户开发部署自己的 webhook server，创建自定义的配置资源对象： ValidatingWebhookConfiguration 或 MutatingWebhookConfiguration。\n可以说有了 Admission Webhook 这种扩展机制，才算是真正为后面 Sidecar 注入实现铺平道路。\nIstio Sidecar 自动注入实现 Sidecar Injector 是 Istio 中实现自动注入Sidecar的组件，它就是以 Kubernetes 准入控制器 Admission Controller 的形式运行的。回顾前面的概念，Admission Controller 的基本工作原理是拦截 Kube-apiserver 的请求，在对象持久化之前、认证鉴权之后进行拦截。之前提到 Admission Controller有两种：一种是内置的，另一种是用户自定义的。后者就是 Kubernetes 允许用户以 Webhook 的方式自定义准入控制器，Sidecar Injector 就是这样一种特殊的 MutatingAdmissionWebhook。\nSidecar 注入的过程如下图所示：\n如图，Sidecar Injector 只在创建 Pod 时进行 Sidecar 容器注入，在 Pod 的创建请求到达 Kube-apiserver 后，首先进行认证鉴权，然后在准入控制阶段，Kube-apiserver 以 REST 的方式同步调用 Sidecar Injector Webhook 服务进行 init 与 istio-proxy 容器的注入，最后将Pod对象持久化存储到etcd中。\n还可以看一下 MutatingWebhookConfiguration 的配置：\nkubectl get MutatingWebhookConfiguration istio-sidecar-injector -n istio-system -o yaml istio-sidecar-injector MutatingWebhookConfiguration\napiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;admissionregistration.k8s.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;MutatingWebhookConfiguration\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;sidecar-injector\u0026#34;,\u0026#34;install.operator.istio.io/owning-resource\u0026#34;:\u0026#34;installed-state\u0026#34;,\u0026#34;istio.io/rev\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;operator.istio.io/component\u0026#34;:\u0026#34;Pilot\u0026#34;,\u0026#34;operator.istio.io/managed\u0026#34;:\u0026#34;Reconcile\u0026#34;,\u0026#34;operator.istio.io/version\u0026#34;:\u0026#34;1.6.2\u0026#34;,\u0026#34;release\u0026#34;:\u0026#34;istio\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;istio-sidecar-injector\u0026#34;},\u0026#34;webhooks\u0026#34;:[{\u0026#34;clientConfig\u0026#34;:{\u0026#34;caBundle\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;service\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;istiod\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;istio-system\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/inject\u0026#34;}},\u0026#34;failurePolicy\u0026#34;:\u0026#34;Fail\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;sidecar-injector.istio.io\u0026#34;,\u0026#34;namespaceSelector\u0026#34;:{\u0026#34;matchLabels\u0026#34;:{\u0026#34;istio-injection\u0026#34;:\u0026#34;enabled\u0026#34;}},\u0026#34;rules\u0026#34;:[{\u0026#34;apiGroups\u0026#34;:[\u0026#34;\u0026#34;],\u0026#34;apiVersions\u0026#34;:[\u0026#34;v1\u0026#34;],\u0026#34;operations\u0026#34;:[\u0026#34;CREATE\u0026#34;],\u0026#34;resources\u0026#34;:[\u0026#34;pods\u0026#34;]}],\u0026#34;sideEffects\u0026#34;:\u0026#34;None\u0026#34;}]} creationTimestamp: \u0026#34;2020-06-16T06:47:00Z\u0026#34; generation: 2 labels: app: sidecar-injector install.operator.istio.io/owning-resource: installed-state istio.io/rev: default operator.istio.io/component: Pilot operator.istio.io/managed: Reconcile operator.istio.io/version: 1.6.2 release: istio managedFields: ... webhooks: - admissionReviewVersions: - v1beta1 clientConfig: caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ... service: name: istiod namespace: istio-system path: /inject port: 443 failurePolicy: Fail matchPolicy: Exact name: sidecar-injector.istio.io namespaceSelector: matchLabels: istio-injection: enabled objectSelector: {} reinvocationPolicy: Never rules: - apiGroups: - \u0026#34;\u0026#34; apiVersions: - v1 operations: - CREATE resources: - pods scope: \u0026#39;*\u0026#39; sideEffects: None timeoutSeconds: 30 Sidecar Injector 默认只对标签匹配 istio-injection：enabled 的命名空间下的Pod资源对象的创建生效。Webhook服务的访问路径为/inject，地址及访问凭证等都在clientConfig字段下进行配置。 Istio Sidecar Injector组件是由sidecar-injector进程实现的。Sidecar Injector 的实现主要由两部分组成：\n  MutatingWebhookConfiguration\n  Webhook Server，为应用工作负载自动注入Sidecar容器\n  Sidecar Injector 以轻量级 HTTPS 服务器的形式处理 Kube-apiserver 的 AdmissionRequest 请求。\n通常Pod Sidecar容器的注入由以下步骤完成:\n 解析 Webhook REST 请求，将 AdmissionReview 原始数据反序列化； 解析 Pod，将 AdmissionReview 中的 AdmissionRequest 反序列化； 利用 Pod 及网格配置渲染 Sidecar 配置模板； 利用 Pod 及渲染后的模板创建 JSON patch； 构造 AdmissionResponse； 构造 AdmissionReview，在进行 JSON 编码后，将其发送给 HTTP 客户端即Kube-apiserver；  可以查看注入的配置项 istio-sidecar-injector：\nkubectl describe configmap istio-sidecar-injector -n istio-system istio-sidecar-injector configmap (简略版本)\nName: istio-sidecar-injector Namespace: istio-system Labels: install.operator.istio.io/owning-resource=installed-state istio.io/rev=default operator.istio.io/component=Pilot operator.istio.io/managed=Reconcile operator.istio.io/version=1.6.2 release=istio Annotations: Data ==== config: ---- policy: enabled ... template: | rewriteAppHTTPProbe: {{ valueOrDefault .Values.sidecarInjectorWebhook.rewriteAppHTTPProbe false }} initContainers: ... - name: istio-validation ... - name: istio-init ... args: - istio-iptables - \u0026#34;-p\u0026#34; - 15001 ... containers: - name: istio-proxy ports: - containerPort: 15090 protocol: TCP name: http-envoy-prom args: ... env: - name: JWT_POLICY value: {{ .Values.global.jwtPolicy }} ... ConfigMap 保存了默认注入策略（policy）和 sidecar 注入模板（template）。 策略（policy）:\n disabled：sidecar 注入器默认不会注入到 pod 中。添加pod模板定义中的注解 sidecar.istio.io/inject 值为 true会启用注入功能。 enabled：sidecar 注入器默认会注入到 pod 中。添加pod模板定义中的注解 sidecar.istio.io/inject 值为 false会禁止注入功能。  参考  http://dockone.io/article/8975 https://cloud.tencent.com/developer/article/1445760 https://www.dazhuanlan.com/2019/12/10/5deec157cba32/ https://www.codercto.com/a/27364.html https://blog.csdn.net/weixin_34014277/article/details/89570351 https://developer.aliyun.com/article/620700 《云原生服务网格 Istio: 原理、实践、架构及源码解析》张超盟  "
},
{
	"uri": "https://blog.yingchi.io/posts/2020/5/what-is-cloud-native.html",
	"title": "Cloud Native 云原生 | 概念解读",
	"tags": ["cloud-native"],
	"description": "",
	"content": "定义的探索之路 都在提「云原生」的概念，但是真正理解这个概念的又有多少人呢？每次浏览一些社区的时候（不乏一些专业的技术社区），看到有些朋友在讨论云原生相关的话题，有些时候总感觉他们对于云原生的理解还是有那么一点问题。\n很多朋友直接就认为部署在云端的应用就叫做云原生应用，一般出现在刚接触云计算与云原生概念的群体中，这个明显是错误的认识，或者说就是「概念滥用」，再有就是懂一些容器化相关概念的内行人士，他们认为，云原生应用是通过容器技术构建，部署在 Kubernetes 这种容器编排平台上的，才能是云原生应用，这听起来像那么回事了，但是呢，还是不够严谨，只是从云原生应用的实现层面来讲的，但是对于其核心概念还是没解释，比如 Cloud Native 的 Native 具体体现在什么上？\n其实也不能怪大家伙没搞明白啥是云原生，其实就连最早发起云原生概念的这群人自己也是在逐渐摸索。\n来看 CNCF 最初对云原生的定义：\n The CNCF defines “cloud-native” a little more narrowly, to mean using open source software stack to be containerized, where each part of the app is packaged in its own container, dynamically orchestrated so each part is actively scheduled and managed to optimize resource utilization, and microservices-oriented to increase the overall agility and maintainability of applications.\n 简而言之，云原生的最初定义就是，满足以下特性的应用：\n 采用容器化技术构建； 动态编排调度； 面向微服务；  明显，这是一个比较狭义的定义，其实说白了就是讲了一个容器生态技术，还称不上真正的云原生。\n然后，到了2017年, 云原生应用的提出者之一的 Pivotal 在其官网上将云原生的定义概况为DevOps、持续交付、微服务、容器这四大特征，这也成了很多人对 Cloud Native 的基础印象。\n2018年6月11日，CNCF 给出了 Cloud Native 的明确定义，大家可以参考 Github，CNCF Cloud Native Definition v1.0，这个定义也成了如今大家最为熟悉的定义，英文原文如下：\n Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.\nThese techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil.\nThe Cloud Native Computing Foundation seeks to drive adoption of this paradigm by fostering and sustaining an ecosystem of open source, vendor-neutral projects. We democratize state-of-the-art patterns to make these innovations accessible for everyone.\n 中文版本的翻译：\n 云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。\n这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。\n云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。我们通过将最前沿的模式民主化，让这些创新为大众所用。\n 这个定义简单来说包含这几个要点：\n 基于容器、服务网格、微服务、不可变基础设施和声明式 API 等技术构建的可弹性扩展的云环境应用； 高容错性、易管理和易观察的松耦合系统； 云原生基于开源的生态系统，与云厂商提供的服务保持中立，  注意里面有个很重要的概念，叫做不可变基础设施，这个看起来让人一头雾水的概念，其实很简单。Immutable Infrastructure 直译过来就是「不可变基础设施」。不可变基础设施里的「不可变」非常类似于程序设计中的不可变变量（Immutable Variable）概念，就是在完成赋值后就不能发生更改，比如 Go 语言中的 string，你不要想着通过”+“在原有的 string 上进行拼接，不可变变量本身是不可以更改的，拼接后的字符串其实是一个新的字符串变量，而不是基于旧字符串的更改，具有这样的特性这种变量可以在并发环境下安全的使用。对于基础设施的不可变性，最基本的就是指运行服务的服务器在完成部署后，就不在进行更改。说白了，就是如果想要更改旧的组件，唯一的方式就是通过新的资源代替它，而不是在原有组件上修改，这就是所谓的不可变基础设施。\nCloud Native 的重新解读 官方定义虽然就写在这里了，但是我觉得还应该有更进一步的分析来让大家理解，所以，结合前面官方的定义，我们现在重新再来认识云原生这个概念。回到 Cloud Native 这个词组，我们拆解开来看：\nCloud 很好理解，云，就是指的云计算环境，这是最基本的条件，按照官方的定义，应该是公有云、私有云和混合云等新型动态环境，也就是说，云原生的概念不会限定你基于哪一种具体的云环境，在几乎任意一种云环境下，都不影响云原生应用标准实现。云本质可以看作是一种提供稳定计算存储资源的对象，为了实现这点，像虚拟化、弹性扩展、高可用、高容错性、自恢复这些都是云的基本属性，云原生作为一种云计算，这是所具备的第一层含义\nNative Native 的本意指的是「原住民；土著」，Native 最大的特点就是原本生活在此且对此环境的高度适应，可以理解为两层意思，一个是限定不能是后期移入，另一个是要对环境适应，就像生活在北极的因纽特人，只有他们才可以称得上是北极地区的 Native，但是换做其他人，哪怕你再能抗寒，你也称不上是 Native。那么对于 Cloud Native 中的 Native，我们就很容易理解了，作为一个 Cloud Native 应用，首先，你这个应用架构天生就是为云环境而设计的，无论是服务的调用方式，服务的治理架构等，从一开始就应该从根本上为云环境而考虑，从适应性角度，就比如说，部署在 Pod 中的服务，很有可能随着 Pod 挂掉而挂掉，这在云环境中是个很常见且很正常的事，因为云环境资源的调度是非常灵活的，但是放在传统环境中，应用挂掉是个难以接受的事情，因此应用要适应云环境有时另外一个层面的要求。也就是说，对于一些按照传统应用模式开发，修修补补打包成镜像跑在 Kubernetes 上的应用，其实只是算一些「伪 Cloud Native」\n总结 至此，我相信对于 Cloud Native 的概念已经算是讨论清楚了，首先理解官方给出的定义很关键，我们必须要知道云原生的核心技术支撑还是容器、微服务这些概念，核心环境是公有云、私有云和混合云这些各种各样的云环境，然后关键还是回到对于 Cloud 和 Native 两个词的理解上。\n最后可以重新捋一遍概念，云原生指的是遵循敏捷开发原则，使用高度自动化的研发工具，专门针对云环境特点而开发的应用，这些应用采用自动化的，可扩展的，和高可用的架构，在此基础上工程师能够实现对应用的高效观察与管理，并可以通过自动化的手段对系统做出改进。\n参考  https://zhuanlan.zhihu.com/p/110421613 https://github.com/cncf/toc/blob/master/DEFINITION.md https://www.cnblogs.com/ck951203/p/6132262.html http://vlambda.com/wz_wzQMkRRNam.html https://zhuanlan.zhihu.com/p/120073918 https://en.wikipedia.org/wiki/Cloud_native_computing  "
},
{
	"uri": "https://blog.yingchi.io/tags/cloud-native.html",
	"title": "cloud-native",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.yingchi.io/posts/2020/5/client-go.html",
	"title": "client-go 初步认识与实践",
	"tags": ["kubernetes"],
	"description": "",
	"content": " 最近本人的一个容器应用管理平台项目需要实现对接 Kubernetes 平台并进行一些相关资源的操作，查阅了官方文档、GitHub 以及相关技术文章，发现有个叫做 client-go 的 go 语言库是非常适合做 Kubernetes 二次开发的，于是就边实践，边学习，对 client-go 这个库有了一定程度的了解。对于其中比较复杂的设计，如 informer 部分，之后有时间的话会结合 kube-controller-manager 相关机制的研究学习过程加以介绍分享。\n client-go 是 Kubernetes 项目所采用的编程式交互客户端库，官方从2016年8月份开始，资源交互操作相关的核心源码，也就是 client-go 抽取出来，独立出来作为一个项目。也就是现在所用到的 Kubernetes 内部都是集成有 client-go 的，因此对于这个库的编码质量应该是值得放心的。\nclient-go 所谓编程式交互客户端库说白了就是可以通过写一些 Go 代码实现对kubernetes集群中资源对象（包括deployment、service、ingress、replicaSet、pod、namespace、node等）的增删改查操作。\n源码简介 源码目录简述  discovery：通过Kubernetes API 进行服务发现； kubernetes：提供 ClientSet 客户端，可以对 Kubernetes 内置资源对象进行操作； dynamic：提供 DynamicClient 客户端，可以实现对任意 Kubernetes 资源对象操作； rest：提供 RESTClient 客户端，可以实现对 kube-apiserver 执行 REST 请求实现资源操作； scale：提供 ScaleClient 客户端，主要用于 Deployment 等资源的扩缩容； listers：为 Kubernetes 资源提供 Lister 功能，对 Get / List 请求提供只读的缓存数据； informers：提供每种 Kubernetes 资源的 Informer 实现； transport：用于提供安全的 TCP 连接； tools/cache：提供常用工具；提供 Client 查询和缓存机制，以缓解 kube-apiserver 压力； util：提供常用方法；  Client 对象 学习 client-go 进行 kubernetes 二次开发的很大一部分工作是学会熟练使用它的几种 client，client-go 有如下 4 种 client 客户端对象，通过 kubeconfig 配置信息连接到指定集群的 kube-apiserver 从而实现对于资源的相关操作。\n RESTClient：client-go 中最基础的客户端，其它 client 都基于 RESTClient 实现，RESTClient 实现了 RESTful 风格的 API 请求封装，可以实现对任意 Kubernetes 资源（包括内置资源及 CRDs）的 RESTful 风格交互，如 Post() / Delete() / Put() / Get()，同时支持 Json 和 protobuf； ClientSet：与 Kubernetes 内置资源对象交互最常用的 Client，强调，只能处理 Kubernetes 内置资源，不包括 CRD 自定义资源，使用时需要指定 Group、指定 Version，然后根据 Resource 获取。ClientSet 的操作代码是通过 client-gen 代码生成器自动生成的； DynamicClient：DynamicClient 能处理包括 CRD 自定义资源在内的任意 kubernetes 资源。但是要注意，DynamicClient 返回的对象是一个 map[string]interface{}，如果一个 controller 中需要控制所有的 API，可以使用dynamic client，DynamicClient 只支持JSON； DiscoveryClient：用于发现 kube-apiserver 支持的 Group / Version / Resource 信息；  client-go 客户端初始化 kubeconfig 配置信息 client-go 想要访问 kube-apiserver 进行交互操作，首先要配置相关的连接及身份认证等信息，配置信息由 kubeconfig 文件提供，默认情况下集群的 kubeconfig 文件存放路径在：\n$HOME/.kube/config 以本人的测试集群的 kubeconfig 文件为例：\napiVersion: v1 kind: Config clusters: - cluster: name: kubernetes server: https://node01:6443 certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJ... contexts: - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes users: - name: kubernetes-admin user: client-certificate-data: LS0tLS1CRUdJTiBDM4akND... client-key-data: LS0tLS1CFURSBLRVktLS0tLQpNS... 其中主要包含了如下信息：\n apiVersion：配置文件资源的版本 kind：配置文件资源的种类，即 Config clusters：定义 Kubernetes 集群相关信息  cluster：定义每一个集群的名称、kube-apiserver 地址、证书信息；   contexts：集群上下文环境  context：定义具体每个集群的命令空间及用户信息，用于将请求发送到指定的集群；   users：定义用户身份验证信息，客户端凭据；  client-go 读取 kubeconfig 配置信息生成 config 对象：\n... config, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;,\u0026#34;./configs/kubeconfig.conf\u0026#34;) ... kubeconfig, _ = ioutil.ReadFile(\u0026#34;./configs/kubeconfig.conf\u0026#34;) restConf, _ = clientcmd.RESTConfigFromKubeConfig(kubeconfig) 由 config 对象进一步生成需要的 client 对象，之后才能与 kube-apiserver 通信进行资源的交互操作，每种 client 对象都有自己的生成方式。\n常用的 Client 初始化及资源操作示例 介绍一下 RESTClient / ClientSet / DynamicClient 三种 Client 的用法，DiscoveryClient 暂不做介绍\nRESTClient RESTClient 初始化\nconfig, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;,\u0026#34;./configs/kubeconfig.conf\u0026#34;) if err != nil{ panic(err) } config.APIPath = \u0026#34;api\u0026#34; config.GroupVersion = \u0026amp;corev1.SchemeGroupVersion config.NegotiatedSerializer = scheme.Codecs restClient, err := rest.RESTClient(config) if err != nil{ panic(err) } RESTClient 资源操作示例\npods := \u0026amp;corev1.PodList{} err = restClient.Get() .Namespace(\u0026#34;yingchi\u0026#34;) .Resource(\u0026#34;pods\u0026#34;) .VersionedParams(\u0026amp;metav1.ListOptions{}, scheme.ParameterCodec) .Do() .Into(pods) if err != nil { panic(err) } ClientSet client-go 中最常用的 client 对象，原因就是 ClientSet 相比 RESTClient 封装的更加易用，尤其是仅仅是去操作 Kubernetes 内置资源时，ClientSet 应该是首选 Client 对象。ClientSet 在 RESTClient 基础上封装了对 Version 和 Resource 的管理方法，每个 Resource 都可以理解为一个 client，这也是 ClientSet 名称的由来。\nClientSet 初始化\nconfig, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;,\u0026#34;./configs/kubeconfig.conf\u0026#34;) if err != nil{ panic(err) } clientSet, err := kubernetes.NewForConfig(config) if err != nil{ panic(err) } ClientSet 资源操作示例\npods, err := clientSet.CoreV1() .Pods(\u0026#34;yingchi\u0026#34;) .List(metav1.ListOptions{}) if err != nil{ panic(err) } DynamicClient DynamicClient 也是基于 RESTClient 封装的一种动态客户端，与 ClientSet 不同的是，DynamicClient 能处理包括 CRD 自定义资源在内的任意 kubernetes 资源。\n能处理 CRD 的原因是 DynamicClient 内部实现了 Unstructured，用于处理非结构化的或者说未知结构的数据结构，这是处理 CRD 类型资源的关键，而 ClientSet 内部的数据都是结构化的，即知道每种 Resource 和 Version 对应的具体资源数据类型。\n需要注意的是，DynamicClient 不是类型安全的，使用 DynamicClient 对资源进行交互时尤其要注意指针问题，操作不当可能会导致程序崩溃。\nDynamicClient 初始化\nconfig, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, \u0026#34;./configs/kubeconfig.conf\u0026#34;) if err != nil{ panic(err) } dynamicClient, err := dynamic.NewForConfig(config) if err != nil{ panic(err) } DynamicClient 资源操作示例\nresource := schema.GroupVersionResource{ Version: \u0026#34;v1\u0026#34;, Resource: \u0026#34;pods\u0026#34; } obj, err := dynamicClient.Resource(resource) .Namespace(\u0026#34;yingchi\u0026#34;) .List(metav1.ListOptions{}) if err != nil{ panic(err) } pods := \u0026amp;corev1.PodList{} err = runtime.DefaultUnstructuredConverter .FromUnstructured(obj.UnstructuredContent(), pods) Informer 机制 Client-go 包中一个相对较为高端的设计在于 Informer 的设计，如果想要资源交互更优雅的模式，就要用到 Informer 的机制。\n通过之前的例子我们知道可以直接通过 Kubernetes API 交互，但是考虑一点就是交互的形式，Informer设计为List/Watch 的方式。Informer 在初始化的时先通过 List 去从 Kubernetes API 中取出资源的全部 object 对象，并同时缓存，然后后面通过 Watch 的机制去监控资源，这样的话，通过 Informer 及其缓存，我们就可以直接和Informer 交互而不是每次都和 Kubernetes API 交互，避免了轮询 kube-apiserver 等方式对 apiserver 带来的访问压力。\nInformer 提供了事件 handler 机制，并会触发回调，这样上层应用如 Controller 就可以基于异步回调的方式进行具体业务逻辑的处理。因为 Informer 通过 List、Watch 机制可以监控到所有资源的所有事件，因此只要给Informe r添加 ResourceEventHandler 实例的回调函数实例取实现 OnAdd(obj interface{}) OnUpdate(oldObj, newObj interface{})和 OnDelete(obj interface{})这三个方法，就可以处理好资源的创建、更新和删除操作。Kubernetes 中各种controller都会用到Informer。\nInformer 是 client-go 中比较精髓的\n参考   《Kubernetes 源码剖析》郑东旭\n  https://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md\n  https://www.jianshu.com/p/d17f70369c35\n  Accessing Kubernetes CRDs from the client-go package\n  client-go under the hood\n  Kubernetes Deep Dive: Code Generation for CustomResources\n  kubernetes client-go\n  Kubernetes Informer 详解\n  如何用 client-go 拓展 Kubernetes 的 API\n  Using Kubernetes API from Go\n  "
},
{
	"uri": "https://blog.yingchi.io/posts/2020/5/linux-iptables.html",
	"title": "Linux Netfilter/iptables 学习",
	"tags": ["kubernetes"],
	"description": "",
	"content": "Linux 网络协议栈非常高效，同时比较复杂。如果我们希望在数据的处理过程中对关心的数据进行一些操作，则该怎么做呢？Linux 提供了一套机制来为用户实现自定义的数据包处理过程。在 Linux 网络协议栈中有一组回调函数挂接点，通过这些挂接点挂接的钩子函数可以在 Linux 网络栈处理数据包的过程中对数据包进行一些操作，例如过滤、修改、丢弃等。整个挂接点技术叫作 Netfilter 和 iptables。\nNetfilter 与 iptables 不是两个独立的组件，Netfilter 是一个位于内核空间的防火墙框架，而 iptables 可以认为是一个位于用户空间的客户端。\nNetfilter 的核心功能就是数据包过滤、数据包修改、网络地址转换（NAT）\n基础概念 规则概念 iptables 最核心的概念是 Rules，即规则，一句话概括其工作逻辑就是“对于匹配到规则的数据包执行预先指定好的逻辑”。这里涉及到几个概念，首先是匹配，从字面上很好理解，匹配就是看对不对的上号，对于 iptables 而言，它面对的是数据包，因此它要匹配的自然是与数据包相关的信息，比如源地址、目的地址、传输协议、服务类型，只有当这些可以匹配的时候，才执行一些规则逻辑，比如放行、拒绝、丢弃等。\n五链 或许你对 iptables 具体是做什么的，怎么工作的并不熟悉，但是当你听到一个内行来讲 iptables 的时候，他一定会提到“四表五链”，那么什么是 iptables 的四表无链？他们又有什么作用呢？\n首先说“链”，这里的链指的是“规则链”，即在 iptables 的工作过程中，并不是只通过一条规则来处理数据包的，而是有许多规则，这些规则按照一定的顺序排列起来，报文经过 iptables 时就要对着一些规则一条一条进行匹配，执行相应的动作，我们把这种一系列的规则看作是一种串联，则称为是“链”。\n比如以其中一条称作 PREROUTING 的链来看，它的内部结构是这样的：\n数据包会在这条链里经过很多条的规则匹配，如果该数据包不符合链中任一条规则，iptables就会根据预先定义的默认策略来处理数据包。\n在 iptables 中存在着如下五条链：\n PREROUTING 链：路由选择前； INPUT 链：路由目的地为本机； FORWARD 链：路由目的地非本机，转发； OUTPUT 链：本机发出数据包； POSTROUTING 链：路由选择后；  四表 知道了五链之后，接下来看四表，如果说链是表现的是一系列规则的执行顺序关系，那么表则是表现的一系列规则的功能逻辑关系，我们把具有相同功能的规则集合称为“表”，因为我们会发现有时在不同的链上执行的规则它们之间是有内在关联的，或是对数据的过滤，或是对报文数据的修改等等，iptables 为我们提供了如下的规则分类：\n Filter 表：iptables 默认表，负责包过滤，防火墙功能； NAT 表：负责网络地址转换功能，对应内核模块； Mangle 表：主要负责修改数据包，对应内核模块； Raw 表：优先级最高，关闭 NAT 表启用的连接追踪机制；  注意这些表是有优先级之分的，优先级高到低：raw\u0026ndash;\u0026gt;mangle\u0026ndash;\u0026gt;nat\u0026ndash;\u0026gt;filter\n链与表的对应也不是随随便便的，有些表的规则只有对应链上可能存在，具体的链表对应关系如下：：\nPREROUTING 链：raw表，mangle表，nat表。\nINPUT 链：mangle表，filter表，（centos7中还可以有nat表）\nFORWARD 链：mangle表，filter表。\nOUTPUT 链：raw表mangle表，nat表，filter表。\nPOSTROUTING 链：mangle表，nat表。\n再看规则 一开始只是提到了规则这个概念，那么规则的匹配和规则逻辑该如何定义和进行呢？\n匹配条件 分为基本匹配条件和扩展匹配条件\n 基本匹配条件：源 IP，目标 IP； 扩展匹配条件：除了上述的条件可以用于匹配，还有很多其他的条件可以用于匹配，这些条件泛称为扩展条件，如源端口，目的端口等；  规则逻辑 规则逻辑在 iptables 中称为 target，常见的一些处理逻辑如下：\n  ACCEPT：允许数据包通过；\n  DROP：直接丢弃数据包；\n  REJECT：拒绝数据包；\n  SNAT：源地址转换，可以解决内网用户用同一个公网地址上网的问题；\n  DNAT：目标地址转换；\n  MASQUERADE：是SNAT的一种特殊形式，适用于动态的、临时会变的ip上；\n  REDIRECT：在本机做端口映射；\n  MARK：数据包打标记；\n  注意：\n 目标地址转换一般在 PREROUTING 链上操作 源地址转换一般在 POSTROUTING 链上操作  总结 1、当主机收到一个数据包后，数据包先在内核空间中处理，若发现目的地址是自身，则传到用户空间中交给对应的应用程序处理，若发现目的不是自身，则会将包丢弃或进行转发。\n2、iptables实现防火墙功能的原理是：在数据包经过内核的过程中有五处关键地方，分别是PREROUTING、INPUT、OUTPUT、FORWARD、POSTROUTING，用户可以在这五条链上编写 Rules，对经过的数据包进行处理，规则一般的定义为“如果数据包头符合这样的条件，就这样处理数据包”。\n3、iptables中定义有5条链，每条链中可以定义多条 Rules，每当数据包到达一条链时，iptables就会从链上第一条规则开始匹配，看该数据包是否满足规则所定义的条件。如果满足，系统就会根据该条规则所定义的规则逻辑（称为 target）处理该数据包；否则 iptables 将继续检查下一条规则，如果该数据包不符合链中任一条规则，iptables就会根据预先定义的默认策略来处理数据包；\n4、iptables中定义有表，分别表示提供的功能，有filter表（实现包过滤）、nat表（实现网络地址转换）、mangle表（实现包修改）、raw表（实现数据跟踪），这些表具有一定的优先级：raw\u0026ndash;\u0026gt;mangle\u0026ndash;\u0026gt;nat\u0026ndash;\u0026gt;filter\n参考   https://www.cnblogs.com/liang2580/articles/8400140.html\n  https://blog.csdn.net/qq_29344757/article/details/81128150\n  http://www.zsythink.net/archives/1199\n  https://blog.csdn.net/u011537073/article/details/82685586\n  https://blog.csdn.net/longbei9029/article/details/53056744\n  "
},
{
	"uri": "https://blog.yingchi.io/posts/2020/4/docker-k8s-network-3.html",
	"title": "Kubernetes &amp; Docker 网络原理（三）",
	"tags": ["kubernetes", "network"],
	"description": "",
	"content": "Service 通信 kube-proxy 运行机制 为了支持集群的水平扩展、高可用性，Kubernetes抽象出了Service的概念。Service是对一组Pod的抽象，它会根据访问策略（如负载均衡策略）来访问这组Pod。 Kubernetes在创建服务时会为服务分配一个虚拟的IP地址，客户端通过访问这个虚拟的IP地址来访问服务，服务则负责将请求转发到后端的Pod上。起到一个类似于反向代理的作用，但是它和普通的反向代理还是有一些不同：首先，它的Service 的 IP 地址，也就是所谓的 ClusterIP 是虚拟的，想从外面访问还需要一些技巧；其次，它的部署和启停是由Kubernetes统一自动管理的。\nService 和 Pod 一样，其实仅仅是一个抽象的概念，背后的运作机制是依赖于 kube-proxy 组件实现的。\n在 Kubernetes 集群的每个 Node 上都会运行一个 kube-proxy 服务进程，我们可以把这个进程看作 Service 的透明代理兼负载均衡器，其核心功能是将到某个 Service 的访问请求转发到后端的多个 Pod 实例上。此外，Service的Cluster IP与 NodePort 等概念是 kube-proxy 服务通过iptables的NAT转换实现的，kube-proxy 在运行过程中动态创建与 Service 相关的 iptables 规则，这些规则实现了将访问服务（Cluster IP或NodePort）的请求负载分发到后端 Pod 的功能。由于 iptables 机制针对的是本地的 kube-proxy 端口，所以在每个 Node 上都要运行 kube-proxy 组件，这样一来，在 Kubernetes 集群内部，我们可以在任意 Node 上发起对 Service 的访问请求。综上所述，由于 kube-proxy 的作用，在 Service 的调用过程中客户端无须关心后端有几个 Pod，中间过程的通信、负载均衡及故障恢复都是透明的。\nkube-proxy 运行模式 kube-proxy 的具体运行模式其实是随着 Kubernetes 版本的演进有着较大的变化的，整体上分为以下几个模式的演化：\n userspace (用户空间代理)模式 iptables 模式 IPVS 模式  userspace 模式\nkube-proxy 最早的工作模式便是 userspace 用户空间代理模式，在这种模式下 kube-proxy 是承担着真实的 TCP/UDP 代理任务的，当 Pod 通过 Cluster IP 访问 Service 的时候，流量被 iptables 拦截后转发到节点的 kube-proxy 进程，服务的路由信息通过 watch API Server 进行获取，然后 kube-proxy 进程再与具体的 Pod 建立 TCP/UDP 连接，从而将请求发送给 Service 的后端 Pod 上，在这个过程中实现负载均衡。\niptables 模式\n从 kubernetes 1.2 版本开始不再采用 userspace 用户空间代理模式，取而代之的是 iptables 模式，在 iptables 模式下 kube-proxy 不再担任直接的 proxy 作用，它的核心职责变为：一方面通过 watch API Server 实时获取 Service 与 Endpoint 的变更信息，然后动态地更新 iptables 规则，然后流量会根据 iptables 的 NAT 机制直接路由到目标 Pod，而不是再去单独建立连接。\n与之前的 userspace 模式相比，iptables 模式完全工作在内核态，不需要切换到用户态的 kube-proxy，避免了内核态用户态的频繁切换使得性能相比之前有所提高。\n但是 iptables 也存在着局限性，就是由于 iptables 客观因素，当 Kubernetes 集群规模扩大，Pod 数量大量增加之后，iptables 的规则数量会随之急剧增加，进而导致其转发性能的下降，甚至会出现规则丢失的情况（故障非常难以重现和排查），因此 iptables 模式也有待于改进。\nIPVS 模式\nIPVS 模式即 IP Virtual Server 模式，在 Kubernetes 1.11中 IPVS 模式升级为 GA，IPVS 虽然和 iptables 都是基于 Netfilter 实现，但是定位有着本质不同，iptables 设计为防火墙使用，而 IPVS 用于高性能负载均衡，而且从规则的存储角度，IPVS 采用的是 Hash Table 结构，因此理论上讲更适合在不影响性能的情况下大规模地扩展，同时 IPVS 支持比 iptables 更复杂的负载均衡算法（最小负载/最小连接数/加权等），支持服务器健康检查和连接重试等功能，另外还可以动态修改 ipset 集合。\n在 IPVS 模式下，并不是就直接抛弃 iptables 了，虽然 IPVS 在性能上肯定是要优于 iptables 的，但同时也有许多功能 IPVS 相比 iptables 是缺失的，比如包过滤、地址伪装、SNAT 等功能，因此在一些场景下是需要 IPVS 与 iptables 配合工作的，比如 NodePort 实现。同时在 IPVS 模式下，kube-proxy 使用的是 iptables 的扩展 ipset，而不是直接通过 iptables 生成规则链。iptables 规则链是线性的数据结构，而 ipset 是带索引的数据结构，因此当规则很多时，可以高效地匹配查找。\n"
},
{
	"uri": "https://blog.yingchi.io/tags/network.html",
	"title": "network",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.yingchi.io/posts/2020/4/docker-k8s-network-2.html",
	"title": "Kubernetes &amp; Docker 网络原理（二）",
	"tags": ["kubernetes", "network"],
	"description": "",
	"content": "Kubernetes Pod 间通信 之前的文章中主要关于 Docker 的网络实现进行了介绍和探讨，对于 Docker 网络而言，其最大的局限性在于跨主机的容器通信方案上存在空白，而 Kubernetes 作为适合大规模分布式集群的容器编排平台，其在网络实现层面上主要解决的问题就包括了如下几点：\n 容器间通信； Pod 间通信； Pod 与 Service 通信； 集群内外通信；  这篇博文主要针对 Kubernetes 的容器间通信和 Pod 间通信进行介绍和探讨，之后再通过单独一篇文章去探讨 Pod 与 Service 的通信，也就是 kube-proxy 工作原理和 Service 机制相关。\n容器间通信 学习 Kubernetes 的容器间通信方案之前要理解 Kubernetes 中的 Pod 概念，Pod 是 Kubernetes 中最基本的调度单位，而不是 Docker 容器，Pod 的本意是豆荚，可以将容器理解为豆荚中的豆子，一个 Pod 可以包含多个有关联关系的容器，之后讨论的 Pod 与 Service 的通信也是从 Pod 层面而言的。这是必须要提前认识的概念，但是在底层，还是涉及到容器之间的通信，毕竟 Pod 只是一个抽象概念。\n同一个 Pod 内的容器不会跨主机通信，它们共享同一个 Network Namesapce 空间，共享同一个 Linux 协议栈。所以对于网络的各类操作，因此可以把一个 Pod 视作一个独立的「主机」，内部的容器可以用 localhost 地址访问彼此的端口。这么做的结果是简单、安全和高效，也能减小将已经存在的程序从物理机或者虚拟机移植到容器下运行的难度。\n如图，Node 上运行着一个 Pod 实例，Pod 内部的容器共享同一个 Network Namespace，因此容器1和容器2之间的通信非常简单，就可以通过直接的本地 IPC 方式通信，对于网络应用，可以直接通过 localhost 访问指定端口通信。因此对于一些传统程序想要移植到 Pod 中，几乎不需要做太多的修改。\nPod 间通信 刚才说同一个 Pod 内的容器都在同一个 Node 上，因此不会出现跨节点通信的问题，但是在 Pod 层面，作为 Kubernetes 的基本调度单位，不同的 Pod 是很有可能被调度到不同的 Node 上的，当然也有可能被调度到同一个 Node 上，因此对于 Pod 间的通信，应该分为两种来探讨：\n 相同 Node 下 Pod 间通信 不同 Node 下 Pod 间通信  相同 Node 下 Pod 间通信 每一个 Pod 都有一个真实的全局IP地址，同一个 Node 内的不同 Pod 之间可以直接采用对方Pod的IP地址通信，而且不需要采用其他发现机制，例如 DNS、Consul 或者 Etcd。\n同一个 Node 下，不同的 Pod 都通过 Veth 设备对连接至 docker0 网桥，Docker bridge 模式在之前的 Docker 网络实现里已经讲过了，Pod 的 IP 都是从 docker0 网桥上动态分配的，图上的 Pod1、Pod2、docker0 网桥它们三者属于同一个网段，即它们之间是可以直接通信的，这个很好理解。\n对于每个 Pod 的 eth0 这一点，难道大家没有疑问吗？为什么 Pod 是一个抽象的虚拟的概念，自己还能有一个独立的网络协议栈，即 Network Namespace，还能挂载一个 Veth 设备？实际上这里的图上没有划出来，每个 Pod 默认会有一个 pause 容器 (实际名称：google_containers/pause)，可以认为是 Pod 的一个「管家容器」，pause 容器负责了包括 Pod 网络相关的一些初始化工作，pause 容器使用的就是之前讲到的 Docker 的默认网络通信模型 Bridge，pause 通过 Veth 设备对与 docker0 桥接，而 Pod 内的其它容器采用了非默认的网络配置和映射容器的模型，指定映射目标容器到 Pause 容器上，这么做的目的很简单，为实现一个 Pod 内多个容器，本身没有很好的方式进行连接，pause 提供一个 Pod 内部的通信「桥梁」，为什么不是后一个容器关联前一个容器的方式呢？这种方式的话一旦前一个容器启动不起来或者挂掉，后面的容器都会跟着受影响。\n不同 Node 下 Pod 间通信 在同一个 Node 下的 Pod 间通信因为可以直接通过 docker0 桥接实现，因此很容易理解，但是在不同 Node 下的 Pod 间通信如何实现就是个很有学问的事情了。\n首先我们要知道的是，每个宿主机上 docker0 网桥为 Pod 分配的都是私有 IP，而 Kubernetes 要求网络对 Pod 的地址是平面且直达的，说白了就是在集群中可以通过 Pod 的私有 IP 在各个不同 Node 之间通信。因此我们可以知道，对于 Pod IP 的规划是非常重要的，要实现上面讲的可以在集群内部使用私有 IP 进行不同 Node 间的 Pod 通信，最起码要保证在集群层面这些私有 IP 一定是不冲突的才行，注意，这些 Pod 私有 IP 是保存在 Etcd 集群中的。\n另外，我们知道，不同 Node 之间的通信一定是要经由宿主机的物理网卡，因此要实现 Pod 在不同 Node 之间的通信，还要通过 Node 的 IP 进行寻址和通信，这也是需要关注的一点。\n综上我们可知，对于不同 Node 下的 Pod 间通信，核心是满足两点：\n Pod IP 实现集群层级的不冲突。Pod 的 IP 分配虽然是有本地 docker0 负责，但是具体的地址规划一定是要在集群层面，保证其不冲突，这是通过私有 Pod IP 跨 Node 通信的基本条件； Pod IP 借助 Node IP 进行寻址访问。Node 之间通信的桥梁还是 Node 实实在在的物理网卡，因此需要找到一种方法，将 Pod IP 与 Node IP 关联起来，通过这个关联实现不同 Node 的 Pod 之间的访问。  对于第一点，实现 Pod IP 在集群层级的不冲突，我们需要对 docker0 的地址段进行规划，保证每个 Node 上的 docker0 网段都是没有冲突的，针对这一点，可以手工配置，当然如果是小集群还好，如果是大规模集群的话，我认为这是在扯淡，因此，应该有不这么反运维工程师的方案，比如做一个分配规则，让程序自己去分配地址段，这是很容易想到的，没错，多亏了 Kubernetes 的 CNI（容器网络接口），有一些优秀的 Kubernetes 网络增强软件就可以对接进来帮我们完成这些工作，典型的比如 Flannel、Calico。\n对于第二点，其核心目标就是实现 Pod IP 经由 Node IP 的寻址，也就是说需要有一个机制知道 Pod IP 具体在哪一个 Node 上，通过宿主机将数据转发到目标 Node 上，然后再由目标 Node 将数据转发到具体的本地 docker0 上，最后转发到目标 Pod 中，整个过程大致如图所示：\n一些知名的云平台本身就设计实现 Pod 的 IP 管理，因此 Pod 的通信可以借助平台层的网络设计打通，但是在大部分情况下，尤其是一般企业自己维护的 Kubernetes 集群上，可能无法享受到这种机制，因此还需要自己进行相关的网络配置，让网络满足 Kuberntes 的要求之后才能实现 Pod 之前的正常通信，进而实现集群的正常运转，就像上面第一点中说的那样，好在 Kubernetes 的高可扩展性，通过 CNI 机制，一些网络增强组件（Flannel、Calico 等）实现了上面这些网络要求。但是基本的原理，基本的需求是上面描述的，这个是必须要理解的，但是对于其实际实现，每个增强组件都有自己的实现方案，这里没法一个一个去详细介绍，心有余而力不足，真有时间的话会好好研究并且撰写相关的博文与大家分享。\n"
},
{
	"uri": "https://blog.yingchi.io/tags/docker.html",
	"title": "docker",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.yingchi.io/posts/2020/4/docker-k8s-network-1.html",
	"title": "Kubernetes &amp; Docker 网络原理（一）",
	"tags": ["kubernetes", "docker", "network"],
	"description": "",
	"content": "Docker 网络实现 平时在进行 Kubernetes 开发和运维的时候，接触到的最多的概念应该就是 Docker 与 Kubernetes 的网络概念了，尤其是 Kubernetes，各种各样的 IP，Port，有时候会混淆，因此有必要对 Docker 和 Kubernetes 的底层网络实现进行学习。这篇文章呢就先针对 Docker 的网络实现进行一下分析介绍。\nDocker 网络基础 Docker 的网络实现主要利用到的还是 Linux 网络相关的技术，如 Network Namespace、Veth 设备对、网桥、iptables、路由。\nNetwork Namespace 基本原理 作用可以用一句话概括：\n实现 Linux 网络虚拟化，即容器间网络协议栈层面的隔离\n通过 Network Namespace 技术就可以实现不同的 Docker 容器拥有自己完全隔离的网络环境，就像各自拥有自己独立的网卡一样。不同的 Network Namespace 下默认是不可以直接通信的。\nLinux 的 Network Namespace 中可以有自己独立的路由表及独立的 iptables 设置来提供包转发、NAT 及 IP 包过滤等操作。为了隔离出独立的协议栈，需要纳入命名空间的元素有进程、套接字、网络设备等。进程创建的套接字必须属于某个命名空间，套接字的操作也必须在命名空间中进行。同样，网络设备也必须属于某个命名空间。因为网络设备属于公共资源，所以可以通过修改属性实现在命名空间之间移动。\nLinux 的网络协议栈是非常复杂的，这里因为毕竟不是做系统底层开发，所以争取从概念层面对于 Linux 的 Network Namespace 这种网络隔离机制进行理解：\n通过查阅相关书籍知道，Linux 网络协议栈为了支持 Namespace 这种隔离机制，方法就是让一些与网络协议栈相关的全局变量称为一个 Network Namespace 变量的成员，协议栈函数调用时指定 Namespace 参数，这个就是 Linux 实现 Network Namespace 的核心原理，通过这种方式，实现一些协议栈全局变量的私有化，保证有效的隔离。\n新生成的 Network Namespace 里只有一个回环设备，其它网络设备都不存在，Docker 容器都是在启动时才创建和配置的这些网络设备。物理设备一般只能关联到 root 这个 Network Namespace 中，而虚拟的网络设备则可以关联到自定义的一些 Network Namespace 中，而且可以在这些 Namespace 中转移。\nNetwork Namespace 代表的是一个完全独立的网络协议栈，即与外界是隔离的，可以理解为连网线都没连接的一台”主机“，因此如何实现 Network Namespace 下与外界进行网络通信是个问题，最基本的实现就是：Veth 设备对\n注意：对于 Network Namespace 相关的操作可以使用 Linux iproute2 系列工具实现，要求 root 用户\nVeth 设备对 Veth 其实就是虚拟的以太网卡，平时在 Linux 下配置以太网卡时我们一般见到的都是包含 ethxxx 这种名字，这里加了个 V，明显就是指虚拟的网卡。因此 Veth 设备对就更好理解了，刚才提到引入Veth设备对的核心目的就是是为了实现两个完全隔离的 Network Namespace 之间通信，说白了就是利用它可以直接将两个 Network Namespace 连接起来，所以Veth设备都是成对出现的，才称作是 Veth 设备对，最贴切的形容就是：像一对以太网卡，并且中间有一根直连的网线，就构成了所谓的设备对。既然是一对网卡，那么我们将其中一端称为另一端的peer。在 Veth 设备的一端发送数据时，它会将数据直接发送到另一端，并触发另一端的接收操作。\n值得一提的是，在 Docker 实现中，将 Veth 的一端放进容器 Network Namespace 后，Veth 名称会被改为 eth\u0026hellip; 不知道的还以为是本地网卡呢！\nLinux 网桥 网桥的作用，顾名思义，就是一个桥接的作用，桥接的对象是如果网络设备，目的是实现各网络中的主机之间的相互通信。\n网桥是一个二层的虚拟网络设备，把若干个网络接口“连接”起来，以使得网络接口之间的报文能够互相转发，二层网络的转发依据就是网络设备的 MAC 地址。网桥能够解析收发的报文，读取目标 MAC 地址的信息，和自己记录的 MAC 表结合，来决策报文的转发目标网络接口。为了实现这些功能，网桥是具有自学习功能的，它会学习源 MAC 地址。在转发报文时，网桥只需要向特定的网口进行转发，来避免不必要的网络交互。如果它遇到一个自己从未学习到的地址，就无法知道这个报文应该向哪个网络接口转发，就将报文广播给所有的网络接口（报文来源的网络接口除外），其中，为了适应网络拓扑的变化，网桥学习到的 MAC 地址表是有过期时间的，长时间北邮收到对应 MAC 回发的包，就认为设备已经不在那个转发端口上了，下一次给这个 MAC 的包就会以广播的形式转发。\nLinux内核是通过一个虚拟的网桥设备（Net Device）来实现桥接的。这个虚拟设备可以绑定若干个以太网接口设备，从而将它们桥接起来，这种 Net Device 网桥和普通的设备不同，最明显的一个特性是它还可以有一个IP地址。这里说 Linux 网桥和传统的交换机概念还不能划等号，因为交换机是个单纯的二层设备，对于报文的操作要么转发要么丢弃，而 Linux 网桥除此之外，还有可能会将报文送至协议栈上层，即网络层，因此 Linux 网桥既可以看作二层设备，又可以看作三层设备。\nLinux 路由 Linux 系统包含一个完整的路由功能。当 IP 层在处理数据发送或者转发时，会使用路由表来决定发往哪里。在通常情况下，如果主机与目的主机直接相连，那么主机可以直接发送IP报文到目的主机，这个过程比较简单。例如，通过点对点的链接或网络共享，如果主机与目的主机没有直接相连，那么主机会将 IP 报文发送给默认的路由器，然后由路由器来决定往哪里发送IP报文。 路由功能由 IP 层维护的一张路由表来实现。当主机收到数据报文时，它用此表来决策接下来应该做什么操作。当从网络侧接收到数据报文时，IP 层首先会检查报文的 IP 地址是否与主机自身的地址相同。如果数据报文中的 IP 地址是主机自身的地址，那么报文将被发送到传输层相应的协议中。如果报文中的 IP 地址不是主机自身的地址，并且主机配置了路由功能，那么报文将被转发，否则，报文将被丢弃。\nDocker bridge 网络实现 Docker 支持下列四种网络模式：bridge 模式（默认）、none 模式、host 模式、container 模式。由于目前涉及到容器云运维的工作都是以 Kubernetes 为核心，而 Docker 的这四种网络通信模式中只有 bridge 模式在 Kubernetes 中被用到，因此主要是介绍一下 Docker 的 bridge 网络模式。\nDocker 默认为容器采用 bridge 网络模式。在 bridge 模式下，Docker Daemon 首次启动时会创建一个虚拟的网桥，默认的名称是 docker0，然后在私有网络空间中给这个网桥分配一个子网（通常 IP 以 172 开头）。针对由Docker创建的每一个容器，都会创建一个之前讲的 Veth 设备对，其中一端关联到网桥上，另一端使用 Linux 的 Network Namespace 技术，映射到容器内的 eth0 (Veth) 设备，然后从网桥的地址段内给eth0接口分配一个IP地址，相应的 MAC 地址也根据这个 IP 地址进行相应的生成操作。（是的，你没看错，可以分配 IP\u0026hellip;所以说不要当作是交换机）\n经过 bridge 桥接之后，在同一台主机内的容器之间可以相互通信，不同主机上的容器暂时还不能相互通信，实际上它们甚至有可能在相同的网络地址范围内（不同主机上的docker0的地址段可能是一样的）。\nDocker 网络局限与简单探讨 从 Docker 支持的网络模式上我们就能感受到，Docker 从一开始就没有考虑到这种大规模容器集群，容器跨主机通信的问题，像 Kubernetes 这种容器编排框架是之后才发布的，因此，Docker 网络最大的局限性就在于一开始没有考虑到多主机互联的网络解决方案。\n从 Docker 的设计理念来讲，其一直以来的理念都是“简单为美”，Docker 最大的贡献就是让容器化这个提了几十年的概念在如今”飞入寻常百姓家“，我认为做到这一步其实已经是非常优秀了，这也是Docker迅速走红的一个原因。\n综上，对于大规模的分布式集群，容器集群，如何实现容器间优雅的跨主机节点通信的问题，就由一些其它的框架来解决，比如之后要写的 Kubernetes，之后我会详细地介绍在 Kubernetes 中是如何解决其包含容器在内各个组件网络通信问题的。\n"
},
{
	"uri": "https://blog.yingchi.io/posts/2020/4/kubernetes-resources.html",
	"title": "理解 Kubernetes 的 Resource 设计概念",
	"tags": ["kubernetes"],
	"description": "",
	"content": " Kubernetes 是一个完全以资源为中心的容器编排平台，这一点从 kube-apiserver 对外暴露的 REST API 设计上其实就能很明显地感受到。Kubernetes 的生态系统围绕着诸多组件资源的控制维护而运作，因此也可以认为它本质上是一个「资源控制系统」\n Group / Version / Resource 针对于资源这一概念，如果在一个庞大而复杂的容器编排平台上仅设计这么一个简单的「资源」语义显然是有点单薄，或者说表达力过于欠缺，因此对于资源这么一个概念，在 Kubernetes 上又进行了分组和版本话，于是就有了我们平时运维与开发中常见到的一些术语：Group / Version / Resource / Kind，分别代表的意义：资源组 / 资源版本 / 资源 / 资源种类。\n他们之间的关系是这样的：\n Kubernetes 系统支持多个 Group(资源组)； 每个 Group 支持多个资源版本(Version)； 每个资源版本又支持多种资源(Resource)，部分资源还拥有自己的子资源； Kind 与 Resource 属于同一级概念，Kind 用于描述 Resource 的种类；  定位一个资源的完整形式如下：\n\u0026lt;GROUP\u0026gt;/\u0026lt;VERSION\u0026gt;/\u0026lt;RESOURCE\u0026gt;[/\u0026lt;SUBSOURCE\u0026gt;] 以 Deployment 为例：apps/v1/deployments/status\n在 Kubernetes 中还有一种描述资源的概念叫做「资源对象」(Resource Object)，其描述形式为：\n\u0026lt;GROUP\u0026gt;/\u0026lt;VERSION\u0026gt;, Kind=\u0026lt;RESOURCE_NAME\u0026gt; 以 Deployment 为例：apps/v1, Kind=Deployment\n资源概念的一些基本特点：\n 每个资源都有一定数量的操作方法，称为 Verbs，如 create / delete / update / get / list / watch \u0026hellip;（8种）； 每个资源 Version 至少有两种，包括一个面向用户请求的外部版本，还有 api-server 内部使用的内部版本； Kubernetes 资源整体上分为内置资源以及 Custom Resources 自定义资源，其中 CR 通过 CRD 自定义资源定义实现；  Group 资源组，Kubenetes API Server 也称为 APIGroup，其有如下特点：\n 资源组的划分依据是资源的功能； 支持不同资源组中拥有不同资源版本，从而方便组内资源迭代升级； 支持不同 Group 内有同名 Kind 的 Resource； 支持通过 CRD 扩展自定义资源； 使用 kubectl 命令进行资源交互时，可以不指定 Group；  有组名 Group 及无组名 Group\n包含拥有组名的 Group 和没有组名的 Core Groups（如 v1/pods），其 HTTP 请求方式也有差别：\n有组名 Group 资源: .../apis/\u0026lt;GROUP\u0026gt;/\u0026lt;VERSION\u0026gt;/\u0026lt;RESOURCE\u0026gt; 无组名 Group 资源: .../api/\u0026lt;VERSION\u0026gt;/\u0026lt;RESOURCE\u0026gt; Version Kubernetes 的资源版本 Version 采用的是语义化的版本号，版本号以 v 开头，版本号后面跟着版本的测试阶段(Alpha -\u0026gt; Beta -\u0026gt; Stable)，例如：v1alpha1、v1beta1、v2stable1\n版本的各个测试阶段情况如下：\n Alpha 阶段：内部测试版本，存在很多的缺陷和漏洞，版本极其不稳定，官方随时可能会放弃支持该版本，仅仅用于开发者的测试使用，Alpha 版本中的功能默认情况下会被禁用。常见命名方式如 v1alpha1； Beta 阶段：相对稳定版本，经过了官方和社区的测试，版本迭代时会有些许改变，但不会像 Alpha 版本一样不稳定，Beta 阶段下的功能默认是开启的。常见命名方式如 v2beta1； Stable 阶段：正式发布版本，功能稳定版本，基本形成了产品，默认情况下所有功能全部开启，命名方式一般不加 stable，直接是 v1、v2 这种；  Resource Resource 是 Kubernetes 中的核心概念，Kubernetes 的本质就是管理、调度及维护其下各种 Resources。\n Resource 实例化后称为一个 Resource Object； Kubernetes 中所有的 Resource Object 都称为 Entity； 可以通过 Kubenetes API Server 去操作 Resource Object；  Kubernetes 目前将 Entity 分为两大类：\n Persistent Entity：即 Resource Object 创建后会持久存在，绝大部分都是 PE，如 Deployment / Service； Ephemeral Entity: 短暂实体，Resource Object 创建后不稳定，出现故障/调度失败后不再重建，如 Pod；  资源操作方法 虽然在 Etcd 层面而言，对于资源的操作最终也只转换为增删改查这些基本操作，但是抽象到资源层面，Kubernetes 赋予了资源比较多的操作方法，之前提到过，称之为「Verbs」，分别是 create / delete / deletecollection / patch / update / get / list / watch，我们仍然可以把它们归到增删改查四大类:\n 增：  create：Resource Object 创建   删：  delete：单个 Resource Object 删除 deletecollection：多个 Resource Objects 删除   改：  patch：Resource Object 局部字段更新 update：Resource Object 整体更新   查：  get：单个 Resource Object 获取 list：多个 Resource Objects 获取 watch：Resource Objects 监控    Resource 与 Namespace Kubernetes 同样支持 Namespace（命名空间）的概念，可以解决 Resource Object 过多时带来的管理复杂性问题。Namespace 有如下几个特点；\n 每个 Namespace 可以视作「虚拟集群」，即不同的 Namespace 间可以实现隔离； 不同的 Namespace 间可以实现跨 Namespace 的通信； 可以对不同的用户配置对不同 Namespace 的访问权限；  因此我们知道，Namespace 即可实现资源的隔离，同时有能满足跨 Namespace 的通信，是一个非常灵活的概念，在很多场景下，比如多租户的实现、生产/测试/开发环境的隔离等场景中都会起到重要作用。\nResource Manifest File 资源对象描述文件 无论是 Kubernetes 的内置资源还是通过 CRD 定义的 Custom Resource，都是通过资源对象描述文件进行 Resource Object 的创建。\nKubernetes 中 Manifest File 可以通过两种格式来定义：YAML 和 JSON，无论格式如何，Manifest File 各个字段都是固定的意义：\n apiVersion：注意这里的 APIVersion 其实指的是 APIGroup/APIVersion，如 Deployment 可以写为 apps/v1，而对于 Pod，因为它属于 Core Group，即无名 Group，因此省略 Group，写为 v1 即可； kind：Resource Object 的种类； metadata：Resource Object 的元数据信息，常用的包括 name / namespace / labels； spec：Resource Object 的期望状态（Desired Status） status：Resource Object 的实际状态（Actual Status）  参考  《Kubernetes 源码剖析》郑东旭  "
},
{
	"uri": "https://blog.yingchi.io/posts/2020/4/kubernetes-rolling-update.html",
	"title": "Kubernetes Rolling Update 滚动升级",
	"tags": ["kubernetes"],
	"description": "",
	"content": " 用户希望应用程序始终可用，而开发人员则需要每天多次部署它们的新版本。在 Kubernetes 中，这些是通过滚动更新（Rolling Updates）完成的。 滚动更新 允许通过使用新的实例逐步更新 Pod 实例，零停机进行 Deployment 更新。\n Kubernetes Rolling Update 基本概念 概念 当集群中的某个服务需要升级时，传统的做法是，先将要更新的服务下线，业务停止后再更新版本和配置，然后重新启动并提供服务。这种方式很明显的一个问题就是：会导致服务较长时间不可用，并且在大规模服务场景下会产生极大的工作量。\n滚动更新就是针对多实例服务的一种不中断服务的更新升级方式。一般情况下，对于多实例服务，滚动更新采用对各个实例逐个进行单独更新而非同一时刻对所有实例进行全部更新的方式。\n对于 kubernetes 集群部署的 service 来说，rolling update 就是指一次仅更新一个pod，并逐个进行更新，而不是在同一时刻将该 service 下面的所有 pod 全部停止，然后更新为新版本后再全部上线，rolling update 方式可以避免业务中断。\n特点 优点：\n 业务不中断，用户体验影响较小，较平滑 相对于蓝绿部署，更加节约资源——它不需要运行两个集群、两倍的实例数  滚动更新也并不是银弹，有很多问题需要考虑到，比如：因为是逐步更新，那么我们在上线代码的时候，就会短暂出现新老版本不一致的情况，如果对上线要求较高的场景，那么就需要考虑如何做好兼容的问题。\nK8S 基于 Deployment 的 Rolling Update kubernetes 的 Deployment 是一个相比较早前 Replication Controller 以及现在的 Replica Set 更高级别的抽象。Deployment会创建一个Replica Set，用来保证Deployment中的Pod的副本数。要 rolling-update deployment 中的 Pod，只需要修改 Deployment 自己的yml 文件并应用即可。这个修改会创建一个新的 Replica Set，在增加这个新 RS 的 pod 数的同时，减少旧RS的pod，直至完全升级。而这一切都发生在 Server 端，并不需要 kubectl 参与。\n创建一个Deployment yml文件nginx-demo-dm.yml：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy spec: replicas: 3 selector: matchLabels: app: nginx-deploy minReadySeconds: 10 template: metadata: labels: app: nginx-deploy version: v1 spec: containers: - name: nginx-deploy image: nginx:1.10.1 ports: - containerPort: 80 protocol: TCP 创建该deployment：\nkubect create -f nginx-demo-dm.yml --record 直接修改该 deployment 的 YAML 文件，修改到目标版本配置，如下 ：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy spec: replicas: 3 selector: matchLabels: app: nginx-deploy minReadySeconds: 10 template: metadata: labels: app: nginx-deploy version: v2 spec: containers: - name: nginx-deploy image: nginx:1.11.9 ports: - containerPort: 80 protocol: TCP 修改了其中一个 label，修改了 image 的 tag，然后执行：\nkubectl apply -f nginx-demo-dm.yml --record 没错，不需要像 kubectl rolling-update [SVC] -f [TARGET-RC]这种 Replication Controller 时代的滚动更新形式，只需要像平常一样 apply -f即可，唯一不同的是需要加一个 --record ，目的很明显，方便回滚。\n此时可以查看 Deployment 具体的滚动更新过程：\nkubectl describe deployment nginx-demo 由于通过 --record 记录了更新历史，可以通过 kubectl rollout history 命令查看\nkubectl rollout history deployment nginx-demo 回滚 如果需要回滚到之前一个版本：\nkubectl rollout undo deployment nginx-demo 回滚到指定版本：\nkubectl rollout undo deployment xxx-deployment --to-version=2 参考   https://kubernetes.io/zh/docs/tutorials/kubernetes-basics/update/update-intro/\n  https://www.cnblogs.com/tylerzhou/p/10995774.html\n  "
},
{
	"uri": "https://blog.yingchi.io/posts/2020/4/kubernetes-arch.html",
	"title": "Kubernetes 架构浅析",
	"tags": ["kubernetes"],
	"description": "",
	"content": "前言 Kubernetes 就像它在英语中原意“舵手”一样，指挥，调度\u0026hellip; 它的定位就是这么一个容器编排调度基础平台，来源于 Google 内部的容器集群管理平台 Borg，Borg 发布于 2003 年，从最初的一个小项目，到如今成为支撑起 Google 内部成千上万的应用程序和任务作业的内部集群管理系统，它的成功不言而喻。2014 年，Google 便以 Borg 开源版本的名义发布了 Kubernetes，这是振奋人心的，随后巨硬、IBM、RedHat 一些大佬企业也加入 Kubernetes 社区添砖加瓦，项目日益成熟，社区非常活跃，如今的 Kubernetes 项目也已经成为了开源项目中最耀眼的其中之一。\nKubernetes 的成功在于它填补了大规模容器集群的编排调度管理平台的空白，在此之前，大家都仿佛在云时代中做着石器时代的活，费时费力地部署管理自己的应用，虽然容器概念已经流行，但是还是用着最原始的方式去使用它们，虽然有一些技术框架，如 Docker Swarm 尝试改变这一现状，但是反响并不好，直到 Kubernetes 的出现，人们都惊呆了，原来 Kubernetes 与 Docker 与 微服务可以这么有机地结合？难以置信，它们虽然是不同的项目不同的设计思想，但是当融合在一起的时候是如此的完美。\n因此，我们的确有必要去学习去了解优秀的 Kubernetes，当然，学习它的架构实现，从宏观角度理解它的运转机制就是必不可少的环节。\n架构概述 Kubernetes 系统架构整体采用的是 C/S 的架构，即 Master 作为 Server，各个 Worker 节点作为 Client，在一个面向生产环境的集群中，通常可以采用多个 Master 节点实现 HA。\n然后从 Master 与 Worker 两种不同的节点类型来概述一下它们的「职责」\nMaster Node 主要职责：  管理集群所有的 Node； 调度集群的 Pod； 管理集群的运行状态；  主要组件：  kube-apiserver: 负责处理资源的 CRUD 请求，提供 REST API 接口； kube-scheduler: 负责集群中 Pod 资源的调度（哪个 Pod 运行在 哪个 Node 上）； kube-controller-manager: 控制器管理器，自动化地管理集群状态（如自动扩容、滚动更新）；  Worker Node 主要职责：  管理容器的生命周期，网络，存储等； 监控上报 Pod 的运行状态；  主要组件：  kubelet: 管理容器的生命周期，与 Master 节点进行通信，可理解为 Kubernetes 在 Worker 节点的 Agent； kube-proxy: 负责 Kubernetes Service 组件的通信，原理是为当前节点 Pod 动态地生成 iptables 或 ipvs 规则，并且与 kube-apiserver 保持通信，一旦发现某一个 Service 的后端 Pod 改变，需要将改变保存在 kube-apiserver 中； container engine: 负责接收 kubelet 指令，对容器进行基础地管理；  组件浅析 [Master] kube-apiserver 顾名思义，「apiserver」即本质是提供 API，而且是 REST API，我们提到 REST API 总是会想到「资源」这个概念，没错，这里的 kube-apiserver 就是为 Kubernetes 集群中的各种资源提供 CRUD 的 REST API。\n值得注意的是，kube-apiserver 是集群中唯一与 Etcd 集群交互的核心组件，原因很简单，集群的状态信息、元数据，包括集群资源对象的信息都是存放在 Etcd 存储集群的 /registry 目录下，kube-apiserver 涉及到这些资源的 CRUD，肯定会频繁的与 Etcd 打交道。\n然后还要了解的是，作为集群的运维管理或开发人员，如何与 kube-apiserver 进行交互呢？常见的与 kube-apiserver 的交互方式分为以下两种：\n kubectl 命令行工具: 其与 kube-apiserver 通过 HTTP/JSON 协议进行交互。一次 kubectl 命令的执行流程是这样的：用户编写 kubectl 命令并执行，命令转换成相应的 HTTP 请求，然后将请求发送给 kube-apiserver，kube-apiserver 接收到请求后进行相应的处理，将结果返回给 kubectl，kubectl 接收到响应然后回显结果。 client-go: 对于 kubenetes 的二次开发而言，我们更期望通过代码的方式去管理 kubernetes 资源，如你所想，官方提供了 Go 语言的客户端来满足二次开发需求，client-go 针对 Kubernetes 做了非常多的优化，并且很多 Kubernetes 的核心组件，包括 kube-scheduler、kube-controller-manager 等内部都是通过 client-go 实现与 kube-apiserver 交互，因此对于想基于 kubernetes 做二次开发的云计算开发/运维开发相关的朋友，掌握 client-go 的使用是很有必要的，笔者这段时间也是在这方面进行了一定的实践，发现 client-go 的确是一个易用、高效的工具包，有时间可能会单独针对 client-go 写一篇文章。  [Master] kube-scheduler Kubernetes 集群默认的调度器。我们知道 Kubernetes 中基本的调度单位就是 Pod，那么 kube-scheduler 的职责从宏观上将其实就是为集群中每一个 Pod 资源对象寻找一个合适的 Worker Node 去运行，如果从微观上讲呢？仅仅是在 Pod 的 spec.nodeName 字段上填上一个 Worker Node 的名称，当然，为了填上这么一个名称，kube-scheduler 内部进行的算法计算还是非常复杂的。\n调度算法分为两种：预选调度算法和优选调度算法，前者是一票否决的方式，满足所有的才可以调度，后者是计算评分，评分高的调度，相同的随机：\n 预选调度算法：从所有的节点当中，去排除那些完全不能符合对应pod的基本运行要求的节点；  CheckNodeCondition： 检查节点是否正常 HostName： 检查Pod对象是否定义了 pod.spec.hostname MatchNodeSelector：pods.spec.nodeSelector PodFitsResources：检查Pod的资源需求是否能被节点所满足； NoDiskConflict: 检查Pod依赖的存储卷是否能满足需求； NoExecute：污点，pod如果无法容忍会被驱离 CheckNodePIDPressure：检查节点pid数量资源压力过大 CheckNodeDiskPressure：检查节点磁盘IO是否过高 MatchInterPodAffinity：检查节点是否满足POD是否满足亲和或者反亲和（需要自己定义） \u0026hellip; （不再一一列举）   优选调度算法 : 计算基于一系列的算法函数，把每一个节点的数据输入进去计算优先级，计算完以后，在排序，取得分最高的，就是我们最佳匹配的节点；优选函数的话简单列举一下：  LeastRequested： 由节点的空闲资源与节点的总容量来比较一个比值，根据空闲比例来评估 BalancedResourceAllocation：CPU和内存资源被占用率相近的胜出 NodePreferAvoidPods: 优先级较高，根据节点是否由注解信息来判定节点注解信息“scheduler.alpha.kubernetes.io/preferAvoidPods” TaintToleration：将Pod对象的spec.tolerations列表项与节点的taints列表项进行匹配度检查，匹配条目越多，得分越低； SeletorSpreading： 把同一个标签选择器的pod散开至多个节点 InterPodAffinity： 匹配项越多得分越高 NodeAffinity： 节点亲和型 MostRequested： 跟LeastRequested相反，空闲越小分越高，尽可能把一个节点资源用完（默认关闭） NodeLabel： 根据节点是否有标签（默认关闭） ImageLocality：根据满足当前Pod对象需求的已有镜像的体积大小之和 （默认关闭）    kube-scheduler 支持 HA，通过基于 Etcd 集群的分布式锁实现 Leader 选举机制，通过 Kube-apiserver 的资源所进行选举竞争，获得锁的实例成为 Leader 节点，执行调度主逻辑，其它 Candidate 节点处于阻塞状态。\n[Master] kube-controller-manager kube-controller-manager 管理的是集群状态，包括了资源状态，节点状态等，而且是自动化的控制，即，它的核心关键就是确保集群始终处于预期状态，对于一些资源而言，就是控制它们向 spec 里配置的期望状态收敛。\nkube-controller-manager 称为“控制器管理器”，是因为他提供了一些 Controllers，例如 DeploymentControllers、NamespaceControllers、PsersistentVolumeControllers，控制器通过 kube-apiserver 组件提供的接口对资源对象的 status 进行监控，一旦资源 status 偏离 spec 预期状态时，就会尝试将资源修复到 spec 预期的状态。\n同样，kube-controller-manager 也支持 HA，通过基于 Etcd 集群的分布式锁实现 Leader 选举机制，通过 Kube-apiserver 的资源所进行选举竞争，获得锁的实例成为 Leader 节点，执行 controller-manager主逻辑，其它 Candidate 节点处于阻塞状态。\n[Worker] kubelet kubelet 是 Worker Node 上的核心组件之一，他的核心职责就是对 Worker Node 上的 Pod 资源对象的生命周期进行一个管理。kubelet 与 Master Node 上的 kube-apiserver 进行交互，得到下发的任务，然后去执行对 Pod 的管理逻辑，也就是起到一个代理的作用；同时，kubelet 也会定期监控节点的资源使用状态并且上报 kube-apiserver，这些反馈来的数据会辅助 kube-scheduler 执行相关的调度逻辑。kubelet 也会对 Worker Node 上的镜像和容器做一个清理工作，充当着一个 Worker Node 资源管家的角色。\nkubelet 另一块儿比较核心的概念是，它定义了三种接口：\n CRI（Container Runtime Interface）：容器运行时接口，提供计算资源 CNI（Container Network Interface）：容器网络接口，提供网络资源 CSI（Container Storage Interface）：容器存储接口，提供存储资源  对于接口这个概念，大家一定不陌生，某种程度上接口也可以理解为「协议」，通过接口，可以将某些逻辑的具体实现解耦，不同的组件，只要实现这个接口，就可以拿来使用，Kubernetes 引入了 CRI、CNI、CSI 这三个高度开放的接口，分别从容器的运行时、网络、存储三个层面提高了其可扩展性.\n简单来说，以 CRI 为例，虽然当前默认的容器运行时还是 Docker，但是有了 CRI 之后，我们就可以引入其它优秀的容器运行时作为我们的 Pod 后端，就比如 kata，比如 rkt\u0026hellip;；以 CNI 为例，不用多说，Calico / Flannel 这种大家都是耳熟能详了，各有各的优点，百花齐放。\n[Master] kube-proxy 说白了 kube-proxy 作用主要是负责 Kubernetes 中 Service 资源功能的实现，即实现了内部从 Pod -\u0026gt; Service 和外部 NodePort -\u0026gt; Service 的访问。Service 是一组 Pod 的服务抽象，相当于对于 Pod 的一个\u0026quot;LB\u0026rdquo;，负责将请求分发给对应的 Pod。Service 会为这个 \u0026ldquo;LB\u0026quot;提供一个 IP，一般称为 cluster IP。\n Cluster IP 是一个虚拟的 IP，但更像是一个伪造的 IP 网络，原因有以下几点:\n Cluster IP 仅仅作用于 Kubernetes Service 这个对象，并由 Kubernetes 管理和分配P地址 Cluster IP 无法被 ping，他没有一个“实体网络对象”来响应 Cluster IP 只能结合 Service Port 组成一个具体的通信端口，单独的Cluster IP不具备通信的基础，并且他们属于Kubernetes集群这样一个封闭的空间。 在不同Service下的pod节点在集群间相互访问可以通过 Cluster IP   kube-proxy 的工作原理就是通过监控 kube-apiserver 的 Service 与 Endpoint 资源变化，通过 iptables / ipvs 的动态配置，从而实现 Service 后端 Pod 的负载均衡；\n需要注意的是，kube-proxy 仅仅是面向 Service 通信以及 Pod 请求，并不适用于其它场景。\n参考   《Kubernetes 源码剖析》郑东旭\n  https://kubernetes.io/zh/docs/concepts/architecture/\n  https://blog.csdn.net/zisefeizhu/article/details/85702327\n  https://zhuanlan.zhihu.com/p/33390023\n  https://blog.csdn.net/weixin_45413603/article/details/103194387\n  https://my.oschina.net/u/1000241/blog/3022714\n  "
},
{
	"uri": "https://blog.yingchi.io/posts/2020/4/kubeadm-flannel-mannul.html",
	"title": "记一次 Kubeadm 部署 k8s + Flannel",
	"tags": ["kubernetes"],
	"description": "",
	"content": " 以 master 节点的配置为例记录一下在 CentOS 7.0 上使用 kubeadm 部署 kubernetes 集群 + Flannel 插件的过程\n 一、基础环境配置 安装 wget yum install -y wget 配置 YUM 软件源  配置阿里镜像源  wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 配置 kubernetes 源  vi /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg yum clean all yum makecache fast 常用软件安装 yum install -y net-tools yum install -y vim 安装 Docker wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O/etc/yum.repos.d/docker-ce.repo yum -y install docker-ce systemctl start docker systemctl enable docker 配置时间同步 不然后面 Flannel 安装时会出现证书错误\nyum install ntpdate -y ntpdate cn.pool.ntp.org 安装 kubeadm，kubelet 和 kubectl yum install -y kubectl-1.18.0 kubeadm-1.18.0 kubelet-1.18.0 --nogpgcheck 关闭防火墙 systemctl stop firewalld systemctl disable firewalld 禁用 SELinux setenforce 0 vi /etc/selinux/config SELINUX=disabled 关闭 swap vim /etc/fstab # 永久关闭 注释swap那一行 创建 /etc/sysctl.d/k8s.conf 文件，添加如下内容： net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 配置 hosts 文件 根据个人集群规划配置 /etc/hosts\n192.168.99.101 node01 192.168.99.102 node02 192.168.99.103 node03 二、Master 节点部署 生成配置文件 kubeadm config print init-defaults ClusterConfiguration \u0026gt; kubeadm.yaml 修改配置文件 vim kubeadm.yaml apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 192.168.99.101 # 配置 master 节点地址 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock name: node01 # 节点名 --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: registry.aliyuncs.com/google_containers # 配置阿里云镜像地址 kind: ClusterConfiguration kubernetesVersion: v1.18.0 # 配置 kubernetes 版本 networking: dnsDomain: cluster.local podSubnet: 10.244.0.0/16 # 配置 Pod 子网 serviceSubnet: 10.96.0.0/12 scheduler: {} 执行 kubeadm 初始化 kubeadm init --config kubeadm.yaml --ignore-preflight-errors=Swap 初始化后保存好最后一行和 join token 相关的日志，后面 Worker 节点加入集群会用到\nkubeadm join 192.168.204.101:6443 --token abcdef.0123456789abcdef \\  --discovery-token-ca-cert-hash sha256:c8fc8a67017...5b09d8aa1104 根据日志中的要求执行如下命令：\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 检查 Pod 的运行情况：\nkubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-7ff77c879f-5zv6l 1/1 Pending 0 43m kube-system coredns-7ff77c879f-p94cx 1/1 Pending 0 43m kube-system etcd-practice 1/1 Running 0 43m kube-system kube-apiserver-practice 1/1 Running 0 43m kube-system kube-controller-manager-practice 1/1 Running 0 43m kube-system kube-flannel-ds-amd64-6vffj 1/1 Running 0 33m kube-system kube-proxy-dnp4l 1/1 Running 0 43m kube-system kube-scheduler-practice 1/1 Running 0 43m 发现 coredns 处于 Pending 状态，无法成功运行，这是因为集群还没有安装网络插件，接下来我们安装 Flannel\nFlannel 网络插件安装 准备 kube-flannel-rbac.yml kube-flannel.yml 两个文件，国内被墙可能无法 wget 下来，这里直接手动准备文件，我是从本机用 SS 翻墙下载好 scp -r 到 Linux 服务器上的\nkubectl apply -f kube-fannel-rbac.yml kubectl apply -f kube-flannel.yml (文末附 kube-flannel-rbac.yml kube-flannel.yml 两个文件)\nFlannel 安装好后，检查 Pod 运行情况\nkubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-7ff77c879f-5zv6l 1/1 Running 0 43m kube-system coredns-7ff77c879f-p94cx 1/1 Running 0 43m kube-system etcd-practice 1/1 Running 0 43m kube-system kube-apiserver-practice 1/1 Running 0 43m kube-system kube-controller-manager-practice 1/1 Running 0 43m kube-system kube-flannel-ds-amd64-6vffj 1/1 Running 0 33m kube-system kube-proxy-dnp4l 1/1 Running 0 43m kube-system kube-scheduler-practice 1/1 Running 0 43m 注意：coredns 一直 pending 的话，子节点加入就好了，因为主节点有 taints，无法调度\n三、Worker 节点加入 worker 节点的环境配置参考 master 即可，只不过最后的 kubeadm 通过 join 而不是 Init 加入集群，加入集群的命令在上面 master 节点初始化集群后已经回显给出，在 worker 节点运行即可：\nkubeadm join 192.168.204.101:6443 --token abcdef.0123456789abcdef \\  --discovery-token-ca-cert-hash sha256:c8fc8a67017...5b09d8aa1104 --ignore-preflight-errors=Swap 注意加上 --ignore-preflight-errors=Swap，否则可能会报错\n可以加入 --v=2 命令查看 join 过程详情，便于排查故障\n给 Node 打上 Worker 标签 kubectl label node node02 node-role.kubernetes.io/worker=worker kubectl label node node03 node-role.kubernetes.io/worker=worker NAME STATUS ROLES AGE VERSION node01 NotReady master 8m22s v1.18.0 node02 NotReady worker 3m56s v1.18.0 node03 NotReady worker 3m54s v1.18.0  故障排除 1. 提示 [ERROR FileContent\u0026ndash;proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1 解决方式：\necho 1 \u0026gt; /proc/sys/net/bridge/bridge-nf-call-iptables echo 1 \u0026gt; /proc/sys/net/bridge/bridge-nf-call-ip6tables 2. master 初始化时报 [kubelet-check] Initial timeout of 40s passed 解决方式：\nswapoff -a # will turn off the swap kubeadm reset systemctl daemon-reload systemctl restart kubelet iptables -F \u0026amp;\u0026amp; iptables -t nat -F \u0026amp;\u0026amp; iptables -t mangle -F \u0026amp;\u0026amp; iptables -X # will reset iptables 这是在 Stack Overflow 上找到的解决方案：\nhttps://stackoverflow.com/questions/53525975/kubernetes-error-uploading-crisocket-timed-out-waiting-for-the-condition\n3. worker 节点 join 时 [preflight] Running pre-flight checks 之后卡住，\u0026ndash;v=2 输出过程日志发现：connect: no route to host 问题说明：iptables 防火墙问题\n解决方式：\nsystemctl stop kubelet systemctl stop docker iptables --flush iptables -tnat --flush systemctl start kubelet systemctl start docker 4. coredns 一直 pending 子节点加入就好了，因为主节点有 taints，无法调度\n 附 kube-flannel-rbac.yml\n# Create the clusterrole and clusterrolebinding: # $ kubectl create -f kube-flannel-rbac.yml # Create the pod using the same namespace used by the flannel serviceaccount: # $ kubectl create --namespace kube-system -f kube-flannel.yml --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel rules: - apiGroups: - \u0026#34;\u0026#34; resources: - pods verbs: - get - apiGroups: - \u0026#34;\u0026#34; resources: - nodes verbs: - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system kube-flannel.yml\napiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default spec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: \u0026#34;/etc/cni/net.d\u0026#34; - pathPrefix: \u0026#34;/etc/kube-flannel\u0026#34; - pathPrefix: \u0026#34;/run/flannel\u0026#34; readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: [\u0026#39;NET_ADMIN\u0026#39;] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unused in CaaSP rule: \u0026#39;RunAsAny\u0026#39; --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel rules: - apiGroups: [\u0026#39;extensions\u0026#39;] resources: [\u0026#39;podsecuritypolicies\u0026#39;] verbs: [\u0026#39;use\u0026#39;] resourceNames: [\u0026#39;psp.flannel.unprivileged\u0026#39;] - apiGroups: - \u0026#34;\u0026#34; resources: - pods verbs: - get - apiGroups: - \u0026#34;\u0026#34; resources: - nodes verbs: - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flannel data: cni-conf.json: | { \u0026#34;name\u0026#34;: \u0026#34;cbr0\u0026#34;, \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;flannel\u0026#34;, \u0026#34;delegate\u0026#34;: { \u0026#34;hairpinMode\u0026#34;: true, \u0026#34;isDefaultGateway\u0026#34;: true } }, { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;portMappings\u0026#34;: true } } ] } net-conf.json: | { \u0026#34;Network\u0026#34;: \u0026#34;10.244.0.0/16\u0026#34;, \u0026#34;Backend\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;vxlan\u0026#34; } } --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds-amd64 namespace: kube-system labels: tier: node app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux - key: kubernetes.io/arch operator: In values: - amd64 hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.12.0-amd64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.12.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; limits: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; securityContext: privileged: false capabilities: add: [\u0026#34;NET_ADMIN\u0026#34;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds-arm64 namespace: kube-system labels: tier: node app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux - key: kubernetes.io/arch operator: In values: - arm64 hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.12.0-arm64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.12.0-arm64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; limits: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; securityContext: privileged: false capabilities: add: [\u0026#34;NET_ADMIN\u0026#34;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds-arm namespace: kube-system labels: tier: node app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux - key: kubernetes.io/arch operator: In values: - arm hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.12.0-arm command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.12.0-arm command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; limits: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; securityContext: privileged: false capabilities: add: [\u0026#34;NET_ADMIN\u0026#34;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds-ppc64le namespace: kube-system labels: tier: node app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux - key: kubernetes.io/arch operator: In values: - ppc64le hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.12.0-ppc64le command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.12.0-ppc64le command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; limits: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; securityContext: privileged: false capabilities: add: [\u0026#34;NET_ADMIN\u0026#34;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds-s390x namespace: kube-system labels: tier: node app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux - key: kubernetes.io/arch operator: In values: - s390x hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.12.0-s390x command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.12.0-s390x command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; limits: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; securityContext: privileged: false capabilities: add: [\u0026#34;NET_ADMIN\u0026#34;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg 参考  K8S安装部署在centos7下 https://www.cnblogs.com/niewx5201314/p/11663137.html kubeadm join 超时 uploading crisocket: timed out waiting for the condition https://blog.csdn.net/gs80140/article/details/92798027 kubeadm 安装kubetnetes(flannel) https://blog.csdn.net/qq_21816375/article/details/73691684  "
},
{
	"uri": "https://blog.yingchi.io/posts/2020/3/go-goroutine.html",
	"title": "Goroutine 并发模型",
	"tags": ["golang"],
	"description": "",
	"content": "并发基础 在学习 Goroutine 之前，如果对于 Linux 基本的并发模型不了解，那么可能会学的一头雾水，所以一切的一切之前，从 Linux 基本的并发知识说起，复习一下。\n并发与并行  并发（Concurrency）：指宏观上看起来两个程序在同时运行，比如说在单核cpu上的多任务。但是从微观上看两个程序的指令是交织着运行的，你的指令之间穿插着我的指令，我的指令之间穿插着你的，在单个周期内只运行了一个指令。这种并发并不能提高计算机的性能，只能提高效率； 并行（Parallelism）：提到并行时往往涉及到的概念就是分布式/多核/多机这种概念，即一定是指严格物理意义上的同时运行，比如多核cpu，两个程序分别运行在两个核上，两者之间互不影响，单个周期内每个程序都运行了自己的指令，也就是运行了两条指令。这样说来并行的确提高了计算机的效率。所以现在的cpu都是往多核方面发展。  进程与线程   定位：进程是资源分配的最小单位，线程是CPU调度的最小单位；\n  线程依赖于进程而存在，一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程；\n  进程在执行过程中拥有独立的内存单元，而多个线程共享进程的内存资源；\n  创建和撤销开销： 进程的创建和撤销操作开销远大于线程创建和撤销的开销（系统都要为进程分配或回收资源，如内存空间等）；\n  切换开销：进程切换时，涉及到整个当前进程 CPU 环境的保存以及新被调度运行的进程的 CPU 环境的设置。而线程切换只须保存和设置少量寄存器的内容，并不涉及存储器管理方面的操作。即，进程切换的开销也远大于线程切换的开销；\n  通信：由于同一进程中的多个线程具有相同的地址空间，使它们之间的同步和通信的实现比较容易。进程间通信则是通过诸如管道、共享内存、信号、信号量、Socket、消息队列等实现；\n  进程编程调试简单可靠性高，但是创建销毁开销大；线程正相反，开销小，切换速度快，但是编程调试相对复杂；\n  进程间不会相互影响 ；线程一个线程挂掉将导致整个进程挂掉；\n  进程适应于多核、多机分布；线程适用于多核；\n  系统调用 \u0026amp; 用户态内核态 \u0026amp; 进程切换/调度 用户进程生存在用户空间中，无法直接操纵计算机的硬件，但是内核空间中的内核是可以做到的，因此内核会暴露出一些接口供用户进程使用，用户进程通过这些接口去使用内核的功能，进而操控计算机的硬件，这个用户空间与内核空间之间的桥梁，就叫做“系统调用(System call)”，与普通程序函数不同的是，内核调用会导致内核空间的数据存取和指令的执行，而普通函数只在用户空间中起作用，如果普通函数需要对内核空间进行访问，也是借助于系统调用相关函数实现的。\n然后说，用户态和内核态，这是为了保证操作系统安全而建立的一个特性，大部分时间里 CPU 处于用户态，此时 CPU 只能对用户空间进行访问，用户态下的用户进程是不允许访问内核空间的，当用户进程发出系统调用的时候，内核会把 CPU 从用户态切换到内核态，然后执行相关的内核函数，执行完毕后切换回用户态，并把执行结果返回给用户。\n最后说到进程，为了实现一开始说的操作系统并发特性，Linux 操作系统可以凭借 CPU 的强大性能在多个进程之间快速切换，这个过程从专业上讲我们称为进程间的上下文切换，通过这种快速的切换，营造了多个进程同时运行的假象，而每个进程也地以为自己独占 CPU，但是我们要知道的是，同一时刻正在运行的进程仅会有一个。最重要的是，进程的切换是需要付出代价的，就像一开始提到的，进程切换时，涉及到整个当前进程 CPU 环境的保存以及新被调度运行的进程的 CPU 环境的设置，即进程切换的开销是比较大的。此外，除了进程切换，为了使每个生存的进程都有运行的机会，内核还要考虑下次切换时运行哪个进程，何时进行切换，被换下的进程何时重新换上，这些类似的问题称为进程调度。\n上述是程序并发的基础，后面的所有实现都离不开前面这些最基本的底层实现。\n线程模型 线程的实现模型主要有3个，用户级线程模型、内核级线程模型、两级线程模型；三者之间最大的差异在于线程与内核调度实体（Kernel Scheduling Enitity，KSE）之间的对应关系上，KSE 即内核级线程，也就是内核调度的最基本单位：\n 用户级线程模型：用户级线程与 KSE 的对应关系为 M:1，即全部用户线程都映射到一个OS线程上，上下文切换成本最低，但是线程库对线程的调度完全不受内核控制，无法利用多核资源，这种模型存在严重的缺陷，因此现代 OS 都不采用这种线程模型； 内核级线程模型：即进程中每个线程与 KSE 1:1 对应，该模型下的线程由内核负责管理，应用程序对于线程的管理都通过内核提供的系统调用来完成；一对一线程避开了多对一线程的很多弊端，可以真正实现线程的并发运行，但是同时内核级线程模型给内核调度器造成了很大的负担，尤其是当一个进程创建了非常多的线程的时候。尽管如此，还是相比用户级线程有较大优势的，包括 Linux 在内的很多现代操作系统都是以内核级线程模型实现线程的； 两级线程模型：线程与 KSE 对应关系 M:N，显然两级线程模型是在之前两种模型基础上进行优化的，这种模型下，一个进程对应多个 KSE，而应用程序线程与 KSE 并不是一一对应关系。通俗来讲，两级线程模型下，线程库通过 OS 内核创建多个内核级线程，然后通过这些内核级线程对应用程序线程进行调度，应用程序线程与内核级线程动态关联。  两级线程模型下内核资源消耗大大减少，但是缺点是，模型实现比较复杂，所以并没有在操作系统上采用。但是 Go 语言的并发模型就与两级线程模型非常类似，只不过在 Go 中，这种不受操作系统内核管理的独立控制流不叫应用程序线程，而称为 Goroutine\nGoroutine 为什么 Go Scheduler 需要实现 M:N 的方案？\n**线程创建开销大。**对于内核级线程而言，其很多特性均是操作系统给予的，但对于 Go 程序而言，其中很多特性可能非必要的。这样一来，如果是 1:1 的方案，那么每次 go func(){...}都需要创建一个 OS 线程，而在创建线程过程中，内核级线程里某些 Go 用不上的特性会转化为不必要的性能开销，不经济。\n**减少 Go 垃圾回收的复杂度。**依据1:1方案，Go 产生所用用户级线程均交由 OS 直接调度。 Go 的垃圾回收器要求在运行时需要停止所有线程，才能使得内存达到稳定一致的状态，而 OS 不可能清楚这些，垃圾回收器也不能控制 OS 去阻塞线程。\n因此 Go Scheduler 的 M:N 方案出现，就是为了解决上面的问题。\nGo Scheduler Go Scheduler 主要涉及的三个元素：\n  M: Machine，Go runtime 将一个 M 紧密对应于一个 KSE，即内核调度实体；\n  P: Processor， 逻辑处理器，通常表示执行上下文，用于匹配 M 和 G 。P 的数量不能超过 GOMAXPROCS 配置数量，这个参数的默认值为CPU核心数；通常一个 P 可以与多个 M 对应，但同一时刻，这个 P 只能和其中一个 M 发生绑定关系；M 被创建之后需要自行在 P 的 free list 中找到 P 进行绑定，没有绑定 P 的 M，会进入阻塞态。GOMAXPROCS 参数很重要，其决定了 P 的最大数量，也决定了自旋 M 的最大数量。\n  G: Goroutine，Go 的用户级线程，常说的协程的概念，真正携带代码执行逻辑的部分；\n   线程自旋（Spinning Threads）\n线程自旋的核心是避免线程陷入阻塞，简单点说就是“忙等待”，通俗点说就是“死循环”，再通俗点就是“CPU 空转”，缺点很明显，一定程度上是浪费资源的，但是好处也很明显，避免线程阻塞就可以避免线程上下文切换，因此哪怕浪费一些资源，但是可以保证效率与性能，M 自旋就是循环执行一个指定的调度逻辑，就是不停地寻找 G。当然前面也提到过，为了避免过多浪费 CPU 资源，自旋的线程数不会超过 GOMAXPROCS ，这是因为一个 P 在同一个时刻只能绑定一个 M，P 的数量不会超过 GOMAXPROCS，因此可绑定的 M 也不会超过这个数。\n G0 概念: 本质也是 G ，也需要跟具体的 M 结合才能被执行，只不过他比较特殊，其本身就是一个 schedule 函数，这个函数涉及到几个概念：\n  本地运行队列（Local Runable Queue）： 本地是相对 P 而言的本地，每个 P 维护一个本地队列；与 P 绑定的 M 中如若生成新的 G，一般情况下会放到 P 的本地队列；当本地队列满了的时候，才会截取本地队列中 “一半” 的元素放入全局运行队列中；\n  **全局运行队列（Global Runable Queue）：**承载本地队列“溢出”的 G。为了保证调度公平性，schedule 过程中有 1/61 的几率优先检查全局队列，否则本地队列一直满载的情况下，全局队列中的 G 将永远无法被调度到；\n  任务窃取（Stealing）： 目的很简单，就是为了使得空闲（idle）的 M 有活干，别闲着，提高计算资源的利用率。但是窃取也是有章法的，规则是随机从其他 P 的本地队列里窃取 “一半” 的 G。\n  总结一下 Go scheduler 的调度流程就是：\n  1/61 的几率在全局队列中找 G，60/61 的几率在本地队列找 G；\n  如果全局队列找不到 G，从 P 的本地队列找 G；\n  如果找不到，从其他 P 的本地队列中窃取 G；\n  如果找不到，则从全局队列中拿取一部分 G 到本地队列。\n  如果找不到，从网络中 poll G。\n  只要找到了 G， 就会立马丢给 M 执行。\n  当然上述任何执行逻辑如果没有 running 的 M 参与，都是无法真正被执行的，这包括调度逻辑本身。\n总结： Go scheduler 调度的本质就是 P 将 G 合理的分配给某个 M 的过程。\n参考资料   https://www.zhihu.com/question/20862617\n  https://studygolang.com/articles/20991\n  https://rakyll.org/scheduler/\n  https://morsmachine.dk/go-scheduler\n  https://lingchao.xin/post/gos-work-stealing-scheduler.html\n  https://segmentfault.com/a/1190000018775901\n  https://segmentfault.com/a/1190000018777972#articleHeader6\n  https://github.com/golang/go/blob/master/src/runtime/proc.go\n  https://mp.weixin.qq.com/s/uu7J_9nsBu-K7E7Chw5ttw\n  "
},
{
	"uri": "https://blog.yingchi.io/posts/2020/3/go-map.html",
	"title": "哈希表原理 &amp; Go Map 实现",
	"tags": ["golang"],
	"description": "",
	"content": "Go Map 也就是所谓的“ HashTable ”数据结构，有些语言，比如 Python 中称作“字典”，但是无论如何都是一种东西。 HashTable 最重要的特点是：\n 提供键值对形式的的存储结构，即提供键值之间的映射； 具有O(1)的读写性能  HashTable 的思想很简单，但是实现原理思路在不同的语言中都有着些许不同，本文主要针对 Go Map 这种 HashTable 的实现和相关问题展开讨论。\n基本原理 在讨论 Go 的 Map 之前，首先要熟悉 HashTable 的基本原理，当然这些都是上个世纪的知识点了，但是还是有必要深入理解透彻的。\nHashTable 的两个主要概念涉及到：Hash Function 和 冲突处理。\nHash Function 前面说过 Hash Table 是存储键值对的数据结构，所以容易理解，所谓 Hash Function 就是将 key 映射到某个存储位置的函数。\nHash Function 的选择非常重要，好的 Hash Function 可以确保 Hash 结果尽可能的均匀，最理想的情况是每一个不同的 key 都能映射到一个独立的存储索引位置上，但是，毕竟，这只是理想。\n比较实际的思想还是让 Hash Function 的结果能够尽可能的均匀分布即可，既然是尽可能均匀分布，那么就有冲突的风险，冲突很好理解：\n比如有个 Hash Function 是 key %3，那么对 key = 1/4/7 执行 hash 结果就是：\nhash(1) = 1 hash(4) = 1 hash(7) = 1 可以看到三个不同的 key 都映射到了索引 1 上，这样的话就是产生了 Hash 冲突，换言之就是 Hash 的结果不够均匀，都集中在了索引 1 上，这样类似的情况造成的直接后果就是 HashTable 性能变差，即使我们采用冲突处理方式，也是无法改变这种不均匀 Hash 带来的性能影响。\nHash 结果均匀的情况，Hash Table 增删改查的时间复杂度都是 O(1)，而不均匀的 Hash 最严重可以导致时间复杂度退化到 O(n)，也就是全部映射到一个索引上，那么查了等于没查；\n冲突处理 再完美的 Hash Function 也抵不住更多的 key 的侵袭\u0026hellip;(哲学问题？)，况且我们的 Hash Function 往往都是不完美的，Hash 冲突在所难免，为了确保键值的唯一映射，我们需要对 Hash 冲突进行处理，前人总结出了两种常用的方法：开放寻址法和拉链法（哈希桶）\n开放寻址法 用网上见过的一个例子来讲就是：去公共厕所要找坑位的时候，推开一扇门，发现一个小哥直勾勾瞪着你，然后你默默关上门，然后去找其它的坑位，然后找其它的坑位并不一定是按照顺序找对吧，反正就是这么个意思。\n回到 HashTable 上，如果把 hash 得到指定索引位置比作槽，那么开放寻址法的核心思想通俗而言就是这个槽放不下的(冲突的)，找其它槽放下，其中在“找其它槽放下”这一点上，又有着不同的解决方式，分为：线性探查、二次探查、双重哈希，其中效果比较好的是双重哈希，关于这三种方法，继续深入理解的话要去查阅相关的算法介绍，这里不再赘述。\n开放地址法读取数据的过程：当需要查找某个键对应的值时，就会从索引的位置开始对数组进行线性探测，找到目标键值对或者空内存就意味着这一次查询操作的结束。\n开放寻址法中对性能影响最大的就是装载因子：\n开放寻址法装载因子 = 元素数量 / 数组大小\n随着装载因子的增加，线性探测的平均用时就会逐渐增加，这会同时影响 HashTable 的读写性能，当装载率超过 70% 之后， HashTable 的性能就会急剧下降，而一旦装载率达到 100%，整个 HashTable 就会完全失效，这时查找任意元素都需要遍历数组中全部的元素，所以在实现 HashTable 时一定要时刻关注装载因子的变化。\n拉链法（哈希桶） 还是以找坑位的例子来讲，当你推开一个坑位的门，发现一个小哥直勾勾瞪着你，然后这时候你并没有默默关上门，而是告诉他，能进一个坑是缘分，挤一挤吧\u0026hellip;。这就是拉链法，更形象地，哈希桶\u0026hellip;\n与开放地址法相比，哈希桶的方式还是更常见一点的，大多数的编程语言都用拉链法实现 HashTable ，它的实现比较开放地址法稍微复杂一些，但是平均查找的长度也比较短，各个用于存储节点的内存都是动态申请的，可以节省比较多的存储空间。\n实现拉链法一般会使用数组加上链表，不过某些语言会在拉链法的哈希中引入红黑树以优化性能。当以链表数组作为哈希底层的数据结构时，我们可以将它看成一个可以扩展的二维数组。当我们需要将一个键值对写 HashTable 时，键值对中的 key 先经过一个 Hash Function ， Hash Function 返回的哈希会帮助我们选择一个桶，和开放地址法一样，选择桶的方式就是直接对哈希返回的结果取模：\n哈希桶法读取数据过程：当 Hash Function 命中某个桶时，它会依次遍历桶中的链表，然而遍历到链表的末尾也没有找到期望的键，所以 HashTable 中没有该键对应的值。\n计算哈希、定位桶和遍历链表三个过程是 HashTable 读写操作的主要开销，使用哈希桶实现的 HashTable 也有装载因子这一概念：\n哈希桶法装载因子 = 元素数量 / 桶数量\n与开放地址法一样，拉链法的装载因子越大，哈希的读写性能就越差，在一般情况下使用拉链法的 HashTable 装载因子都不会超过 1，当 HashTable 的装载因子较大时就会触发哈希的扩容，创建更多的桶来存储哈希中的元素，保证性能不会出现严重的下降。如果有 1000 个桶的 HashTable 存储了 10000 个键值对，它的性能是保存 1000 个键值对的 1/10，但是仍然比在链表中直接读写好 1000 倍。\nGo Map 实现 Go 语言运行时同时使用了多个数据结构组合表示 HashTable ，其中使用 hmap 结构体来表示 Go Map 结构，我们先来看一下这个结构体内部的字段：\n// A header for a Go map. type hmap struct { count int // 元素个数，内置的 len 函数会通过unsafe.Pointer会从这里读取 \tflags uint8 B uint8 // bucket的数量是2^B, 最多可以放 loadFactor * 2^B 个元素，再多就要扩容 \tnoverflow uint16 //overflow 的 bucket 近似数 \thash0 uint32 // hash seed \tbuckets unsafe.Pointer //2^B 大小的 buckets 数组 \toldbuckets unsafe.Pointer // 扩容的时候，buckets 长度会是 oldbuckets 的两倍 \tnevacuate uintptr // 指示扩容进度，小于此地址的 buckets 迁移完成 \textra *mapextra } Bucket 结构体\n这个是 bucket 在编码期间的结构体\ntype bmap struct { tophash [bucketCnt]uint8 } runtime 期编译器会动态创建新的结构体\ntype bmap struct { topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr } bmap 就是我们常说的“bucket”结构，每个 bucket 里面最多存储 8 个 key，这些 key 经过 Hash Function 计算后落入一个桶内。在桶内，又会根据 key 计算出来的 hash 值的高 8 位来决定 key 到底落入桶内的哪个位置（一个桶内最多有8个位置），每个 bucket 设计成最多只能放 8 个 key-value 对，如果有第 9 个 key-value 落入当前的 bucket，那就需要再构建一个 overflow bucket（溢出桶） ，通过 overflow 指针连接起来。\nMap 元素查找的过程如下：\n key 进行 Hash 计算得到 Key 的 Hash 值； Hash 值低8位用于定位 key 属于哪个 bucket（hash值的低八位和bucket数组长度取余）; Hash 值高8位用于用来快速判断key是否存在，存在则返回对应 value，不存在则退出；  注意，这里高8位不是用来当作key/value在bucket内部的offset的，而是作为一个主键，在查找时对tophash数组的每一项进行顺序匹配的。先比较hash值高位与 bucket 的 tophash[i] 是否相等，如果相等则再比较bucket的第i个的key与所给的key是否相等。如果相等，则返回其对应的value，反之，在overflow buckets中按照上述方法继续寻找。即高8位为 tophash[i] 的比较的主要作用是为了加快判断 key 的速度。\n注意：bucket 不是采用 key/value 的形式，这样做的好处是：在key和value的长度不同的时候，可以消除padding(内存对齐)带来的空间浪费。\nGo Map 创建 var m map[kT]vT 声明后的 map 并没有分配内存空间，即 nil，此时对其访问会触发 panic:\npanic: assignment to entry in nil map Go Map 初始化 可以通过 make关键字分配初始空间（make只用于 Go 的 slice / map / chan 这三个内置对象类型），可以通过字面量初始化，例如：\nm0 := make(map[int]bool) m1 := make(map[int]bool, 5) // 可以指定 size m2 := map[string]string{ \u0026#34;Joey\u0026#34;: \u0026#34;Handsome\u0026#34;, \u0026#34;Sophie\u0026#34;: \u0026#34;Pretty\u0026#34; } make函数实际上会被编译器定位到调用 runtime.makemap，主要做的工作就是初始化 hmap 结构体的各种字段，例如计算 B 的大小，设置哈希种子 hash0 等等。\n注意，runtime.makemap 返回的结果 *hmap 是一个指针，而 slice 的 make 过程中 makeslice 函数返回的是 Slice 结构体对象。这也是 makemap 和 makeslice 返回值的区别所带来一个不同点：当 map 和 slice 作为函数参数时，在函数参数内部对 map 的操作会影响 map 自身；而对 slice 却不会。主要原因，一个是指针（*hmap），一个是结构体（slice）。Go 语言中的函数传参都是值传递，在函数内部，参数会被 copy 到本地。*hmap指针 copy 完之后，仍然指向同一个 map，因此函数内部对 map 的操作会影响实参。而 slice 被 copy 后，会成为一个新的 slice，对它进行的操作不会影响到实参。\n另外，初始化过程还有一些值得关注的点：\n 当 HashTable 中的元素数量少于或者等于 25 个时，编译器会直接将所有的键值对一次加入到 HashTable 中； 一旦 HashTable 中元素的数量超过了 25 个，就会在编译期间创建两个数组分别存储键和值的信息，这些键值对会通过一个for 循环存储； 不过无论使用哪种方法，使用字面量初始化的过程都会使用 Go 语言中的关键字 make 来创建新的哈希并通过最原始的 [] 语法向哈希追加元素；  扩容 有两种情况下，需要做扩容。\n 触发 load factor 的最大值，负载因子已达到当前界限（默认负载因子6.5） 溢出桶 overflow buckets 过多  当满足条件后，将开始扩容，扩容的规则如下：\n  若是负载因子 load factor 达到当前界限，将会动态扩容当前大小的两倍作为其新容量大小；\n  若是溢出桶 overflow buckets 过多的情况，本次扩容规则将是 sameSizeGrow，即是不改变大小的扩容动作；\n  有以下几点需要注意：\n 根据需扩容的原因不同，分为两类容量规则方向，为等量扩容（不改变原有大小）或双倍扩容； 新申请的扩容空间都是预分配，等真正使用的时候才会初始化； 扩容完毕后（预分配），不会马上就进行迁移。而是采取增量扩容的方式，当有访问到具体 bukcet 时，才会逐渐的进行迁移（将 oldbucket 迁移到 bucket）；  所谓”增量扩容“，主要是为了缩短map容器的响应时间。如果不采用增量扩容，当map里面存储的元素很多之后，扩容时系统就会卡往，导致较长一段时间内无法响应请求。不过增量扩容本质上还是将总的扩容时间分摊到了每一次哈希操作上面。扩容会建立一个大小是原来2倍的新的表，将旧的 bucket 搬到新的表中之后，并不会将旧的bucket 从 oldbucket 中删除，而是加上一个已删除的标记。正是由于这个工作是逐渐完成的，这样就会导致一部分数据在 old table 中，一部分在 new table 中， 所以对于hash table的insert, remove, lookup操作的处理逻辑产生影响。只有当所有的 bucket 都从旧表移到新表之后，才会将 oldbucket 释放掉。\n小结 Go 语言使用哈希桶来解决哈希碰撞的问题实现了 map 数据结构 ，它的访问、写入和删除等操作都在编译期间转换成了运行时的函数或者方法。\n哈希在每一个桶中存储键对应哈希的前 8 位，当对哈希进行操作时，这些 tophash 就成为了一级缓存帮助哈希快速遍历桶中元素，每一个桶都只能存储 8 个键值对，一旦当前哈希的某个桶超出 8 个，新的键值对就会被存储到哈希的溢出桶中。\n随着键值对数量的增加，溢出桶的数量和哈希的装载因子也会逐渐升高，超过一定范围就会触发扩容，扩容会将桶的数量翻倍，元素再分配的过程也是在调用写操作时增量进行的，不会造成性能的瞬时巨大抖动。\n参考   https://www.cnblogs.com/maji233/p/11070853.html\n  https://www.jianshu.com/p/aa0d4808cbb8\n  "
},
{
	"uri": "https://blog.yingchi.io/posts/2020/3/go-array-slice.html",
	"title": "Go Array 与 Slice 原理",
	"tags": ["golang"],
	"description": "",
	"content": "数组 Array 几乎每个常见的编程语言都有数组这个概念，但是每个语言对于数组的定位都不一样，有的语言会把数组用作常用的基本的数据结构，比如 JavaScript，而 Golang 中的数组(Array)，更倾向定位于一种底层的数据结构，记录的是一段连续的内存空间数据。但是在 Go 语言中平时直接用数组的时候不多，大多数场景下我们都会直接选用更加灵活的切片(Slice)，我这里很谨慎地说“直接”用数组，因为里面有学问，稍后会说。在Go程序中经常看不到数组的一个很重要原因是，数组的大小是固定的\u0026hellip; ，所以很多场景下我们无法直接给出数组的确定长度，因此才会选择长度“可变”的切片。变与不变是编程中一个恒久远的话题，牵扯到这个话题的往往是性能与灵活性两个关键词，这个话题很庞大，有机会会单独写一篇博客进行探讨。\n回到数组中，数组的声明形式：\nvar arr [5]int var buffer [256]byte 初始化方式有两种，一种是显示声明长度，另一种是[...]T推断长度，注意，推断长度也是给出了长度，这个和之后 Slice 的[]T的声明方式是不一样的：\narr1 := [3]int{0,1,2} arr2 := [...]string{\u0026#34;Joey\u0026#34;,\u0026#34;Sophie\u0026#34;} 第二种初始化属于语法糖，会经过编译器推导，得到数组长度，即最终转换成第一种，显然，两种方式在运行时是没有任何区别的。但是在编译期，Go 为不同类型不同结构的初始化方式进行了优化（不止是数组的初始化这一点上，其它一些代码同样如此），对于优化过程，可以简单概括为下面的话：\n 如果数组中元素的个数小于或者等于 4 个，那么所有的变量会直接在栈上初始化； 如果数组元素大于 4 个，变量就会在静态存储区初始化然后拷贝到栈上，这些转换后的代码才会继续进入中间代码生成和机器码生成两个阶段，最后生成可以执行的二进制文件。  数组虽然比较重要，但是的概念其实比较简单，还有一个非常需要注意的点是，当你用到 Go 数组的时候，一定要注意一个避不开的问题，一定不要越界访问\n切片 Slice 及其与 Array 的关系 刚接触 Go 的一些学习者们肯定会混淆 Array 与 Slice 的用法，我想主要原因是受其它语言影响比较大，比如国内用 Java 的比较多，如果突然换到 Go，一定会对这个 slice 概念一头雾水。\n很多人仅仅知道 Slice 与 Array 的区别是：Slice 长度可变，如果仅仅是知道这个的话其实是很危险的，平时有一些错误的用法会直接把你整的找不着北，我们需要从底层了解这个语言特性。\n学习 slice，或者说区别 Slice 与 Array 的首要关键是记住下面几点：\n Slice 不是 Array，它描述一个 Array； Slice 的本质是一个 Struct，携带一个数组指针，长度，容量，这是他长度可变的根本原因；  可以在 Go 源码中找到 sliceHeader 的定义：\ntype sliceHeader struct { Data unsafe.Pointer // 指向的数组  Len int // 长度，即 Slice 截取 Data 的长度  Cap int // 容量，即 Data 的大小，显然不会小于 Len } Slice 的声明方式比较多，我们可以直接构建一个空 Slice 而不需要指定长度，我们也可以直接基于 Array 本身构建一个 Slice，亦可以基于 Slice 构建新的 Slice，我们以一个典型的场景去理解 Slice 与 Array 的关系：\nvar sli0 = make([]int) // make([]T, Len, Cap) var sli1 = arr1[5:10] var sli2 = sli1[2:] sli1 在 arr1 的左闭右开索引区间 [5, 10) 上构建了切片，而 sli2 又在 sli1 的基础上构建了 [2, 5) 的切片，这里值得记住的一点是，切片结构体里保存的是底层数组的指针(引用)，因此他们指向的是同一块底层数组，我们可以知道，sli2[0]的元素就是sli1[2]对应的元素，指向的都是底层数组arr1的arr1[7]元素，此时如果修改 sli2[0] 的话，实际上就是修改的 arr1[7]，因此 sli1[2] 也是会变的，这个场景一定要理解。\n函数传递 Slice 还是强调开始提到的：记住，Slice 是个结构体，只不过这个结构体里存有数组的指针。\n因此切片作为函数参数直接传递时就是个普通的值传递，所有语言都会讲到，函数值传递时只是传递参数值的 Copy 对象，但是 Slice 这个值很特殊，他里面存有数组的指针，又包含了 Slice 的 Len 和数组的 Cap，即又包含指针又包含普通值，因此你也知道我想说什么了，记住：\n 直接传递 Slice 进函数时，传递的是 Slice 的 copy; 对 Slice 的元素进行修改操作，会通过指针直接修改数组，因此是可以实现的； 对 Slice 的长度修改，修改的是 copy 对象的 Len 字段，因此原 Slice 是长度是不会变的； 想要在函数内修改 Slice 的长度，最好的方式是传递 Slice 的指针；  容量与 append sliceHeader中还有一个 Cap  变量，这个变量存储了 Slice 的容量，准确的说应该是底层 Data 数组的长度，即记录数组实际使用了多少的空间，这也是 Len 能达到的最大值。\nSlice 的元素追加通过 append 操作实现，如：\nnames = append(names, \u0026#34;Joey\u0026#34;) 注意 append 返回的是一个新的 slice，直接 append 而不赋值给原 slice 的话，原 slice 长度是不会改变的，只有把 append 后得到的新 slice 赋值回去才可以实现原 slice 基础上的元素追加。多说一句，从编译器层面，Go 在编译期针对于 append 后是否赋值给原 slice 实现了两种编译方式实现优化。\n还有，我这里没写 slice 的移除元素通过什么关键字，显然，不说的话就是同一个关键字，没错，移除的逻辑就很直白，也很反程序员：\nages = append(ages[:5], ages[6:]) 明白了吧，别问，问就是 大道至简 less is more，我现在也不明白到底是谁 less 了谁 more 了，想想还挺心酸的。\n划重点\n关于容量需要记住的就是：当向 Slice 追加元素导致 Len大于 Cap 时，会触发扩容机制，创建一个Cap大于原数组的新数组（首元素地址不一致），并将值拷贝进新数组，之后再改变Slice元素值时改变的是新创建的数组（切断与原数组的引用关系）。是的，当触发扩容机制后，新的 Slice 底层数组已经不再是之前的数组了，对于 Slice 元素的修改都是基于新的底层数组进行。\n需要注意的是，涉及到 Copy 这个词，显然是一个影响效率的行为，因此我们如果真的关注性能这一块儿的话，一定要想办法避免频繁的触发扩容机制，比如当我们明确地知道 Slice 容量上限的时候，在声明时就应该通过 make([]T, Len, Cap) 给出明确的 cap 值。\n 最后，从一段简单的代码入手更直观地去理解\npackage main import \u0026#34;fmt\u0026#34; var ( sli1 = make([]int, 5) sli2From1 = sli1[2:] ) func main() { fmt.Println(\u0026#34;初始情况\u0026#34;) printSliLenCap() fmt.Println(\u0026#34;修改 sli2\u0026#34;) sli2From1[0] = 1 printSliLenCap() fmt.Println(\u0026#34;sli2 追加元素\u0026#34;) sli2From1 = append(sli2From1, 2, 2, 2, 2) printSliLenCap() fmt.Println(\u0026#34;再次修改 sli2\u0026#34;) sli2From1[0] = 3 printSliLenCap() } func printSliLenCap() { fmt.Printf(\u0026#34;sli1: %v, len: %d, cap: %d \\n\u0026#34;, sli1, len(sli1), cap(sli1)) fmt.Printf(\u0026#34;sli2: %v, len: %d, cap: %d \\n\u0026#34;, sli2From1, len(sli2From1), cap(sli2From1)) } 输出结果：\n初始情况 sli1: [0 0 0 0 0], len: 5, cap: 5 sli2: [0 0 0], len: 3, cap: 3 修改 sli2 sli1: [0 0 1 0 0], len: 5, cap: 5 sli2: [1 0 0], len: 3, cap: 3 sli2 追加元素 sli1: [0 0 1 0 0], len: 5, cap: 5 sli2: [1 0 0 2 2 2 2], len: 7, cap: 8 再次修改 sli2 sli1: [0 0 1 0 0], len: 5, cap: 5 sli2: [3 0 0 2 2 2 2], len: 7, cap: 8 ... 可以看到，就像前面说过的：\n  可以基于已有的 slice 构建新的 slice；\n  没有触发扩容机制前，slice 的底层数组会指向同一个数组，因此对于其中一个 slice 元素的修改，如果底层和其它 slice 指向的同一个数组元素，那么会影响到其它 slice 元素值；\n  触发扩容机制后，新的 slice 底层数组改变，因此对其底层数组元素的修改不会影响到之前相关的 slice，因为两个 slice 的底层数组已经不是同一个。\n  "
},
{
	"uri": "https://blog.yingchi.io/about.html",
	"title": "About",
	"tags": [],
	"description": "Hugo zzo, zdoc theme documentation home page",
	"content": "蒋英驰   网易互娱基础架构 SRE 工程师\n  西北工业大学软件工程硕士\n  主要研究方向：云基础架构性能与稳定性优化；\n  专业技能\n 掌握计算机专业相关基础知识，包括计算机网络、操作系统等；熟悉 TCP/IP 协议栈基本原理； 熟悉 GNU/Linux 操作系统，掌握大规模 Linux 服务集群的管理方法； 熟悉 SRE 技术体系，掌握云计算基础架构性能与稳定性优化基本方法； 掌握 Go、Python 编程语言，熟悉系统与脚本开发流程； 熟悉容器化相关概念，了解 Docker / Kubernetes 实现原理并掌握基本使用流程； 了解前后端开发框架 Gin / Vue 基本开发方法，可以快速开发 Web 运维工具； 了解服务网格概念，了解 Istio 服务治理的基本思想；    技能认证\n CKA (Certificate Kubernetes Administrator) | Kubernetes 管理员认证 ACP (Alibaba Cloud Certified Professional) | 阿里巴巴云计算工程师认证 CET-6 (College English Test band-6) | 大学英语六级    联系方式\n 个人主页：https://yingchi.io 邮箱：yingchi1994@gmail.com； 微信号：yingchi_joey；    "
},
{
	"uri": "https://blog.yingchi.io/posts/2019/10/network-devices.html",
	"title": "网络模型学习基础：各层网络设备概念",
	"tags": ["kubernetes", "docker", "network"],
	"description": "",
	"content": " 在学习研究诸如 kubernetes 网络或 Docker 网络等各种开源网络模型时，会涉及到各种或实际或虚拟的网络设备概念，例如各种虚拟网桥。因此必须对这些网络设备有充分的认识才可以进行接下来的学习。\n  根据 OSI 参考模型，网络分为七层，根据之后的学习需要，这里主要针对 L1、L2、L3 层的设备进行学习认识\n L1：Hub（集线器/中继器） L1 层即“物理层”，作为网络的最底层，这一层的网络设备所做的事情比较单调，主要的作用就是实现物理上的网络连通，如下图几个终端节点，\n想要实现这几个节点的互连互通，可以连接到叫做 Hub（集线器）的设备上，如下图，这个设备的主要任务仅仅是把接收到的信号整型放大，从其它各个端口转发出去，因此也称之为“中继器”。\n以 Node0 为例展示一下节点发送网络信号的过程。如下图，Node0 向连接的 Hub 发出网络信号，Hub 接收到信号后，将其整型放大，然后转发给了 Node1、Node2、Node3。这其实就是最简单的一种星型的局域网架构。\n这个过程要注意两点核心：\n Hub 仅仅对信号进行物理上的整型放大操作 Hub 转发时采用的是广播方式，向其它端口节点转发信号，无选择  然后我们再来看一个复杂一点的例子，假如两个机房各有一组终端节点连接到了各自的 Hub 上，各自组成了局域网，如下图。\n我们现在想把两个房间的网络互连起来，应该如何做？按照上面的思想，我们是否可以加入一个更高层级的 Hub（称之为主干 Hub），把各个机房的网络连接起来？如图\n答案是可行的，在这种情况下，通过中间这个主干 Hub的转发，我们可以将一个机房发出的信号转发到另一个机房，如图\n但是，虽然可行，我们应该有这样的疑问，这样做真的好吗？虽然两个机房的网络互通了，但是是否又增加了这个网络的负担呢？我们举一个简单的例子就可以发现问题所在，假如 Node0 仅仅是想将信号发送给 Node1，尽管从网络拓扑中看到他们挨得很近，仅仅通过一个 Hub 就可以转发到，但是由于 Hub 只能采用广播的方式转发，因此这个信号不仅被传到了 Node2 上，还通过刚加入的总集线器被传递到了另一个机房的所有节点中，当然这些都是无用功。\n说到这里，我们还要认识两个概念 —— 冲突域和广播域。其实很好理解，所谓冲突域，很多地方也叫碰撞域，顾名思义，就是在当网络中有一个信号在流转时，此时若有另一个节点向网络中发送信号，会引起干扰，也就是冲突或碰撞，这个受波及的范围就是冲突域，我们可以认为，L1 网络中靠类似 Hub 这种设备连接起来的物理网段都是属于一个冲突域，Hub 是无法隔离冲突域的，在 L2、L3 网络中均有能力隔离冲突域；另一个概念，广播域，广播域就是接收同样广播消息的集合，我们一般认为，广播域其实就是同一个网络的代名词，我们如今所使用的互联网，其实是通过 Router 这种网际交换设备连接不同网络而产生的。\n结合上面这两个概念，我们重新分析通过主干 Hub 连接两个局域网的方式，就能明白为什么这样会严重影响网络性能了。\n那么，还是针对连接两个机房的场景，我们期望既可以实现两个网络的互通，又能提高通信的效率，有没有好的方式呢？\nL2：Bridge（网桥）、Switch(交换机/多端口网桥) 现在到网络模型的第2层，也就是数据链路层，看一下这一层的网络设备。继续讨论之前的场景，这次我们不再采用主干 Hub 的方案，而是加入一个叫做 Bridge（网桥）的设备，狭义上的网桥其实是二端口网桥，如图。所谓 Bridge，我们很容易就能明白这是一个桥接设备，桥接的核心是延长，是扩大，因此，我们可以在某种程度上把它看做为一个智能的、高级的 Hub，智能高级的原因是它是工作在第二层的，在进行它的工作原理介绍之前，重新看一下我之前 L1 层对于 Hub 工作原理的描述，网络中流转的信息我在第一层中是以“信号”称呼的，因为第一层只能操作物理电信号，无非就是对电流的整型放大复制转发。\n来到第二层，电信号已经有了最基本的数据意义，这种基本的数据我们称之为“数据帧”，或简称为“帧”，就像是儿童刚有了认知能力一样，当网络对传递的信息有了数据层面的判断。不仅是传递的信息被封装的高级了，每个通信节点也开始用起了自己的身份证，叫做MAC（Media Access Control Address），有些地方也称之为局域网地址，注意，这个地址是全球唯一的，相当于每个通信设备都会有一个唯一标识，就好比你的身份证不会重号那样。那么，基于第二层实现了这么多概念，就可以做一些更智能的事情，于是，L2 中的 Bridge 设备便可以对帧在识别、过滤的基础上借助 MAC 实现更高效地转发，而且，Bridge 设备还具有一定的记忆功能，会记住网络中节点的 MAC 地址以及对应的端口，这一点很重要。\n如下图，假如 Bridge 已经知道了 Node2 的 MAC，此时 Node0 想给 Node2 发送一条消息，那么 Bridge 接收到数据帧时，就不会再转发到另一个机房中去，只让帧在右边这个网段里转发，这样就已经比之前通过主干 Hub 的方式冲突的几率就更小了，换句话说，通信效率就提高了，Bridge 的这种特性我们称之为“隔离冲突域”\n二端口网桥并没有流行很久，受此启发，多端口网桥研制了出来，将其优点发挥到极致。如图，图中的 Bridge 已经是现在广义概念上的多端口网桥了，在很多地方称之为交换机（Switch），通过 Switch 组建局域网已经是很普遍的事了，很多朋友使用的路由器（Router），其中的功能子集就有 Switch。\n多端口网桥是具有“学习”功能的，通过对帧不断地过滤转发，可以记住网络中节点的地址，让之后的帧转发更高效，如下图，假如 Node0 要给 Node5 发一条数据，在最初状态下，所有 Bridge 都不认识 Node0，也不认识 Node5，因此在转发帧的过程中，从帧中记录下发送方 Node0 的 MAC 和端口号（文章内我们就简单以节点名当做端口号），意思是，“下次别人如果找你的话，我就知道通过哪个端口转发给你了”，而接收方在哪里，需要往哪个指定的端口转发出去，Bridge 一开始并不知道的，因此只能通过广播的方式转发出去，这个过程中，Node5 收到了 Node0 发来的数据帧。\n可以看到，通过多端口网桥构建的网络，和第一层的 Hub 一样，这广播域上也是无法做到隔离的。\n当 Bridge1 中记住了 Node0 的 MAC 和端口，这时如果 Node3 想发送数据给 Node0，Bridge 还会广播给 Node1 和 Node2 端口吗？如图，明显不会了，因为我们说了，Bridge 是具有学习功能的，当能够明确接收方端口的时候，就可以进行针对性转发，没有必要再进行广播浪费资源了。\n而且值得注意的是，使用交换机的情况下，支持 LAN 内的全双工通信，举个例子，Node3 向 Node0 发送数据帧的同时也允许 Node0 向 Node3 发送数据帧，这里原理简单说一下，就是因为 Switch 内部实现了存储转发，并且与终端节点的连接是点对点独立的，也就是 Node3 到 Switch 和 Switch 到 Node0 是两条独立连接，又根据存储转发的特性，可以在线路空闲时将数据帧转发出去，从而完成这种全双工通信。这种提升的意义，如从 Hub 到 Switch 实现了从共享网络带宽到独享网络带宽，全双工实现了数据帧发送和接收的独享带宽。\n最后简单说一下 Router 吧，其实当 L1 L2 这种设备的实现原理能明白之后，对于 L3 及之后的各种设备其实自己也能摸索清楚了，Router 的工作原理可以总结为八个字：路由计算、分组转发，在 Router 层面已经开始面向协议工作了，而 Bridge 与高层的协议无关。Router 通过 IP 地址寻址，对于同一网段的地址，到达路由器接口后就不再转发，实现了隔离广播的作用，这一点是 Bridge 做不到的。因此，作为 L3 的设备，Router 实现了既可以隔离冲突域，又可以隔离广播域。但是我们也要注意到一点，越往高层走，对数据的处理越复杂，这也是会影响通信效率的，所以不同的场景下要选择最合适的设备才是正确的。\n最后总结一下，Hub 无法隔离冲突域，在同一个冲突域内通信；Bridge 可以隔离冲突域，无法隔离广播域，在同一个广播域内通信；路由器可以隔离冲突域和广播域，可以实现不同广播域间通信。或者 Hub 与 Bridge 只能是扩展局域网的设备，而 Router 是连接异构网络的设备。\n参考资料   《思科网络技术学院教程 CCNA Exploration：网络基础知识》\n  Network Devices (Hub, Repeater, Bridge, Switch, Router, Gateways and Brouter)\n  Differences between a switch and a bridge\n  "
}]