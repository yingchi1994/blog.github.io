<!DOCTYPE html>
<html lang="en" data-theme=""><head>
    <title> Joey Jiang | Kubernetes &amp; Docker 网络原理（三） </title>

    
    <meta charset="utf-8"><meta name="generator" content="Hugo 0.71.1" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
    <meta name="description" content="Infrastructure SRE / Golang / Python">
    
    <link rel="stylesheet"
          href="https://blog.yingchi.io/css/style.min.8a9cc11a1afa7bd869914c587182281a374d06d6c3e09e330e86cfccf4908668.css"
          integrity="sha256-ipzBGhr6e9hpkUxYcYIoGjdNBtbD4J4zDobPzPSQhmg="
          crossorigin="anonymous"
          type="text/css">
    
    <link rel="stylesheet"
        href="https://blog.yingchi.io/css/markupHighlight.min.9755453ffb7bc4cd220f86ebb5922107b49f193cc62fc17e9785d27b33a8bf5b.css"
        integrity="sha256-l1VFP/t7xM0iD4brtZIhB7SfGTzGL8F&#43;l4XSezOov1s="
        crossorigin="anonymous"
        type="text/css">
    
    <link rel="stylesheet" 
    href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css" 
    integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA==" 
    crossorigin="anonymous" />

    
    <link rel="shortcut icon" href="https://blog.yingchi.io/favicons/favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon" sizes="180x180" href="https://blog.yingchi.io/favicons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://blog.yingchi.io/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://blog.yingchi.io/favicons/favicon-16x16.png">

    <link rel="canonical" href="https://blog.yingchi.io/posts/2020/4/docker-k8s-network-3.html">

    
    
    
    
    <script type="text/javascript"
            src="https://blog.yingchi.io/js/anatole-header.min.d8599ee07b7d3f11bafbac30657ccc591e8d7fd36a9f580cd4c09e24e0e4a971.js"
            integrity="sha256-2Fme4Ht9PxG6&#43;6wwZXzMWR6Nf9Nqn1gM1MCeJODkqXE="
            crossorigin="anonymous"></script>


    
    <meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://blog.yingchi.io/images/site-feature-image.png"/>

<meta name="twitter:title" content="Kubernetes &amp; Docker 网络原理（三）"/>
<meta name="twitter:description" content="Service 通信 kube-proxy 运行机制 为了支持集群的水平扩展、高可用性，Kubernetes抽象出了Service的概念。Service是对一组Pod的抽象，它会根据访问策略（如负载均衡策略）来访问这组Pod。 Kubernetes在创建服务时会为服务分配一个虚拟的IP地址，客户端通过访问这个虚拟的IP地址来访问服务，服务则负责将请求转发到后端的Pod上。起到一个类似于反向代理的作用，但是它和普通的反向代理还是有一些不同：首先，它的Service 的 IP 地址，也就是所谓的 ClusterIP 是虚拟的，想从外面访问还需要一些技巧；其次，它的部署和启停是由Kubernetes统一自动管理的。
Service 和 Pod 一样，其实仅仅是一个抽象的概念，背后的运作机制是依赖于 kube-proxy 组件实现的。
在 Kubernetes 集群的每个 Node 上都会运行一个 kube-proxy 服务进程，我们可以把这个进程看作 Service 的透明代理兼负载均衡器，其核心功能是将到某个 Service 的访问请求转发到后端的多个 Pod 实例上。此外，Service的Cluster IP与 NodePort 等概念是 kube-proxy 服务通过iptables的NAT转换实现的，kube-proxy 在运行过程中动态创建与 Service 相关的 iptables 规则，这些规则实现了将访问服务（Cluster IP或NodePort）的请求负载分发到后端 Pod 的功能。由于 iptables 机制针对的是本地的 kube-proxy 端口，所以在每个 Node 上都要运行 kube-proxy 组件，这样一来，在 Kubernetes 集群内部，我们可以在任意 Node 上发起对 Service 的访问请求。综上所述，由于 kube-proxy 的作用，在 Service 的调用过程中客户端无须关心后端有几个 Pod，中间过程的通信、负载均衡及故障恢复都是透明的。
kube-proxy 运行模式 kube-proxy 的具体运行模式其实是随着 Kubernetes 版本的演进有着较大的变化的，整体上分为以下几个模式的演化：
 userspace (用户空间代理)模式 iptables 模式 IPVS 模式  userspace 模式"/>

</head>
<body><div class="sidebar animated fadeInDown ">
    <div class="logo-title">
        <div class="title">
            <img src="https://blog.yingchi.io/images/profile.jpg" alt="profile picture">
            <h2 title=""><a href="/">Yingchi&#39;s Blog</a></h2>
            <div class="description">
                <p>Infrastructure SRE / Golang / Python</p>
            </div>
        </div>
    </div>
    <ul class="social-links">
        
            <li>
                <a href="https://yingchi.io" rel="me" aria-label="Home">
                    <i class="fa fa-home" style="font-size:large;" aria-hidden="true"></i>
                </a>
            </li>
        
            <li>
                <a href="https://github.com/yingchi1994" rel="me" aria-label="GitHub">
                    <i class="fab fa-github" style="font-size:large;" aria-hidden="true"></i>
                </a>
            </li>
        
            <li>
                <a href="mailto:yingchi1994@gmail.com" rel="me" aria-label="e-mail">
                    <i class="fas fa-envelope" style="font-size:large;" aria-hidden="true"></i>
                </a>
            </li>
        
    </ul>
    
    <div class="tag-area">
        
<ul>
  <li class="tag-list">
    
    <a href="/tags/cloud-native.html">
      <span>
        <span class="tag-yingchi">
          Cloud Native
          [<span>1</span>]
        </span>
      </span>
    </a>
  </li>
  <li class="tag-list">
    
    <a href="/tags/docker.html">
      <span>
        <span class="tag-yingchi">
          Docker
          [<span>2</span>]
        </span>
      </span>
    </a>
  </li>
  <li class="tag-list">
    
    <a href="/tags/golang.html">
      <span>
        <span class="tag-yingchi">
          Golang
          [<span>5</span>]
        </span>
      </span>
    </a>
  </li>
  <li class="tag-list">
    
    <a href="/tags/istio.html">
      <span>
        <span class="tag-yingchi">
          Istio
          [<span>2</span>]
        </span>
      </span>
    </a>
  </li>
  <li class="tag-list">
    
    <a href="/tags/kubernetes.html">
      <span>
        <span class="tag-yingchi">
          Kubernetes
          [<span>15</span>]
        </span>
      </span>
    </a>
  </li>
  <li class="tag-list">
    
    <a href="/tags/network.html">
      <span>
        <span class="tag-yingchi">
          Network
          [<span>4</span>]
        </span>
      </span>
    </a>
  </li>
  <li class="tag-list">
    
    <a href="/tags/note.html">
      <span>
        <span class="tag-yingchi">
          Note
          [<span>1</span>]
        </span>
      </span>
    </a>
  </li>
  <li class="tag-list">
    
    <a href="/tags/service-mesh.html">
      <span>
        <span class="tag-yingchi">
          Service Mesh
          [<span>2</span>]
        </span>
      </span>
    </a>
  </li>
</ul>

</div>


    <div class="footer">
        <div class="by_farbox">&copy; Joey Jiang  2021 </div>
    </div>
</div>
<div class="main">
    <div class="page-top  animated fadeInDown ">
    <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
    </a>
    <ul class="nav" id="navMenu">
        
        
            
            <li><a 
                   href="/"
                        
                   title="">Home</a></li>
        
            
            <li><a 
                   href="/posts.html"
                        
                   title="">Posts</a></li>
        
            
            <li><a 
                   href="/tags.html"
                        
                   title="">Tags</a></li>
        
            
            <li><a 
                   href="/about.html"
                        
                   title="">About</a></li>
        
        
        
    </ul>
</div>

    <div class="autopagerize_page_element">
        <div class="content">
    <div class="post  animated fadeInDown ">
        <div class="post-content">
            <div class="post-title">
                <h3>Kubernetes &amp; Docker 网络原理（三）</h3>
                
            </div>

            <h1 id="service-通信">Service 通信</h1>
<h2 id="kube-proxy-运行机制">kube-proxy 运行机制</h2>
<p>为了支持集群的水平扩展、高可用性，Kubernetes抽象出了Service的概念。Service是对一组Pod的抽象，它会根据访问策略（如负载均衡策略）来访问这组Pod。
Kubernetes在创建服务时会为服务分配一个虚拟的IP地址，客户端通过访问这个虚拟的IP地址来访问服务，服务则负责将请求转发到后端的Pod上。起到一个类似于反向代理的作用，但是它和普通的反向代理还是有一些不同：首先，它的Service 的 IP 地址，也就是所谓的 ClusterIP 是虚拟的，想从外面访问还需要一些技巧；其次，它的部署和启停是由Kubernetes统一自动管理的。</p>
<p>Service 和 Pod 一样，其实仅仅是一个抽象的概念，背后的运作机制是依赖于 kube-proxy 组件实现的。</p>
<p>在 Kubernetes 集群的每个 Node 上都会运行一个 kube-proxy 服务进程，我们可以把这个进程看作 Service 的透明代理兼负载均衡器，其核心功能是将到某个 Service 的访问请求转发到后端的多个 Pod 实例上。此外，Service的Cluster IP与 NodePort 等概念是 kube-proxy 服务通过iptables的NAT转换实现的，kube-proxy 在运行过程中动态创建与 Service 相关的 iptables 规则，这些规则实现了将访问服务（Cluster IP或NodePort）的请求负载分发到后端 Pod 的功能。由于 iptables 机制针对的是本地的 kube-proxy 端口，所以在每个 Node 上都要运行 kube-proxy 组件，这样一来，在 Kubernetes 集群内部，我们可以在任意 Node 上发起对 Service 的访问请求。综上所述，由于 kube-proxy 的作用，在 Service 的调用过程中客户端无须关心后端有几个 Pod，中间过程的通信、负载均衡及故障恢复都是透明的。</p>
<h2 id="kube-proxy-运行模式">kube-proxy 运行模式</h2>
<p>kube-proxy 的具体运行模式其实是随着 Kubernetes 版本的演进有着较大的变化的，整体上分为以下几个模式的演化：</p>
<ul>
<li>userspace (用户空间代理)模式</li>
<li>iptables 模式</li>
<li>IPVS 模式</li>
</ul>
<p><strong>userspace 模式</strong></p>
<p>kube-proxy 最早的工作模式便是 userspace 用户空间代理模式，在这种模式下 kube-proxy 是承担着真实的 TCP/UDP 代理任务的，当 Pod 通过 Cluster IP 访问 Service 的时候，流量被 iptables 拦截后转发到节点的 kube-proxy 进程，服务的路由信息通过 watch API Server 进行获取，然后 kube-proxy 进程再与具体的 Pod 建立 TCP/UDP 连接，从而将请求发送给 Service 的后端 Pod 上，在这个过程中实现负载均衡。</p>
<p><strong>iptables 模式</strong></p>
<p>从 kubernetes 1.2 版本开始不再采用 userspace 用户空间代理模式，取而代之的是 iptables 模式，在 iptables 模式下 kube-proxy 不再担任直接的 proxy 作用，它的核心职责变为：一方面通过 watch API Server 实时获取 Service 与 Endpoint 的变更信息，然后动态地更新 iptables 规则，然后流量会根据 iptables 的 NAT 机制直接路由到目标 Pod，而不是再去单独建立连接。</p>
<p><img src="docker-k8s-network-3/image-20200712153754598.png" alt="image-20200712153754598"></p>
<p>与之前的 userspace 模式相比，iptables 模式完全工作在内核态，不需要切换到用户态的 kube-proxy，避免了内核态用户态的频繁切换使得性能相比之前有所提高。</p>
<p>但是 iptables 也存在着局限性，就是由于 iptables 客观因素，当 Kubernetes 集群规模扩大，Pod 数量大量增加之后，iptables 的规则数量会随之急剧增加，进而导致其转发性能的下降，甚至会出现规则丢失的情况（故障非常难以重现和排查），因此 iptables 模式也有待于改进。</p>
<p><strong>IPVS 模式</strong></p>
<p>IPVS 模式即 IP Virtual Server 模式，在 Kubernetes 1.11中 IPVS 模式升级为 GA，IPVS 虽然和 iptables 都是基于 Netfilter 实现，但是定位有着本质不同，iptables 设计为防火墙使用，而 IPVS 用于高性能负载均衡，而且从规则的存储角度，IPVS 采用的是 Hash Table 结构，因此理论上讲更适合在不影响性能的情况下大规模地扩展，同时 IPVS 支持比 iptables 更复杂的负载均衡算法（最小负载/最小连接数/加权等），支持服务器健康检查和连接重试等功能，另外还可以动态修改 ipset 集合。</p>
<p><img src="docker-k8s-network-3/image-20200712155152657.png" alt="image-20200712155152657"></p>
<p>在 IPVS 模式下，并不是就直接抛弃 iptables 了，虽然 IPVS 在性能上肯定是要优于 iptables 的，但同时也有许多功能 IPVS 相比 iptables 是缺失的，比如包过滤、地址伪装、SNAT 等功能，因此在一些场景下是需要 IPVS 与 iptables 配合工作的，比如 NodePort 实现。同时在 IPVS 模式下，kube-proxy 使用的是 iptables 的扩展 ipset，而不是直接通过 iptables 生成规则链。iptables 规则链是线性的数据结构，而 ipset 是带索引的数据结构，因此当规则很多时，可以高效地匹配查找。</p>
</div>
        <div class="post-footer">
            <div class="info">
                
                <span class="separator"><a class="tag" href="/tags/kubernetes/">kubernetes</a><a class="tag" href="/tags/network/">network</a></span>

            </div>
        </div>

        
    </div>


        </div>
    </div>
</div>

<script type="text/javascript"
        src="https://blog.yingchi.io/js/jquery.min.86b1e8f819ee2d9099a783e50b49dff24282545fc40773861f9126b921532e4c.js"
        integrity="sha256-hrHo&#43;BnuLZCZp4PlC0nf8kKCVF/EB3OGH5EmuSFTLkw="
        crossorigin="anonymous"></script>




<script type="text/javascript"
        src="https://blog.yingchi.io/js/bundle.min.0f9c74cb78f13d1f15f33daff4037c70354f98acfbb97a6f61708966675c3cae.js"
        integrity="sha256-D5x0y3jxPR8V8z2v9AN8cDVPmKz7uXpvYXCJZmdcPK4="
        crossorigin="anonymous"></script>

<script type="text/javascript"
        src="https://blog.yingchi.io/js/medium-zoom.min.92f21c856129f84aeb719459b3e6ac621a3032fd7b180a18c04e1d12083f8aba.js"
        integrity="sha256-kvIchWEp&#43;ErrcZRZs&#43;asYhowMv17GAoYwE4dEgg/iro="
        crossorigin="anonymous"></script>
</body>

</html>
